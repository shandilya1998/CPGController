2021-03-28 11:41:22.191498: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
Unable to init server: Could not connect: Connection refused
Unable to init server: Could not connect: Connection refused

(learn.py:2416): Gdk-CRITICAL **: 11:41:25.467: gdk_cursor_new_for_display: assertion 'GDK_IS_DISPLAY (display)' failed
2021-03-28 11:41:25.754665: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-28 11:41:25.755982: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-03-28 11:41:25.791782: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-28 11:41:25.793318: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:00:1e.0 name: Tesla T4 computeCapability: 7.5
coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s
2021-03-28 11:41:25.793354: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-28 11:41:25.798828: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-28 11:41:25.798981: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-28 11:41:25.800613: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-03-28 11:41:25.801314: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-03-28 11:41:25.807206: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-03-28 11:41:25.808690: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-03-28 11:41:25.809293: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-28 11:41:25.809668: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-28 11:41:25.811228: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-28 11:41:25.812780: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
[DDPG] Building the actor model
2021-03-28 11:41:26.084276: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-03-28 11:41:26.084508: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-28 11:41:26.084846: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-28 11:41:26.085642: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:00:1e.0 name: Tesla T4 computeCapability: 7.5
coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s
2021-03-28 11:41:26.085680: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-28 11:41:26.085900: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-28 11:41:26.086009: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-28 11:41:26.086070: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-03-28 11:41:26.086119: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-03-28 11:41:26.086166: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-03-28 11:41:26.086214: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-03-28 11:41:26.086257: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-28 11:41:26.086445: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-28 11:41:26.087308: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-28 11:41:26.088048: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-03-28 11:41:26.088095: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-28 11:41:26.954635: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-03-28 11:41:26.954675: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2021-03-28 11:41:26.954683: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2021-03-28 11:41:26.955055: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-28 11:41:26.956073: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-28 11:41:26.956965: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-28 11:41:26.957679: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11669 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5)
[DDPG] Building the actor model
[DDPG] Building the critic model
[DDPG] Building the critic model
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1616931690.627168542, 64.565000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1616931690.628081550, 64.566000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1616931690.628247926, 64.566000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1616931692.010566821, 65.865000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1616931692.871735651, 66.601000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1616931693.716025345, 67.399000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1616931694.566780017, 68.201000000]: Ready to take commands for planning group back_left_leg.[0m
2021-03-28 11:41:45.126289: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 793800000 exceeds 10% of free system memory.
2021-03-28 11:41:49.365638: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 793800000 exceeds 10% of free system memory.
[Actor] GPU>>>>>>>>>>>>
[Actor] [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
[Actor] Memory Growth Allowed
2021-03-28 11:41:56.022177: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 793800000 exceeds 10% of free system memory.
[Actor] Dataset <PrefetchDataset shapes: (((1125, 6), (1125, 34), (1125, 60)), ((1125, 350, 12), (1125, 1), (1125, 12), (1125, 12))), types: ((tf.float32, tf.float32, tf.float64), (tf.float32, tf.float32, tf.float64, tf.float32))>
[Actor] Starting Actor Pretraining
[Actor] Starting Episode 0
2021-03-28 11:42:00.149961: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 793800000 exceeds 10% of free system memory.
2021-03-28 11:42:02.802999: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-28 11:42:03.682330: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
[Actor] Episode 0 Step 0 Loss: 2848.812255859375
[Actor] Episode 0 Step 1 Loss: 2749.227783203125
[Actor] Episode 0 Step 2 Loss: 2684.07470703125
[Actor] Episode 0 Step 3 Loss: 2810.247314453125
[Actor] Episode 0 Step 4 Loss: 2853.66943359375
[Actor] Episode 0 Step 5 Loss: 2656.962890625
[Actor] Episode 0 Step 6 Loss: 2715.443115234375
[Actor] Episode 0 Step 7 Loss: 2618.781494140625
[Actor] Episode 0 Step 8 Loss: 2865.5439453125
[Actor] Episode 0 Step 9 Loss: 2783.814453125
[Actor] Episode 0 Step 10 Loss: 2734.169677734375
[Actor] Episode 0 Step 11 Loss: 2858.233154296875
[Actor] Episode 0 Step 12 Loss: 2641.765625
[Actor] Episode 0 Step 13 Loss: 2663.788330078125
[Actor] Episode 0 Step 14 Loss: 2383.897216796875
[Actor] Episode 0 Step 15 Loss: 2584.820068359375
[Actor] Episode 0 Step 16 Loss: 2468.5341796875
[Actor] Episode 0 Step 17 Loss: 2010.054443359375
[Actor] Episode 0 Step 18 Loss: 2041.7403564453125
[Actor] Episode 0 Step 19 Loss: 2323.04541015625
[Actor] Episode 0 Step 20 Loss: 2020.702392578125
[Actor] Episode 0 Step 21 Loss: 2194.18505859375
[Actor] Episode 0 Step 22 Loss: 2210.07080078125
[Actor] Episode 0 Step 23 Loss: 2209.100830078125
[Actor] Episode 0 Step 24 Loss: 1851.859375
[Actor] Episode 0 Step 25 Loss: 2194.88916015625
[Actor] Episode 0 Step 26 Loss: 2097.157958984375
-------------------------------------------------
[Actor] Episode 0 Average Loss: 2484.244127061632
[Actor] Learning Rate: 0.009963428601622581
[Actor] Epoch Time: 767.145920753479s
-------------------------------------------------
[Actor] Starting Episode 1
2021-03-28 11:54:47.540371: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 793800000 exceeds 10% of free system memory.
[Actor] Episode 1 Step 0 Loss: 2019.28466796875
[Actor] Episode 1 Step 1 Loss: 1879.214111328125
[Actor] Episode 1 Step 2 Loss: 1865.2823486328125
[Actor] Episode 1 Step 3 Loss: 1796.85498046875
[Actor] Episode 1 Step 4 Loss: 1831.8780517578125
[Actor] Episode 1 Step 5 Loss: 1967.5897216796875
[Actor] Episode 1 Step 6 Loss: 1746.08203125
[Actor] Episode 1 Step 7 Loss: 1878.7342529296875
2021-03-28 11:59:28.655951: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
Unable to init server: Could not connect: Connection refused
Unable to init server: Could not connect: Connection refused

(learn.py:2973): Gdk-CRITICAL **: 11:59:31.821: gdk_cursor_new_for_display: assertion 'GDK_IS_DISPLAY (display)' failed
2021-03-28 11:59:32.060077: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-28 11:59:32.061345: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-03-28 11:59:32.088952: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-28 11:59:32.089734: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:00:1e.0 name: Tesla T4 computeCapability: 7.5
coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s
2021-03-28 11:59:32.089795: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-28 11:59:32.093813: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-28 11:59:32.093944: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-28 11:59:32.095438: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-03-28 11:59:32.095854: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-03-28 11:59:32.099777: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-03-28 11:59:32.100662: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-03-28 11:59:32.100961: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-28 11:59:32.101198: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-28 11:59:32.102166: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-28 11:59:32.102833: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
[DDPG] Building the actor model
2021-03-28 11:59:32.328659: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-03-28 11:59:32.328929: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-28 11:59:32.329244: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-28 11:59:32.330073: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:00:1e.0 name: Tesla T4 computeCapability: 7.5
coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s
2021-03-28 11:59:32.330112: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-28 11:59:32.330311: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-28 11:59:32.330377: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-28 11:59:32.330436: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-03-28 11:59:32.330483: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-03-28 11:59:32.330521: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-03-28 11:59:32.330571: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-03-28 11:59:32.330625: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-28 11:59:32.330826: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-28 11:59:32.331706: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-28 11:59:32.332553: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-03-28 11:59:32.332618: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-28 11:59:33.146437: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-03-28 11:59:33.146493: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2021-03-28 11:59:33.146507: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2021-03-28 11:59:33.146895: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-28 11:59:33.147920: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-28 11:59:33.148764: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-28 11:59:33.149398: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11669 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5)
[DDPG] Building the actor model
[DDPG] Building the critic model
[DDPG] Building the critic model
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1616932776.817772143, 926.568000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1616932776.818922231, 926.568000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1616932776.818980521, 926.568000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1616932778.160935223, 927.728000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1616932779.089558895, 928.601000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1616932779.938594361, 929.400000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1616932781.019492990, 930.401000000]: Ready to take commands for planning group back_left_leg.[0m
2021-03-28 11:59:50.460064: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 793800000 exceeds 10% of free system memory.
2021-03-28 11:59:54.641375: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 793800000 exceeds 10% of free system memory.
[Actor] GPU>>>>>>>>>>>>
[Actor] [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
[Actor] Memory Growth Allowed
2021-03-28 12:00:01.446880: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 793800000 exceeds 10% of free system memory.
[Actor] Dataset <PrefetchDataset shapes: (((1125, 6), (1125, 34), (1125, 60)), ((1125, 350, 12), (1125, 1), (1125, 12), (1125, 12))), types: ((tf.float32, tf.float32, tf.float64), (tf.float32, tf.float32, tf.float64, tf.float32))>
[Actor] Starting Actor Pretraining
[Actor] Starting Episode 0
2021-03-28 12:00:05.679465: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 793800000 exceeds 10% of free system memory.
2021-03-28 12:00:08.165198: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-28 12:00:08.979573: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
[Actor] Episode 0 Step 0 Loss: 2749.76806640625
[Actor] Episode 0 Step 1 Loss: 2791.765869140625
[Actor] Episode 0 Step 2 Loss: 2523.808349609375
[Actor] Episode 0 Step 3 Loss: 2706.363037109375
[Actor] Episode 0 Step 4 Loss: 2833.839111328125
[Actor] Episode 0 Step 5 Loss: 2590.216064453125
[Actor] Episode 0 Step 6 Loss: 2686.11572265625
[Actor] Episode 0 Step 7 Loss: 2914.583740234375
[Actor] Episode 0 Step 8 Loss: 2812.360595703125
[Actor] Episode 0 Step 9 Loss: 2519.41943359375
[Actor] Episode 0 Step 10 Loss: 2727.81689453125
[Actor] Episode 0 Step 11 Loss: 2764.111572265625
[Actor] Episode 0 Step 12 Loss: 2719.081298828125
[Actor] Episode 0 Step 13 Loss: 2967.973876953125
[Actor] Episode 0 Step 14 Loss: 2783.568603515625
[Actor] Episode 0 Step 15 Loss: 2617.1796875
[Actor] Episode 0 Step 16 Loss: 2906.818115234375
[Actor] Episode 0 Step 17 Loss: 2467.799072265625
[Actor] Episode 0 Step 18 Loss: 2620.076171875
[Actor] Episode 0 Step 19 Loss: 2774.262451171875
[Actor] Episode 0 Step 20 Loss: 3000.611572265625
[Actor] Episode 0 Step 21 Loss: 2701.98095703125
[Actor] Episode 0 Step 22 Loss: 2786.019287109375
[Actor] Episode 0 Step 23 Loss: 2859.09423828125
[Actor] Episode 0 Step 24 Loss: 2930.339599609375
[Actor] Episode 0 Step 25 Loss: 2922.81005859375
[Actor] Episode 0 Step 26 Loss: 3066.473876953125
-------------------------------------------------
[Actor] Episode 0 Average Loss: 2768.3058268229165
[Actor] Learning Rate: 0.009963428601622581
[Actor] Epoch Time: 1435.172872543335s
-------------------------------------------------
[Actor] Starting Episode 1
2021-03-28 12:24:01.132562: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 793800000 exceeds 10% of free system memory.
[Actor] Episode 1 Step 0 Loss: 2832.559814453125
[Actor] Episode 1 Step 1 Loss: 2793.798095703125
[Actor] Episode 1 Step 2 Loss: 2839.17431640625
[Actor] Episode 1 Step 3 Loss: 2915.036376953125
[Actor] Episode 1 Step 4 Loss: 2718.97802734375
[Actor] Episode 1 Step 5 Loss: 2868.581787109375
[Actor] Episode 1 Step 6 Loss: 2809.74365234375
[Actor] Episode 1 Step 7 Loss: 2635.20947265625
[Actor] Episode 1 Step 8 Loss: 2698.380126953125
[Actor] Episode 1 Step 9 Loss: 2946.314208984375
[Actor] Episode 1 Step 10 Loss: 2683.8466796875
[Actor] Episode 1 Step 11 Loss: 2702.648193359375
[Actor] Episode 1 Step 12 Loss: 2785.694580078125
[Actor] Episode 1 Step 13 Loss: 2683.836181640625
[Actor] Episode 1 Step 14 Loss: 2582.32177734375
[Actor] Episode 1 Step 15 Loss: 2853.491455078125
[Actor] Episode 1 Step 16 Loss: 2954.59228515625
[Actor] Episode 1 Step 17 Loss: 2706.861328125
[Actor] Episode 1 Step 18 Loss: 2591.8544921875
[Actor] Episode 1 Step 19 Loss: 2756.06494140625
[Actor] Episode 1 Step 20 Loss: 2750.972900390625
[Actor] Episode 1 Step 21 Loss: 2896.47802734375
[Actor] Episode 1 Step 22 Loss: 2693.075927734375
[Actor] Episode 1 Step 23 Loss: 2870.718017578125
[Actor] Episode 1 Step 24 Loss: 2765.13134765625
[Actor] Episode 1 Step 25 Loss: 2852.736083984375
[Actor] Episode 1 Step 26 Loss: 2703.322021484375
-------------------------------------------------
[Actor] Episode 1 Average Loss: 2773.7563747829863
[Actor] Learning Rate: 0.009926991537213326
[Actor] Epoch Time: 1591.4155147075653s
-------------------------------------------------
[Actor] Starting Episode 2
[Actor] Episode 2 Step 0 Loss: 2910.014404296875
[Actor] Episode 2 Step 1 Loss: 2773.298095703125
[Actor] Episode 2 Step 2 Loss: 2698.11181640625
[Actor] Episode 2 Step 3 Loss: 2807.66064453125
[Actor] Episode 2 Step 4 Loss: 2787.944580078125
[Actor] Episode 2 Step 5 Loss: 2551.588134765625
