2021-03-28 11:41:22.191498: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
Unable to init server: Could not connect: Connection refused
Unable to init server: Could not connect: Connection refused

(learn.py:2416): Gdk-CRITICAL **: 11:41:25.467: gdk_cursor_new_for_display: assertion 'GDK_IS_DISPLAY (display)' failed
2021-03-28 11:41:25.754665: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-28 11:41:25.755982: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-03-28 11:41:25.791782: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-28 11:41:25.793318: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:00:1e.0 name: Tesla T4 computeCapability: 7.5
coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s
2021-03-28 11:41:25.793354: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-28 11:41:25.798828: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-28 11:41:25.798981: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-28 11:41:25.800613: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-03-28 11:41:25.801314: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-03-28 11:41:25.807206: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-03-28 11:41:25.808690: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-03-28 11:41:25.809293: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-28 11:41:25.809668: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-28 11:41:25.811228: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-28 11:41:25.812780: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
[DDPG] Building the actor model
2021-03-28 11:41:26.084276: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-03-28 11:41:26.084508: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-28 11:41:26.084846: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-28 11:41:26.085642: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:00:1e.0 name: Tesla T4 computeCapability: 7.5
coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s
2021-03-28 11:41:26.085680: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-28 11:41:26.085900: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-28 11:41:26.086009: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-28 11:41:26.086070: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-03-28 11:41:26.086119: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-03-28 11:41:26.086166: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-03-28 11:41:26.086214: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-03-28 11:41:26.086257: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-28 11:41:26.086445: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-28 11:41:26.087308: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-28 11:41:26.088048: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-03-28 11:41:26.088095: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-28 11:41:26.954635: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-03-28 11:41:26.954675: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2021-03-28 11:41:26.954683: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2021-03-28 11:41:26.955055: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-28 11:41:26.956073: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-28 11:41:26.956965: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-28 11:41:26.957679: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11669 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5)
[DDPG] Building the actor model
[DDPG] Building the critic model
[DDPG] Building the critic model
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1616931690.627168542, 64.565000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1616931690.628081550, 64.566000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1616931690.628247926, 64.566000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1616931692.010566821, 65.865000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1616931692.871735651, 66.601000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1616931693.716025345, 67.399000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1616931694.566780017, 68.201000000]: Ready to take commands for planning group back_left_leg.[0m
2021-03-28 11:41:45.126289: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 793800000 exceeds 10% of free system memory.
2021-03-28 11:41:49.365638: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 793800000 exceeds 10% of free system memory.
[Actor] GPU>>>>>>>>>>>>
[Actor] [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
[Actor] Memory Growth Allowed
2021-03-28 11:41:56.022177: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 793800000 exceeds 10% of free system memory.
[Actor] Dataset <PrefetchDataset shapes: (((1125, 6), (1125, 34), (1125, 60)), ((1125, 350, 12), (1125, 1), (1125, 12), (1125, 12))), types: ((tf.float32, tf.float32, tf.float64), (tf.float32, tf.float32, tf.float64, tf.float32))>
[Actor] Starting Actor Pretraining
[Actor] Starting Episode 0
2021-03-28 11:42:00.149961: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 793800000 exceeds 10% of free system memory.
2021-03-28 11:42:02.802999: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-28 11:42:03.682330: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
[Actor] Episode 0 Step 0 Loss: 2848.812255859375
[Actor] Episode 0 Step 1 Loss: 2749.227783203125
[Actor] Episode 0 Step 2 Loss: 2684.07470703125
[Actor] Episode 0 Step 3 Loss: 2810.247314453125
[Actor] Episode 0 Step 4 Loss: 2853.66943359375
[Actor] Episode 0 Step 5 Loss: 2656.962890625
[Actor] Episode 0 Step 6 Loss: 2715.443115234375
[Actor] Episode 0 Step 7 Loss: 2618.781494140625
[Actor] Episode 0 Step 8 Loss: 2865.5439453125
[Actor] Episode 0 Step 9 Loss: 2783.814453125
[Actor] Episode 0 Step 10 Loss: 2734.169677734375
[Actor] Episode 0 Step 11 Loss: 2858.233154296875
[Actor] Episode 0 Step 12 Loss: 2641.765625
[Actor] Episode 0 Step 13 Loss: 2663.788330078125
[Actor] Episode 0 Step 14 Loss: 2383.897216796875
[Actor] Episode 0 Step 15 Loss: 2584.820068359375
[Actor] Episode 0 Step 16 Loss: 2468.5341796875
[Actor] Episode 0 Step 17 Loss: 2010.054443359375
[Actor] Episode 0 Step 18 Loss: 2041.7403564453125
[Actor] Episode 0 Step 19 Loss: 2323.04541015625
[Actor] Episode 0 Step 20 Loss: 2020.702392578125
[Actor] Episode 0 Step 21 Loss: 2194.18505859375
[Actor] Episode 0 Step 22 Loss: 2210.07080078125
[Actor] Episode 0 Step 23 Loss: 2209.100830078125
[Actor] Episode 0 Step 24 Loss: 1851.859375
[Actor] Episode 0 Step 25 Loss: 2194.88916015625
[Actor] Episode 0 Step 26 Loss: 2097.157958984375
-------------------------------------------------
[Actor] Episode 0 Average Loss: 2484.244127061632
[Actor] Learning Rate: 0.009963428601622581
[Actor] Epoch Time: 767.145920753479s
-------------------------------------------------
[Actor] Starting Episode 1
2021-03-28 11:54:47.540371: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 793800000 exceeds 10% of free system memory.
[Actor] Episode 1 Step 0 Loss: 2019.28466796875
[Actor] Episode 1 Step 1 Loss: 1879.214111328125
[Actor] Episode 1 Step 2 Loss: 1865.2823486328125
[Actor] Episode 1 Step 3 Loss: 1796.85498046875
[Actor] Episode 1 Step 4 Loss: 1831.8780517578125
[Actor] Episode 1 Step 5 Loss: 1967.5897216796875
[Actor] Episode 1 Step 6 Loss: 1746.08203125
[Actor] Episode 1 Step 7 Loss: 1878.7342529296875
2021-03-28 11:59:28.655951: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
Unable to init server: Could not connect: Connection refused
Unable to init server: Could not connect: Connection refused

(learn.py:2973): Gdk-CRITICAL **: 11:59:31.821: gdk_cursor_new_for_display: assertion 'GDK_IS_DISPLAY (display)' failed
2021-03-28 11:59:32.060077: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-28 11:59:32.061345: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-03-28 11:59:32.088952: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-28 11:59:32.089734: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:00:1e.0 name: Tesla T4 computeCapability: 7.5
coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s
2021-03-28 11:59:32.089795: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-28 11:59:32.093813: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-28 11:59:32.093944: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-28 11:59:32.095438: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-03-28 11:59:32.095854: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-03-28 11:59:32.099777: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-03-28 11:59:32.100662: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-03-28 11:59:32.100961: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-28 11:59:32.101198: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-28 11:59:32.102166: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-28 11:59:32.102833: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
[DDPG] Building the actor model
2021-03-28 11:59:32.328659: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-03-28 11:59:32.328929: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-28 11:59:32.329244: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-28 11:59:32.330073: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:00:1e.0 name: Tesla T4 computeCapability: 7.5
coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s
2021-03-28 11:59:32.330112: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-28 11:59:32.330311: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-28 11:59:32.330377: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-28 11:59:32.330436: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-03-28 11:59:32.330483: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-03-28 11:59:32.330521: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-03-28 11:59:32.330571: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-03-28 11:59:32.330625: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-28 11:59:32.330826: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-28 11:59:32.331706: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-28 11:59:32.332553: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-03-28 11:59:32.332618: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-28 11:59:33.146437: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-03-28 11:59:33.146493: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2021-03-28 11:59:33.146507: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2021-03-28 11:59:33.146895: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-28 11:59:33.147920: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-28 11:59:33.148764: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-28 11:59:33.149398: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11669 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5)
[DDPG] Building the actor model
[DDPG] Building the critic model
[DDPG] Building the critic model
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1616932776.817772143, 926.568000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1616932776.818922231, 926.568000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1616932776.818980521, 926.568000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1616932778.160935223, 927.728000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1616932779.089558895, 928.601000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1616932779.938594361, 929.400000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1616932781.019492990, 930.401000000]: Ready to take commands for planning group back_left_leg.[0m
2021-03-28 11:59:50.460064: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 793800000 exceeds 10% of free system memory.
2021-03-28 11:59:54.641375: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 793800000 exceeds 10% of free system memory.
[Actor] GPU>>>>>>>>>>>>
[Actor] [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
[Actor] Memory Growth Allowed
2021-03-28 12:00:01.446880: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 793800000 exceeds 10% of free system memory.
[Actor] Dataset <PrefetchDataset shapes: (((1125, 6), (1125, 34), (1125, 60)), ((1125, 350, 12), (1125, 1), (1125, 12), (1125, 12))), types: ((tf.float32, tf.float32, tf.float64), (tf.float32, tf.float32, tf.float64, tf.float32))>
[Actor] Starting Actor Pretraining
[Actor] Starting Episode 0
2021-03-28 12:00:05.679465: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 793800000 exceeds 10% of free system memory.
2021-03-28 12:00:08.165198: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-28 12:00:08.979573: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
[Actor] Episode 0 Step 0 Loss: 2749.76806640625
[Actor] Episode 0 Step 1 Loss: 2791.765869140625
[Actor] Episode 0 Step 2 Loss: 2523.808349609375
[Actor] Episode 0 Step 3 Loss: 2706.363037109375
[Actor] Episode 0 Step 4 Loss: 2833.839111328125
[Actor] Episode 0 Step 5 Loss: 2590.216064453125
[Actor] Episode 0 Step 6 Loss: 2686.11572265625
[Actor] Episode 0 Step 7 Loss: 2914.583740234375
[Actor] Episode 0 Step 8 Loss: 2812.360595703125
[Actor] Episode 0 Step 9 Loss: 2519.41943359375
[Actor] Episode 0 Step 10 Loss: 2727.81689453125
[Actor] Episode 0 Step 11 Loss: 2764.111572265625
[Actor] Episode 0 Step 12 Loss: 2719.081298828125
[Actor] Episode 0 Step 13 Loss: 2967.973876953125
[Actor] Episode 0 Step 14 Loss: 2783.568603515625
[Actor] Episode 0 Step 15 Loss: 2617.1796875
[Actor] Episode 0 Step 16 Loss: 2906.818115234375
[Actor] Episode 0 Step 17 Loss: 2467.799072265625
[Actor] Episode 0 Step 18 Loss: 2620.076171875
[Actor] Episode 0 Step 19 Loss: 2774.262451171875
[Actor] Episode 0 Step 20 Loss: 3000.611572265625
[Actor] Episode 0 Step 21 Loss: 2701.98095703125
[Actor] Episode 0 Step 22 Loss: 2786.019287109375
[Actor] Episode 0 Step 23 Loss: 2859.09423828125
[Actor] Episode 0 Step 24 Loss: 2930.339599609375
[Actor] Episode 0 Step 25 Loss: 2922.81005859375
[Actor] Episode 0 Step 26 Loss: 3066.473876953125
-------------------------------------------------
[Actor] Episode 0 Average Loss: 2768.3058268229165
[Actor] Learning Rate: 0.009963428601622581
[Actor] Epoch Time: 1435.172872543335s
-------------------------------------------------
[Actor] Starting Episode 1
2021-03-28 12:24:01.132562: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 793800000 exceeds 10% of free system memory.
[Actor] Episode 1 Step 0 Loss: 2832.559814453125
[Actor] Episode 1 Step 1 Loss: 2793.798095703125
[Actor] Episode 1 Step 2 Loss: 2839.17431640625
[Actor] Episode 1 Step 3 Loss: 2915.036376953125
[Actor] Episode 1 Step 4 Loss: 2718.97802734375
[Actor] Episode 1 Step 5 Loss: 2868.581787109375
[Actor] Episode 1 Step 6 Loss: 2809.74365234375
[Actor] Episode 1 Step 7 Loss: 2635.20947265625
[Actor] Episode 1 Step 8 Loss: 2698.380126953125
[Actor] Episode 1 Step 9 Loss: 2946.314208984375
[Actor] Episode 1 Step 10 Loss: 2683.8466796875
[Actor] Episode 1 Step 11 Loss: 2702.648193359375
[Actor] Episode 1 Step 12 Loss: 2785.694580078125
[Actor] Episode 1 Step 13 Loss: 2683.836181640625
[Actor] Episode 1 Step 14 Loss: 2582.32177734375
[Actor] Episode 1 Step 15 Loss: 2853.491455078125
[Actor] Episode 1 Step 16 Loss: 2954.59228515625
[Actor] Episode 1 Step 17 Loss: 2706.861328125
[Actor] Episode 1 Step 18 Loss: 2591.8544921875
[Actor] Episode 1 Step 19 Loss: 2756.06494140625
[Actor] Episode 1 Step 20 Loss: 2750.972900390625
[Actor] Episode 1 Step 21 Loss: 2896.47802734375
[Actor] Episode 1 Step 22 Loss: 2693.075927734375
[Actor] Episode 1 Step 23 Loss: 2870.718017578125
[Actor] Episode 1 Step 24 Loss: 2765.13134765625
[Actor] Episode 1 Step 25 Loss: 2852.736083984375
[Actor] Episode 1 Step 26 Loss: 2703.322021484375
-------------------------------------------------
[Actor] Episode 1 Average Loss: 2773.7563747829863
[Actor] Learning Rate: 0.009926991537213326
[Actor] Epoch Time: 1591.4155147075653s
-------------------------------------------------
[Actor] Starting Episode 2
[Actor] Episode 2 Step 0 Loss: 2910.014404296875
[Actor] Episode 2 Step 1 Loss: 2773.298095703125
[Actor] Episode 2 Step 2 Loss: 2698.11181640625
[Actor] Episode 2 Step 3 Loss: 2807.66064453125
[Actor] Episode 2 Step 4 Loss: 2787.944580078125
[Actor] Episode 2 Step 5 Loss: 2551.588134765625
[Actor] Episode 2 Step 6 Loss: 2806.539306640625
[Actor] Episode 2 Step 7 Loss: 2636.409423828125
[Actor] Episode 2 Step 8 Loss: 2557.68408203125
[Actor] Episode 2 Step 9 Loss: 2966.904296875
[Actor] Episode 2 Step 10 Loss: 2582.015625
[Actor] Episode 2 Step 11 Loss: 2623.51220703125
[Actor] Episode 2 Step 12 Loss: 2695.220947265625
[Actor] Episode 2 Step 13 Loss: 2805.454345703125
[Actor] Episode 2 Step 14 Loss: 2715.35546875
[Actor] Episode 2 Step 15 Loss: 2903.230712890625
[Actor] Episode 2 Step 16 Loss: 3004.494873046875
[Actor] Episode 2 Step 17 Loss: 2823.7646484375
[Actor] Episode 2 Step 18 Loss: 2821.469970703125
[Actor] Episode 2 Step 19 Loss: 2901.77099609375
[Actor] Episode 2 Step 20 Loss: 2646.19287109375
[Actor] Episode 2 Step 21 Loss: 2789.963623046875
[Actor] Episode 2 Step 22 Loss: 2923.177490234375
[Actor] Episode 2 Step 23 Loss: 2879.4453125
[Actor] Episode 2 Step 24 Loss: 2637.694580078125
[Actor] Episode 2 Step 25 Loss: 2832.12353515625
[Actor] Episode 2 Step 26 Loss: 2808.209716796875
-------------------------------------------------
[Actor] Episode 2 Average Loss: 2773.6759892216437
[Actor] Learning Rate: 0.00989068765193224
[Actor] Epoch Time: 1563.365995168686s
-------------------------------------------------
[Actor] Starting Episode 3
[Actor] Episode 3 Step 0 Loss: 2874.5869140625
[Actor] Episode 3 Step 1 Loss: 2707.513916015625
[Actor] Episode 3 Step 2 Loss: 2754.878662109375
2021-03-29 08:18:35.219953: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
Unable to init server: Could not connect: Connection refused
Unable to init server: Could not connect: Connection refused

(learn.py:7326): Gdk-CRITICAL **: 08:18:38.317: gdk_cursor_new_for_display: assertion 'GDK_IS_DISPLAY (display)' failed
2021-03-29 08:18:38.551398: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-29 08:18:38.552537: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-03-29 08:18:38.581758: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-29 08:18:38.582549: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:00:1e.0 name: Tesla T4 computeCapability: 7.5
coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s
2021-03-29 08:18:38.582585: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-29 08:18:38.586084: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-29 08:18:38.586243: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-29 08:18:38.587934: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-03-29 08:18:38.588295: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-03-29 08:18:38.592373: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-03-29 08:18:38.593343: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-03-29 08:18:38.593611: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-29 08:18:38.593823: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-29 08:18:38.594684: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-29 08:18:38.595369: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
[DDPG] Building the actor model
2021-03-29 08:18:38.812431: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-03-29 08:18:38.812654: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-29 08:18:38.812966: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-29 08:18:38.813709: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:00:1e.0 name: Tesla T4 computeCapability: 7.5
coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s
2021-03-29 08:18:38.813743: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-29 08:18:38.813938: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-29 08:18:38.814013: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-29 08:18:38.814061: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-03-29 08:18:38.814109: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-03-29 08:18:38.814184: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-03-29 08:18:38.814262: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-03-29 08:18:38.814326: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-29 08:18:38.814513: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-29 08:18:38.815305: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-29 08:18:38.815923: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-03-29 08:18:38.815970: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-29 08:18:39.615861: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-03-29 08:18:39.615904: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2021-03-29 08:18:39.615915: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2021-03-29 08:18:39.616322: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-29 08:18:39.617338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-29 08:18:39.618239: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-29 08:18:39.618991: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11669 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5)
[DDPG] Building the actor model
[DDPG] Building the critic model
[DDPG] Building the critic model
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1617005923.209417239, 11017.290000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1617005923.210465640, 11017.291000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1617005923.210535501, 11017.291000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1617005924.588069469, 11018.608000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1617005924.910168396, 11018.921000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1617005925.324067567, 11019.323000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1617005925.741460813, 11019.722000000]: Ready to take commands for planning group back_left_leg.[0m
2021-03-29 08:18:55.736477: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 793800000 exceeds 10% of free system memory.
2021-03-29 08:18:59.949975: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 793800000 exceeds 10% of free system memory.
[Actor] GPU>>>>>>>>>>>>
[Actor] [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
[Actor] Memory Growth Allowed
[DDPG] Loading Actor Weights
2021-03-29 08:19:06.787931: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 793800000 exceeds 10% of free system memory.
[Actor] Dataset <PrefetchDataset shapes: (((1125, 6), (1125, 34), (1125, 60)), ((1125, 350, 12), (1125, 1), (1125, 12), (1125, 12))), types: ((tf.float32, tf.float32, tf.float64), (tf.float32, tf.float32, tf.float64, tf.float32))>
[Actor] Starting Actor Pretraining
[Actor] Starting Episode 0
2021-03-29 08:19:10.756377: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 793800000 exceeds 10% of free system memory.
2021-03-29 08:19:13.964936: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-29 08:19:15.725289: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
[Actor] Episode 0 Step 0 Loss: 922.55712890625
[Actor] Episode 0 Step 1 Loss: 1110.0494384765625
[Actor] Episode 0 Step 2 Loss: 962.4906005859375
[Actor] Episode 0 Step 3 Loss: 1037.127685546875
[Actor] Episode 0 Step 4 Loss: 1090.3111572265625
[Actor] Episode 0 Step 5 Loss: 940.427734375
[Actor] Episode 0 Step 6 Loss: 994.2732543945312
[Actor] Episode 0 Step 7 Loss: 1100.5479736328125
[Actor] Episode 0 Step 8 Loss: 999.9473876953125
[Actor] Episode 0 Step 9 Loss: 1147.276611328125
[Actor] Episode 0 Step 10 Loss: 1073.763916015625
[Actor] Episode 0 Step 11 Loss: 1077.286376953125
[Actor] Episode 0 Step 12 Loss: 983.427001953125
[Actor] Episode 0 Step 13 Loss: 869.8671264648438
[Actor] Episode 0 Step 14 Loss: 990.9269409179688
[Actor] Episode 0 Step 15 Loss: 993.850830078125
[Actor] Episode 0 Step 16 Loss: 982.8817138671875
[Actor] Episode 0 Step 17 Loss: 946.7070922851562
[Actor] Episode 0 Step 18 Loss: 898.4057006835938
[Actor] Episode 0 Step 19 Loss: 1021.8389282226562
[Actor] Episode 0 Step 20 Loss: 1009.8709716796875
[Actor] Episode 0 Step 21 Loss: 1005.525146484375
[Actor] Episode 0 Step 22 Loss: 1031.5926513671875
[Actor] Episode 0 Step 23 Loss: 976.8894653320312
[Actor] Episode 0 Step 24 Loss: 830.030029296875
[Actor] Episode 0 Step 25 Loss: 1013.2974243164062
[Actor] Episode 0 Step 26 Loss: 1007.4910278320312
-------------------------------------------------
[Actor] Episode 0 Average Loss: 1000.6911598488136
[Actor] Learning Rate: 0.004981714300811291
[Actor] Epoch Time: 1332.5364980697632s
-------------------------------------------------
[Actor] Starting Episode 1
2021-03-29 08:41:23.538836: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 793800000 exceeds 10% of free system memory.
[Actor] Episode 1 Step 0 Loss: 1084.8223876953125
[Actor] Episode 1 Step 1 Loss: 1025.481689453125
[Actor] Episode 1 Step 2 Loss: 749.9237670898438
[Actor] Episode 1 Step 3 Loss: 911.3323974609375
[Actor] Episode 1 Step 4 Loss: 840.2367553710938
[Actor] Episode 1 Step 5 Loss: 735.582275390625
[Actor] Episode 1 Step 6 Loss: 827.0152587890625
[Actor] Episode 1 Step 7 Loss: 846.773193359375
[Actor] Episode 1 Step 8 Loss: 793.234619140625
[Actor] Episode 1 Step 9 Loss: 800.1885375976562
[Actor] Episode 1 Step 10 Loss: 777.3560791015625
[Actor] Episode 1 Step 11 Loss: 744.7805786132812
[Actor] Episode 1 Step 12 Loss: 772.7877197265625
[Actor] Episode 1 Step 13 Loss: 708.6890869140625
[Actor] Episode 1 Step 14 Loss: 684.2032470703125
[Actor] Episode 1 Step 15 Loss: 749.11376953125
[Actor] Episode 1 Step 16 Loss: 745.18701171875
[Actor] Episode 1 Step 17 Loss: 955.134765625
[Actor] Episode 1 Step 18 Loss: 713.7911987304688
[Actor] Episode 1 Step 19 Loss: 655.87744140625
[Actor] Episode 1 Step 20 Loss: 744.4445190429688
[Actor] Episode 1 Step 21 Loss: 604.1171875
[Actor] Episode 1 Step 22 Loss: 739.0285034179688
[Actor] Episode 1 Step 23 Loss: 675.4556884765625
[Actor] Episode 1 Step 24 Loss: 760.0256958007812
[Actor] Episode 1 Step 25 Loss: 622.2909545898438
[Actor] Episode 1 Step 26 Loss: 568.6359252929688
-------------------------------------------------
[Actor] Episode 1 Average Loss: 771.6855649594908
[Actor] Learning Rate: 0.004963495768606663
[Actor] Epoch Time: 1103.6341533660889s
-------------------------------------------------
[Actor] Starting Episode 2
[Actor] Episode 2 Step 0 Loss: 646.5167236328125
[Actor] Episode 2 Step 1 Loss: 667.2559204101562
[Actor] Episode 2 Step 2 Loss: 539.2051391601562
[Actor] Episode 2 Step 3 Loss: 613.5543212890625
[Actor] Episode 2 Step 4 Loss: 604.172119140625
[Actor] Episode 2 Step 5 Loss: 558.7661743164062
[Actor] Episode 2 Step 6 Loss: 648.2371215820312
[Actor] Episode 2 Step 7 Loss: 590.1871948242188
[Actor] Episode 2 Step 8 Loss: 662.66748046875
[Actor] Episode 2 Step 9 Loss: 622.0565795898438
[Actor] Episode 2 Step 10 Loss: 568.7496337890625
[Actor] Episode 2 Step 11 Loss: 560.2160034179688
[Actor] Episode 2 Step 12 Loss: 601.9261474609375
[Actor] Episode 2 Step 13 Loss: 634.834716796875
[Actor] Episode 2 Step 14 Loss: 596.4683227539062
[Actor] Episode 2 Step 15 Loss: 679.5610961914062
[Actor] Episode 2 Step 16 Loss: 543.192138671875
[Actor] Episode 2 Step 17 Loss: 637.9518432617188
[Actor] Episode 2 Step 18 Loss: 599.2013549804688
[Actor] Episode 2 Step 19 Loss: 648.8339233398438
[Actor] Episode 2 Step 20 Loss: 570.4420776367188
[Actor] Episode 2 Step 21 Loss: 580.4002075195312
[Actor] Episode 2 Step 22 Loss: 572.0263671875
[Actor] Episode 2 Step 23 Loss: 505.4744567871094
[Actor] Episode 2 Step 24 Loss: 572.0729370117188
[Actor] Episode 2 Step 25 Loss: 627.427001953125
[Actor] Episode 2 Step 26 Loss: 530.3466186523438
-------------------------------------------------
[Actor] Episode 2 Average Loss: 599.3238378454138
[Actor] Learning Rate: 0.00494534382596612
[Actor] Epoch Time: 1097.8061077594757s
-------------------------------------------------
[Actor] Starting Episode 3
[Actor] Episode 3 Step 0 Loss: 530.7025146484375
[Actor] Episode 3 Step 1 Loss: 599.7636108398438
[Actor] Episode 3 Step 2 Loss: 569.20263671875
[Actor] Episode 3 Step 3 Loss: 631.2423095703125
[Actor] Episode 3 Step 4 Loss: 584.8176879882812
[Actor] Episode 3 Step 5 Loss: 533.881103515625
[Actor] Episode 3 Step 6 Loss: 568.3916625976562
[Actor] Episode 3 Step 7 Loss: 594.7083129882812
[Actor] Episode 3 Step 8 Loss: 484.2611389160156
[Actor] Episode 3 Step 9 Loss: 548.059814453125
[Actor] Episode 3 Step 10 Loss: 537.8579711914062
[Actor] Episode 3 Step 11 Loss: 499.1628723144531
[Actor] Episode 3 Step 12 Loss: 539.0870361328125
[Actor] Episode 3 Step 13 Loss: 504.0243835449219
[Actor] Episode 3 Step 14 Loss: 565.007080078125
[Actor] Episode 3 Step 15 Loss: 506.9530029296875
[Actor] Episode 3 Step 16 Loss: 520.5098266601562
[Actor] Episode 3 Step 17 Loss: 518.0407104492188
[Actor] Episode 3 Step 18 Loss: 518.8369750976562
[Actor] Episode 3 Step 19 Loss: 467.15869140625
[Actor] Episode 3 Step 20 Loss: 479.6900329589844
[Actor] Episode 3 Step 21 Loss: 467.7575988769531
[Actor] Episode 3 Step 22 Loss: 503.44488525390625
[Actor] Episode 3 Step 23 Loss: 534.0482788085938
[Actor] Episode 3 Step 24 Loss: 595.5632934570312
[Actor] Episode 3 Step 25 Loss: 497.2752685546875
[Actor] Episode 3 Step 26 Loss: 506.3594055175781
-------------------------------------------------
[Actor] Episode 3 Average Loss: 533.5484483506945
[Actor] Learning Rate: 0.0049272580072283745
[Actor] Epoch Time: 1102.3423342704773s
-------------------------------------------------
[Actor] Starting Episode 4
[Actor] Episode 4 Step 0 Loss: 485.18389892578125
[Actor] Episode 4 Step 1 Loss: 594.809326171875
[Actor] Episode 4 Step 2 Loss: 512.5120849609375
[Actor] Episode 4 Step 3 Loss: 491.31884765625
[Actor] Episode 4 Step 4 Loss: 501.4770202636719
[Actor] Episode 4 Step 5 Loss: 427.2281799316406
[Actor] Episode 4 Step 6 Loss: 557.4631958007812
[Actor] Episode 4 Step 7 Loss: 476.1998596191406
[Actor] Episode 4 Step 8 Loss: 450.5523986816406
[Actor] Episode 4 Step 9 Loss: 493.998291015625
[Actor] Episode 4 Step 10 Loss: 510.2301025390625
[Actor] Episode 4 Step 11 Loss: 397.685546875
[Actor] Episode 4 Step 12 Loss: 521.4859619140625
[Actor] Episode 4 Step 13 Loss: 401.20257568359375
[Actor] Episode 4 Step 14 Loss: 458.6385192871094
[Actor] Episode 4 Step 15 Loss: 571.4605712890625
[Actor] Episode 4 Step 16 Loss: 453.5863037109375
[Actor] Episode 4 Step 17 Loss: 431.28985595703125
[Actor] Episode 4 Step 18 Loss: 508.52685546875
[Actor] Episode 4 Step 19 Loss: 410.6875915527344
[Actor] Episode 4 Step 20 Loss: 481.61700439453125
[Actor] Episode 4 Step 21 Loss: 464.7827453613281
[Actor] Episode 4 Step 22 Loss: 533.8269653320312
[Actor] Episode 4 Step 23 Loss: 431.2710266113281
[Actor] Episode 4 Step 24 Loss: 469.4446105957031
[Actor] Episode 4 Step 25 Loss: 413.9860534667969
[Actor] Episode 4 Step 26 Loss: 341.1171875
-------------------------------------------------
[Actor] Episode 4 Average Loss: 473.76231779875576
[Actor] Learning Rate: 0.004909238312393427
[Actor] Epoch Time: 1115.1656606197357s
-------------------------------------------------
[Actor] Starting Episode 5
[Actor] Episode 5 Step 0 Loss: 418.9347839355469
[Actor] Episode 5 Step 1 Loss: 408.12322998046875
[Actor] Episode 5 Step 2 Loss: 420.6216735839844
[Actor] Episode 5 Step 3 Loss: 485.74017333984375
[Actor] Episode 5 Step 4 Loss: 469.2727355957031
[Actor] Episode 5 Step 5 Loss: 466.7715759277344
[Actor] Episode 5 Step 6 Loss: 397.20135498046875
[Actor] Episode 5 Step 7 Loss: 409.0785827636719
[Actor] Episode 5 Step 8 Loss: 471.7648620605469
[Actor] Episode 5 Step 9 Loss: 472.97552490234375
[Actor] Episode 5 Step 10 Loss: 455.3094177246094
[Actor] Episode 5 Step 11 Loss: 441.6053771972656
[Actor] Episode 5 Step 12 Loss: 401.49786376953125
[Actor] Episode 5 Step 13 Loss: 327.2298889160156
[Actor] Episode 5 Step 14 Loss: 392.85443115234375
[Actor] Episode 5 Step 15 Loss: 319.76715087890625
[Actor] Episode 5 Step 16 Loss: 428.7154235839844
[Actor] Episode 5 Step 17 Loss: 352.6739196777344
[Actor] Episode 5 Step 18 Loss: 342.3884582519531
[Actor] Episode 5 Step 19 Loss: 408.8393249511719
[Actor] Episode 5 Step 20 Loss: 476.4503479003906
[Actor] Episode 5 Step 21 Loss: 377.85198974609375
[Actor] Episode 5 Step 22 Loss: 437.1457824707031
[Actor] Episode 5 Step 23 Loss: 429.6679992675781
[Actor] Episode 5 Step 24 Loss: 484.5133056640625
[Actor] Episode 5 Step 25 Loss: 360.8300476074219
[Actor] Episode 5 Step 26 Loss: 355.14361572265625
-------------------------------------------------
[Actor] Episode 5 Average Loss: 415.2951422797309
[Actor] Learning Rate: 0.004891284741461277
[Actor] Epoch Time: 1107.4454081058502s
-------------------------------------------------
[Actor] Starting Episode 6
[Actor] Episode 6 Step 0 Loss: 464.001220703125
[Actor] Episode 6 Step 1 Loss: 335.9092102050781
[Actor] Episode 6 Step 2 Loss: 380.10076904296875
[Actor] Episode 6 Step 3 Loss: 385.9061584472656
[Actor] Episode 6 Step 4 Loss: 300.7880859375
[Actor] Episode 6 Step 5 Loss: 336.2951965332031
[Actor] Episode 6 Step 6 Loss: 361.30950927734375
[Actor] Episode 6 Step 7 Loss: 434.5968322753906
[Actor] Episode 6 Step 8 Loss: 372.6720886230469
[Actor] Episode 6 Step 9 Loss: 386.88458251953125
[Actor] Episode 6 Step 10 Loss: 341.57855224609375
[Actor] Episode 6 Step 11 Loss: 322.4801330566406
[Actor] Episode 6 Step 12 Loss: 364.6712341308594
[Actor] Episode 6 Step 13 Loss: 314.7454528808594
[Actor] Episode 6 Step 14 Loss: 354.07568359375
[Actor] Episode 6 Step 15 Loss: 391.4974670410156
[Actor] Episode 6 Step 16 Loss: 329.0005798339844
[Actor] Episode 6 Step 17 Loss: 353.269287109375
[Actor] Episode 6 Step 18 Loss: 333.46063232421875
[Actor] Episode 6 Step 19 Loss: 306.8728332519531
[Actor] Episode 6 Step 20 Loss: 342.0699768066406
[Actor] Episode 6 Step 21 Loss: 380.6275329589844
[Actor] Episode 6 Step 22 Loss: 343.1167297363281
[Actor] Episode 6 Step 23 Loss: 373.5791931152344
[Actor] Episode 6 Step 24 Loss: 368.1286315917969
[Actor] Episode 6 Step 25 Loss: 367.3037109375
[Actor] Episode 6 Step 26 Loss: 399.9319763183594
-------------------------------------------------
[Actor] Episode 6 Average Loss: 360.921231870298
[Actor] Learning Rate: 0.0048733968287706375
[Actor] Epoch Time: 1092.6821656227112s
-------------------------------------------------
[Actor] Starting Episode 7
[Actor] Episode 7 Step 0 Loss: 369.1820068359375
[Actor] Episode 7 Step 1 Loss: 313.4969177246094
[Actor] Episode 7 Step 2 Loss: nan
[Actor] Episode 7 Step 3 Loss: nan
[Actor] Episode 7 Step 4 Loss: nan
2021-03-29 10:49:16.502350: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
Unable to init server: Could not connect: Connection refused
Unable to init server: Could not connect: Connection refused

(learn.py:8471): Gdk-CRITICAL **: 10:49:23.642: gdk_cursor_new_for_display: assertion 'GDK_IS_DISPLAY (display)' failed
2021-03-29 10:49:24.129484: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-29 10:49:24.131665: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-03-29 10:49:24.200145: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-29 10:49:24.200947: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:00:1e.0 name: Tesla T4 computeCapability: 7.5
coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s
2021-03-29 10:49:24.200993: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-29 10:49:24.225169: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-29 10:49:24.225320: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-29 10:49:24.246124: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-03-29 10:49:24.253202: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-03-29 10:49:24.280939: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-03-29 10:49:24.288146: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-03-29 10:49:24.290419: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-29 10:49:24.290613: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-29 10:49:24.291556: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-29 10:49:24.292350: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
[DDPG] Building the actor model
2021-03-29 10:49:24.482179: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-03-29 10:49:24.482488: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-29 10:49:24.482791: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-29 10:49:24.483722: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:00:1e.0 name: Tesla T4 computeCapability: 7.5
coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s
2021-03-29 10:49:24.483753: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-29 10:49:24.483939: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-29 10:49:24.483989: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-29 10:49:24.484049: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-03-29 10:49:24.484091: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-03-29 10:49:24.484130: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-03-29 10:49:24.484168: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-03-29 10:49:24.484213: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-29 10:49:24.484365: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-29 10:49:24.485308: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-29 10:49:24.486119: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-03-29 10:49:24.486964: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-29 10:49:26.168983: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-03-29 10:49:26.169041: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2021-03-29 10:49:26.169055: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2021-03-29 10:49:26.169393: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-29 10:49:26.170376: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-29 10:49:26.171216: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-29 10:49:26.171858: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13963 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5)
[DDPG] Building the actor model
[DDPG] Building the critic model
[DDPG] Building the critic model
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1617014969.326129174, 19749.490000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1617014969.327019385, 19749.491000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1617014969.327188528, 19749.492000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1617014970.602912046, 19750.764000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1617014970.968388410, 19751.127000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1617014971.369680017, 19751.527000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1617014971.777606604, 19751.927000000]: Ready to take commands for planning group back_left_leg.[0m
[Actor] GPU>>>>>>>>>>>>
[Actor] [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
[Actor] Memory Growth Allowed
[DDPG] Loading Actor Weights
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.8/site-packages/tensorflow/python/training/py_checkpoint_reader.py", line 95, in NewCheckpointReader
    return CheckpointReader(compat.as_bytes(filepattern))
RuntimeError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for weights/actor_pretrain/exp17/pretrain_enc/actor_pretrained_pretrain_actor_17_6.ckpt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "learn.py", line 1024, in <module>
    learner.load_actor(
  File "learn.py", line 583, in load_actor
    self.actor.model.load_weights(path)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py", line 2199, in load_weights
    py_checkpoint_reader.NewCheckpointReader(filepath)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/tensorflow/python/training/py_checkpoint_reader.py", line 99, in NewCheckpointReader
    error_translator(e)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/tensorflow/python/training/py_checkpoint_reader.py", line 35, in error_translator
    raise errors_impl.NotFoundError(None, None, error_message)
tensorflow.python.framework.errors_impl.NotFoundError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for weights/actor_pretrain/exp17/pretrain_enc/actor_pretrained_pretrain_actor_17_6.ckpt
2021-03-29 10:50:30.180543: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
Unable to init server: Could not connect: Connection refused
Unable to init server: Could not connect: Connection refused

(learn.py:8843): Gdk-CRITICAL **: 10:50:33.859: gdk_cursor_new_for_display: assertion 'GDK_IS_DISPLAY (display)' failed
2021-03-29 10:50:34.245658: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-29 10:50:34.246894: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-03-29 10:50:34.327292: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-29 10:50:34.328132: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:00:1e.0 name: Tesla T4 computeCapability: 7.5
coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s
2021-03-29 10:50:34.328171: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-29 10:50:34.332974: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-29 10:50:34.333144: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-29 10:50:34.334955: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-03-29 10:50:34.335569: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-03-29 10:50:34.340326: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-03-29 10:50:34.341252: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-03-29 10:50:34.341512: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-29 10:50:34.341703: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-29 10:50:34.343138: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-29 10:50:34.344178: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
[DDPG] Building the actor model
2021-03-29 10:50:34.607787: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-03-29 10:50:34.608352: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-29 10:50:34.608813: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-29 10:50:34.609840: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:00:1e.0 name: Tesla T4 computeCapability: 7.5
coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s
2021-03-29 10:50:34.609954: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-29 10:50:34.610283: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-29 10:50:34.610463: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-29 10:50:34.610585: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-03-29 10:50:34.610704: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-03-29 10:50:34.610860: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-03-29 10:50:34.610974: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-03-29 10:50:34.611084: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-29 10:50:34.611295: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-29 10:50:34.612306: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-29 10:50:34.613263: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-03-29 10:50:34.613362: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-29 10:50:35.400986: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-03-29 10:50:35.401031: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2021-03-29 10:50:35.401044: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2021-03-29 10:50:35.401380: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-29 10:50:35.402371: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-29 10:50:35.403257: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-29 10:50:35.403915: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13963 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5)
[DDPG] Building the actor model
[DDPG] Building the critic model
[DDPG] Building the critic model
[DDPG] Waiting for joint trajectory action
2021-03-29 10:51:00.088097: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
Unable to init server: Could not connect: Connection refused
Unable to init server: Could not connect: Connection refused

(learn.py:9081): Gdk-CRITICAL **: 10:51:03.757: gdk_cursor_new_for_display: assertion 'GDK_IS_DISPLAY (display)' failed
2021-03-29 10:51:04.037235: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-29 10:51:04.038393: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-03-29 10:51:04.098012: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-29 10:51:04.098950: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:00:1e.0 name: Tesla T4 computeCapability: 7.5
coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s
2021-03-29 10:51:04.099008: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-29 10:51:04.103220: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-29 10:51:04.103359: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-29 10:51:04.105119: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-03-29 10:51:04.105531: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-03-29 10:51:04.109833: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-03-29 10:51:04.110875: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-03-29 10:51:04.111202: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-29 10:51:04.111400: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-29 10:51:04.112462: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-29 10:51:04.113370: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
[DDPG] Building the actor model
2021-03-29 10:51:04.363676: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-03-29 10:51:04.363930: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-29 10:51:04.364309: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-29 10:51:04.365133: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:00:1e.0 name: Tesla T4 computeCapability: 7.5
coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s
2021-03-29 10:51:04.365177: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-29 10:51:04.365393: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-29 10:51:04.365494: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-29 10:51:04.365689: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-03-29 10:51:04.365781: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-03-29 10:51:04.365860: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-03-29 10:51:04.365930: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-03-29 10:51:04.365997: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-29 10:51:04.366175: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-29 10:51:04.367067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-29 10:51:04.367784: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-03-29 10:51:04.367839: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-29 10:51:05.118437: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-03-29 10:51:05.118491: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2021-03-29 10:51:05.118506: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2021-03-29 10:51:05.118837: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-29 10:51:05.119739: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-29 10:51:05.120416: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-29 10:51:05.121031: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13963 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5)
[DDPG] Building the actor model
[DDPG] Building the critic model
[DDPG] Building the critic model
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1617015068.230348603, 19846.663000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1617015068.231253047, 19846.664000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1617015068.231306034, 19846.664000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1617015069.509604722, 19847.940000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1617015069.773776119, 19848.204000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1617015070.175809656, 19848.605000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1617015070.578902788, 19849.005000000]: Ready to take commands for planning group back_left_leg.[0m
[Actor] GPU>>>>>>>>>>>>
[Actor] [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
[Actor] Memory Growth Allowed
[DDPG] Loading Actor Weights
Traceback (most recent call last):
  File "learn.py", line 1027, in <module>
    learner._pretrain_loop(
  File "learn.py", line 315, in _pretrain_loop
    pkl = open(os.path.join(path, 'loss_{ex}_{name}_{ep}.pickle'.format(
UnboundLocalError: local variable 'path' referenced before assignment
  File "learn.py", line 320
    if start != 0:
                 ^
SyntaxError: invalid syntax
2021-03-29 10:53:34.997862: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
Unable to init server: Could not connect: Connection refused
Unable to init server: Could not connect: Connection refused

(learn.py:9657): Gdk-CRITICAL **: 10:53:38.238: gdk_cursor_new_for_display: assertion 'GDK_IS_DISPLAY (display)' failed
2021-03-29 10:53:38.580829: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-29 10:53:38.584255: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-03-29 10:53:38.636693: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-29 10:53:38.637553: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:00:1e.0 name: Tesla T4 computeCapability: 7.5
coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s
2021-03-29 10:53:38.637584: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-29 10:53:38.641407: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-29 10:53:38.641567: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-29 10:53:38.643258: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-03-29 10:53:38.643682: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-03-29 10:53:38.647858: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-03-29 10:53:38.648978: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-03-29 10:53:38.649260: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-29 10:53:38.649428: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-29 10:53:38.650535: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-29 10:53:38.651480: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
[DDPG] Building the actor model
2021-03-29 10:53:38.942012: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-03-29 10:53:38.942280: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-29 10:53:38.942618: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-29 10:53:38.943732: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:00:1e.0 name: Tesla T4 computeCapability: 7.5
coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s
2021-03-29 10:53:38.943767: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-29 10:53:38.943983: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-29 10:53:38.944034: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-29 10:53:38.944080: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-03-29 10:53:38.944134: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-03-29 10:53:38.944179: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-03-29 10:53:38.944223: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-03-29 10:53:38.944273: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-29 10:53:38.944449: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-29 10:53:38.947779: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-29 10:53:38.948895: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-03-29 10:53:38.949318: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-29 10:53:39.872842: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-03-29 10:53:39.872892: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2021-03-29 10:53:39.872906: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2021-03-29 10:53:39.873273: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-29 10:53:39.874154: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-29 10:53:39.874999: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-29 10:53:39.875870: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13963 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5)
[DDPG] Building the actor model
[DDPG] Building the critic model
[DDPG] Building the critic model
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1617015222.786797413, 19999.688000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1617015222.787637913, 19999.689000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1617015222.787697844, 19999.689000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1617015224.067560873, 20000.967000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1617015224.351345127, 20001.250000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1617015224.753426710, 20001.651000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1617015225.152500566, 20002.050000000]: Ready to take commands for planning group back_left_leg.[0m
[Actor] GPU>>>>>>>>>>>>
[Actor] [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
[Actor] Memory Growth Allowed
[DDPG] Loading Actor Weights
[Actor] Dataset <PrefetchDataset shapes: (((1125, 6), (1125, 34), (1125, 60)), ((1125, 350, 12), (1125, 1), (1125, 12), (1125, 12))), types: ((tf.float32, tf.float32, tf.float64), (tf.float32, tf.float32, tf.float64, tf.float32))>
[Actor] Starting Actor Pretraining
[Actor] Starting Episode 7
2021-03-29 10:54:09.480272: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-29 10:54:10.875971: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
[Actor] Episode 7 Step 0 Loss: 335.8009338378906
[Actor] Episode 7 Step 1 Loss: 392.8799743652344
Y Pred
tf.Tensor(
[[[           nan            nan            nan ...            nan
              nan            nan]
  [ 3.9731458e-02 -1.5892178e-02  6.1760270e-03 ... -9.9022994e-03
    7.6410302e-05 -2.8276362e-02]
  [ 3.9731458e-02 -1.5892178e-02  6.1760270e-03 ... -9.9022994e-03
    7.6410302e-05 -2.8276362e-02]
  ...
  [ 3.9731458e-02 -1.5892178e-02  6.1760270e-03 ... -9.9022994e-03
    7.6410302e-05 -2.8276362e-02]
  [ 3.9731458e-02 -1.5892178e-02  6.1760270e-03 ... -9.9022994e-03
    7.6410302e-05 -2.8276362e-02]
  [ 3.9731458e-02 -1.5892178e-02  6.1760270e-03 ... -9.9022994e-03
    7.6410302e-05 -2.8276362e-02]]

 [[           nan            nan            nan ...            nan
              nan            nan]
  [ 3.9731458e-02 -1.5892178e-02  6.1760270e-03 ... -9.9022994e-03
    7.6410302e-05 -2.8276362e-02]
  [ 3.9731458e-02 -1.5892178e-02  6.1760270e-03 ... -9.9022994e-03
    7.6410302e-05 -2.8276362e-02]
  ...
  [ 3.9731458e-02 -1.5892178e-02  6.1760270e-03 ... -9.9022994e-03
    7.6410302e-05 -2.8276362e-02]
  [ 3.9731458e-02 -1.5892178e-02  6.1760270e-03 ... -9.9022994e-03
    7.6410302e-05 -2.8276362e-02]
  [ 3.9731458e-02 -1.5892178e-02  6.1760270e-03 ... -9.9022994e-03
    7.6410302e-05 -2.8276362e-02]]

 [[           nan            nan            nan ...            nan
              nan            nan]
  [ 3.9731458e-02 -1.5892178e-02  6.1760270e-03 ... -9.9022994e-03
    7.6410302e-05 -2.8276362e-02]
  [ 3.9731458e-02 -1.5892178e-02  6.1760270e-03 ... -9.9022994e-03
    7.6410302e-05 -2.8276362e-02]
  ...
  [ 3.9731458e-02 -1.5892178e-02  6.1760270e-03 ... -9.9022994e-03
    7.6410302e-05 -2.8276362e-02]
  [ 3.9731458e-02 -1.5892178e-02  6.1760270e-03 ... -9.9022994e-03
    7.6410302e-05 -2.8276362e-02]
  [ 3.9731458e-02 -1.5892178e-02  6.1760270e-03 ... -9.9022994e-03
    7.6410302e-05 -2.8276362e-02]]

 ...

 [[           nan            nan            nan ...            nan
              nan            nan]
  [ 3.9731458e-02 -1.5892178e-02  6.1760270e-03 ... -9.9022994e-03
    7.6410302e-05 -2.8276362e-02]
  [ 3.9731458e-02 -1.5892178e-02  6.1760270e-03 ... -9.9022994e-03
    7.6410302e-05 -2.8276362e-02]
  ...
  [ 3.9731458e-02 -1.5892178e-02  6.1760270e-03 ... -9.9022994e-03
    7.6410302e-05 -2.8276362e-02]
  [ 3.9731458e-02 -1.5892178e-02  6.1760270e-03 ... -9.9022994e-03
    7.6410302e-05 -2.8276362e-02]
  [ 3.9731458e-02 -1.5892178e-02  6.1760270e-03 ... -9.9022994e-03
    7.6410302e-05 -2.8276362e-02]]

 [[           nan            nan            nan ...            nan
              nan            nan]
  [ 3.9731458e-02 -1.5892178e-02  6.1760270e-03 ... -9.9022994e-03
    7.6410302e-05 -2.8276362e-02]
  [ 3.9731458e-02 -1.5892178e-02  6.1760270e-03 ... -9.9022994e-03
    7.6410302e-05 -2.8276362e-02]
  ...
  [ 3.9731458e-02 -1.5892178e-02  6.1760270e-03 ... -9.9022994e-03
    7.6410302e-05 -2.8276362e-02]
  [ 3.9731458e-02 -1.5892178e-02  6.1760270e-03 ... -9.9022994e-03
    7.6410302e-05 -2.8276362e-02]
  [ 3.9731458e-02 -1.5892178e-02  6.1760270e-03 ... -9.9022994e-03
    7.6410302e-05 -2.8276362e-02]]

 [[           nan            nan            nan ...            nan
              nan            nan]
  [ 3.9731458e-02 -1.5892178e-02  6.1760270e-03 ... -9.9022994e-03
    7.6410302e-05 -2.8276362e-02]
  [ 3.9731458e-02 -1.5892178e-02  6.1760270e-03 ... -9.9022994e-03
    7.6410302e-05 -2.8276362e-02]
  ...
  [ 3.9731458e-02 -1.5892178e-02  6.1760270e-03 ... -9.9022994e-03
    7.6410302e-05 -2.8276362e-02]
  [ 3.9731458e-02 -1.5892178e-02  6.1760270e-03 ... -9.9022994e-03
    7.6410302e-05 -2.8276362e-02]
  [ 3.9731458e-02 -1.5892178e-02  6.1760270e-03 ... -9.9022994e-03
    7.6410302e-05 -2.8276362e-02]]], shape=(1125, 350, 12), dtype=float32)
omega
tf.Tensor(
[[0.]
 [0.]
 [0.]
 ...
 [0.]
 [0.]
 [0.]], shape=(1125, 1), dtype=float32)
mu
tf.Tensor(
[[nan nan nan ... nan nan nan]
 [nan nan nan ... nan nan nan]
 [nan nan nan ... nan nan nan]
 ...
 [nan nan nan ... nan nan nan]
 [nan nan nan ... nan nan nan]
 [nan nan nan ... nan nan nan]], shape=(1125, 12), dtype=float32)
mean
tf.Tensor(
[[nan nan nan ... nan nan nan]
 [nan nan nan ... nan nan nan]
 [nan nan nan ... nan nan nan]
 ...
 [nan nan nan ... nan nan nan]
 [nan nan nan ... nan nan nan]
 [nan nan nan ... nan nan nan]], shape=(1125, 12), dtype=float32)
grads
[<tf.Tensor: shape=(6, 30), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(30,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan], dtype=float32)>, <tf.Tensor: shape=(30, 60), dtype=float32, numpy=
array([[nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       ...,
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(60,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan], dtype=float32)>, <tf.Tensor: shape=(60, 30), dtype=float32, numpy=
array([[nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       ...,
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(30,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan], dtype=float32)>, <tf.Tensor: shape=(30, 25), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],
      dtype=float32)>, <tf.Tensor: shape=(25,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
      dtype=float32)>, <tf.Tensor: shape=(25, 12), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],
      dtype=float32)>, <tf.Tensor: shape=(12,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
      dtype=float32)>, <tf.Tensor: shape=(30, 25), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],
      dtype=float32)>, <tf.Tensor: shape=(25,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
      dtype=float32)>, <tf.Tensor: shape=(25, 12), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],
      dtype=float32)>, <tf.Tensor: shape=(12,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
      dtype=float32)>, <tf.Tensor: shape=(30, 1), dtype=float32, numpy=
array([[nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan]], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>, <tf.Tensor: shape=(34, 40), dtype=float32, numpy=
array([[nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       ...,
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(40,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan], dtype=float32)>, <tf.Tensor: shape=(40, 80), dtype=float32, numpy=
array([[nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       ...,
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(80,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan], dtype=float32)>, <tf.Tensor: shape=(80, 30), dtype=float32, numpy=
array([[nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       ...,
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(30,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan], dtype=float32)>, <tf.Tensor: shape=(30, 30), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(30, 30), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(60,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan], dtype=float32)>, <tf.Tensor: shape=(30, 45), dtype=float32, numpy=
array([[nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       ...,
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(30, 45), dtype=float32, numpy=
array([[nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       ...,
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(90,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
      dtype=float32)>, <tf.Tensor: shape=(45, 24), dtype=float32, numpy=
array([[nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       ...,
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(45, 24), dtype=float32, numpy=
array([[nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       ...,
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(48,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan], dtype=float32)>, <tf.Tensor: shape=(24, 12), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],
      dtype=float32)>, <tf.Tensor: shape=(24, 12), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],
      dtype=float32)>, <tf.Tensor: shape=(24,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
      dtype=float32)>, <tf.Tensor: shape=(30, 30), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(30, 30), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(60,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan], dtype=float32)>, <tf.Tensor: shape=(30, 30), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(30, 30), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(60,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan], dtype=float32)>, <tf.Tensor: shape=(60, 30), dtype=float32, numpy=
array([[nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       ...,
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(60, 30), dtype=float32, numpy=
array([[nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       ...,
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(60,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan], dtype=float32)>]
[Actor] Episode 7 Step 2 Loss: nan
Y Pred
tf.Tensor(
[[[nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]
  ...
  [nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]]

 [[nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]
  ...
  [nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]]

 [[nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]
  ...
  [nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]]

 ...

 [[nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]
  ...
  [nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]]

 [[nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]
  ...
  [nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]]

 [[nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]
  ...
  [nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]]], shape=(1125, 350, 12), dtype=float32)
omega
tf.Tensor(
[[0.]
 [0.]
 [0.]
 ...
 [0.]
 [0.]
 [0.]], shape=(1125, 1), dtype=float32)
mu
tf.Tensor(
[[nan nan nan ... nan nan nan]
 [nan nan nan ... nan nan nan]
 [nan nan nan ... nan nan nan]
 ...
 [nan nan nan ... nan nan nan]
 [nan nan nan ... nan nan nan]
 [nan nan nan ... nan nan nan]], shape=(1125, 12), dtype=float32)
mean
tf.Tensor(
[[nan nan nan ... nan nan nan]
 [nan nan nan ... nan nan nan]
 [nan nan nan ... nan nan nan]
 ...
 [nan nan nan ... nan nan nan]
 [nan nan nan ... nan nan nan]
 [nan nan nan ... nan nan nan]], shape=(1125, 12), dtype=float32)
grads
[<tf.Tensor: shape=(6, 30), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(30,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan], dtype=float32)>, <tf.Tensor: shape=(30, 60), dtype=float32, numpy=
array([[nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       ...,
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(60,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan], dtype=float32)>, <tf.Tensor: shape=(60, 30), dtype=float32, numpy=
array([[nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       ...,
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(30,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan], dtype=float32)>, <tf.Tensor: shape=(30, 25), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],
      dtype=float32)>, <tf.Tensor: shape=(25,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
      dtype=float32)>, <tf.Tensor: shape=(25, 12), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],
      dtype=float32)>, <tf.Tensor: shape=(12,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
      dtype=float32)>, <tf.Tensor: shape=(30, 25), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],
      dtype=float32)>, <tf.Tensor: shape=(25,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
      dtype=float32)>, <tf.Tensor: shape=(25, 12), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],
      dtype=float32)>, <tf.Tensor: shape=(12,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
      dtype=float32)>, <tf.Tensor: shape=(30, 1), dtype=float32, numpy=
array([[nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan]], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>, <tf.Tensor: shape=(34, 40), dtype=float32, numpy=
array([[nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       ...,
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(40,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan], dtype=float32)>, <tf.Tensor: shape=(40, 80), dtype=float32, numpy=
array([[nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       ...,
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(80,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan], dtype=float32)>, <tf.Tensor: shape=(80, 30), dtype=float32, numpy=
array([[nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       ...,
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(30,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan], dtype=float32)>, <tf.Tensor: shape=(30, 30), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(30, 30), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(60,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan], dtype=float32)>, <tf.Tensor: shape=(30, 45), dtype=float32, numpy=
array([[nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       ...,
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(30, 45), dtype=float32, numpy=
array([[nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       ...,
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(90,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
      dtype=float32)>, <tf.Tensor: shape=(45, 24), dtype=float32, numpy=
array([[nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       ...,
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(45, 24), dtype=float32, numpy=
array([[nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       ...,
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(48,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan], dtype=float32)>, <tf.Tensor: shape=(24, 12), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],
      dtype=float32)>, <tf.Tensor: shape=(24, 12), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],
      dtype=float32)>, <tf.Tensor: shape=(24,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
      dtype=float32)>, <tf.Tensor: shape=(30, 30), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(30, 30), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(60,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan], dtype=float32)>, <tf.Tensor: shape=(30, 30), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(30, 30), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(60,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan], dtype=float32)>, <tf.Tensor: shape=(60, 30), dtype=float32, numpy=
array([[nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       ...,
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(60, 30), dtype=float32, numpy=
array([[nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       ...,
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(60,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan], dtype=float32)>]
[Actor] Episode 7 Step 3 Loss: nan
2021-03-29 10:58:00.551203: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
Unable to init server: Could not connect: Connection refused
Unable to init server: Could not connect: Connection refused

(learn.py:10114): Gdk-CRITICAL **: 10:58:03.882: gdk_cursor_new_for_display: assertion 'GDK_IS_DISPLAY (display)' failed
2021-03-29 10:58:04.332630: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-29 10:58:04.334218: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-03-29 10:58:04.445341: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-29 10:58:04.447436: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:00:1e.0 name: Tesla T4 computeCapability: 7.5
coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s
2021-03-29 10:58:04.448070: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-29 10:58:04.457797: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-29 10:58:04.457963: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-29 10:58:04.463393: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-03-29 10:58:04.464071: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-03-29 10:58:04.474375: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-03-29 10:58:04.476545: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-03-29 10:58:04.477775: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-29 10:58:04.478370: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-29 10:58:04.480563: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-29 10:58:04.481735: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
[DDPG] Building the actor model
2021-03-29 10:58:04.805339: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-03-29 10:58:04.805593: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-29 10:58:04.805935: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-29 10:58:04.806747: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:00:1e.0 name: Tesla T4 computeCapability: 7.5
coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s
2021-03-29 10:58:04.806786: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-29 10:58:04.806983: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-29 10:58:04.807057: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-29 10:58:04.807105: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-03-29 10:58:04.807150: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-03-29 10:58:04.807194: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-03-29 10:58:04.807244: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-03-29 10:58:04.807297: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-29 10:58:04.807496: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-29 10:58:04.808300: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-29 10:58:04.808909: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-03-29 10:58:04.808979: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-29 10:58:05.621327: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-03-29 10:58:05.621367: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2021-03-29 10:58:05.621379: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2021-03-29 10:58:05.621766: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-29 10:58:05.622866: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-29 10:58:05.623808: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-29 10:58:05.624578: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13963 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5)
[DDPG] Building the actor model
[DDPG] Building the critic model
[DDPG] Building the critic model
[DDPG] Waiting for joint trajectory action
2021-03-29 10:59:14.742552: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
Unable to init server: Could not connect: Connection refused
Unable to init server: Could not connect: Connection refused

(learn.py:10361): Gdk-CRITICAL **: 10:59:18.059: gdk_cursor_new_for_display: assertion 'GDK_IS_DISPLAY (display)' failed
2021-03-29 10:59:18.361130: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-29 10:59:18.362452: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-03-29 10:59:18.424972: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-29 10:59:18.425952: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:00:1e.0 name: Tesla T4 computeCapability: 7.5
coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s
2021-03-29 10:59:18.425999: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-29 10:59:18.430645: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-29 10:59:18.430781: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-29 10:59:18.432757: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-03-29 10:59:18.433186: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-03-29 10:59:18.438224: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-03-29 10:59:18.439448: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-03-29 10:59:18.439687: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-29 10:59:18.439884: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-29 10:59:18.440983: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-29 10:59:18.441906: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
[DDPG] Building the actor model
2021-03-29 10:59:18.724416: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-03-29 10:59:18.724673: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-29 10:59:18.724993: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-29 10:59:18.726069: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:00:1e.0 name: Tesla T4 computeCapability: 7.5
coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s
2021-03-29 10:59:18.726114: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-29 10:59:18.726285: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-29 10:59:18.726342: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-29 10:59:18.726409: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-03-29 10:59:18.726468: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-03-29 10:59:18.726529: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-03-29 10:59:18.726768: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-03-29 10:59:18.726895: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-29 10:59:18.727080: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-29 10:59:18.728129: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-29 10:59:18.729072: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-03-29 10:59:18.729128: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-29 10:59:19.552268: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-03-29 10:59:19.552305: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2021-03-29 10:59:19.552318: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2021-03-29 10:59:19.552724: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-29 10:59:19.553823: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-29 10:59:19.554764: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-29 10:59:19.555441: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13963 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5)
[DDPG] Building the actor model
[DDPG] Building the critic model
[DDPG] Building the critic model
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1617015562.703106327, 20334.869000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1617015562.704006889, 20334.869000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1617015562.704177982, 20334.870000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1617015563.989279824, 20336.152000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1617015564.411862008, 20336.573000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1617015564.811902109, 20336.972000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1617015565.213928746, 20337.374000000]: Ready to take commands for planning group back_left_leg.[0m
[Actor] GPU>>>>>>>>>>>>
[Actor] [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
[Actor] Memory Growth Allowed
[DDPG] Loading Actor Weights
[Actor] Dataset <PrefetchDataset shapes: (((1125, 6), (1125, 34), (1125, 60)), ((1125, 350, 12), (1125, 1), (1125, 12), (1125, 12))), types: ((tf.float32, tf.float32, tf.float64), (tf.float32, tf.float32, tf.float64, tf.float32))>
[Actor] Starting Actor Pretraining
[Actor] Starting Episode 7
2021-03-29 10:59:49.526236: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-29 10:59:50.127626: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
Y Pred
tf.Tensor(
[[[-9.9596208e-01  8.3028173e-01  4.1436452e-01 ... -3.7925845e-01
    6.1242368e-02  9.0326953e-01]
  [ 2.7849743e-02 -1.8258847e-02  2.8200320e-04 ... -1.5305821e-04
    2.5621934e-03 -2.2179719e-02]
  [ 2.7849743e-02 -1.8258847e-02  2.8200320e-04 ... -1.5305821e-04
    2.5621934e-03 -2.2179719e-02]
  ...
  [ 2.7849743e-02 -1.8258847e-02  2.8200320e-04 ... -1.5305821e-04
    2.5621934e-03 -2.2179719e-02]
  [ 2.7849743e-02 -1.8258847e-02  2.8200320e-04 ... -1.5305821e-04
    2.5621934e-03 -2.2179719e-02]
  [ 2.7849743e-02 -1.8258847e-02  2.8200320e-04 ... -1.5305821e-04
    2.5621934e-03 -2.2179719e-02]]

 [[-9.9427873e-01  8.9244133e-01  4.5565490e-02 ...  6.8045747e-01
   -2.9387096e-01  9.3307531e-01]
  [ 2.7849743e-02 -1.8258847e-02  2.8200320e-04 ... -1.5305821e-04
    2.5621934e-03 -2.2179719e-02]
  [ 2.7849743e-02 -1.8258847e-02  2.8200320e-04 ... -1.5305821e-04
    2.5621934e-03 -2.2179719e-02]
  ...
  [ 2.7849743e-02 -1.8258847e-02  2.8200320e-04 ... -1.5305821e-04
    2.5621934e-03 -2.2179719e-02]
  [ 2.7849743e-02 -1.8258847e-02  2.8200320e-04 ... -1.5305821e-04
    2.5621934e-03 -2.2179719e-02]
  [ 2.7849743e-02 -1.8258847e-02  2.8200320e-04 ... -1.5305821e-04
    2.5621934e-03 -2.2179719e-02]]

 [[ 9.5779550e-01 -5.1633483e-01  8.5057712e-01 ... -9.8193288e-01
    8.0792540e-01 -4.6580794e-01]
  [ 2.7849743e-02 -1.8258847e-02  2.8200320e-04 ... -1.5305821e-04
    2.5621934e-03 -2.2179719e-02]
  [ 2.7849743e-02 -1.8258847e-02  2.8200320e-04 ... -1.5305821e-04
    2.5621934e-03 -2.2179719e-02]
  ...
  [ 2.7849743e-02 -1.8258847e-02  2.8200320e-04 ... -1.5305821e-04
    2.5621934e-03 -2.2179719e-02]
  [ 2.7849743e-02 -1.8258847e-02  2.8200320e-04 ... -1.5305821e-04
    2.5621934e-03 -2.2179719e-02]
  [ 2.7849743e-02 -1.8258847e-02  2.8200320e-04 ... -1.5305821e-04
    2.5621934e-03 -2.2179719e-02]]

 ...

 [[ 9.3538541e-01 -5.1169890e-01 -6.6070005e-02 ...  8.4243083e-01
   -4.1177297e-01 -9.3081707e-01]
  [ 2.7849743e-02 -1.8258847e-02  2.8200320e-04 ... -1.5305821e-04
    2.5621934e-03 -2.2179719e-02]
  [ 2.7849743e-02 -1.8258847e-02  2.8200320e-04 ... -1.5305821e-04
    2.5621934e-03 -2.2179719e-02]
  ...
  [ 2.7849743e-02 -1.8258847e-02  2.8200320e-04 ... -1.5305821e-04
    2.5621934e-03 -2.2179719e-02]
  [ 2.7849743e-02 -1.8258847e-02  2.8200320e-04 ... -1.5305821e-04
    2.5621934e-03 -2.2179719e-02]
  [ 2.7849743e-02 -1.8258847e-02  2.8200320e-04 ... -1.5305821e-04
    2.5621934e-03 -2.2179719e-02]]

 [[ 9.1460240e-01 -5.1404208e-01  8.7983000e-01 ... -9.8201567e-01
    8.5987753e-01 -1.5906133e-01]
  [ 2.7849743e-02 -1.8258847e-02  2.8200320e-04 ... -1.5305821e-04
    2.5621934e-03 -2.2179719e-02]
  [ 2.7849743e-02 -1.8258847e-02  2.8200320e-04 ... -1.5305821e-04
    2.5621934e-03 -2.2179719e-02]
  ...
  [ 2.7849743e-02 -1.8258847e-02  2.8200320e-04 ... -1.5305821e-04
    2.5621934e-03 -2.2179719e-02]
  [ 2.7849743e-02 -1.8258847e-02  2.8200320e-04 ... -1.5305821e-04
    2.5621934e-03 -2.2179719e-02]
  [ 2.7849743e-02 -1.8258847e-02  2.8200320e-04 ... -1.5305821e-04
    2.5621934e-03 -2.2179719e-02]]

 [[ 9.3865520e-01 -4.6560040e-01  3.9214304e-01 ... -2.0467572e-01
    8.1728265e-02 -9.1655916e-01]
  [ 2.7849743e-02 -1.8258847e-02  2.8200320e-04 ... -1.5305821e-04
    2.5621934e-03 -2.2179719e-02]
  [ 2.7849743e-02 -1.8258847e-02  2.8200320e-04 ... -1.5305821e-04
    2.5621934e-03 -2.2179719e-02]
  ...
  [ 2.7849743e-02 -1.8258847e-02  2.8200320e-04 ... -1.5305821e-04
    2.5621934e-03 -2.2179719e-02]
  [ 2.7849743e-02 -1.8258847e-02  2.8200320e-04 ... -1.5305821e-04
    2.5621934e-03 -2.2179719e-02]
  [ 2.7849743e-02 -1.8258847e-02  2.8200320e-04 ... -1.5305821e-04
    2.5621934e-03 -2.2179719e-02]]], shape=(1125, 350, 12), dtype=float32)
omega
tf.Tensor(
[[16.134773]
 [24.40891 ]
 [18.86459 ]
 ...
 [54.473167]
 [20.780285]
 [32.184875]], shape=(1125, 1), dtype=float32)
mu
tf.Tensor(
[[0.52394235 0.08437271 0.5474246  ... 0.46766725 0.12251991 0.51752764]
 [0.4944788  0.10888388 0.48616442 ... 0.5159694  0.09763278 0.49994835]
 [0.5124029  0.09555022 0.5256144  ... 0.49030614 0.11364705 0.51361156]
 ...
 [0.50957966 0.07192199 0.5082338  ... 0.4753806  0.10971459 0.48651293]
 [0.5052083  0.10167765 0.51059806 ... 0.5020725  0.10782071 0.5095322 ]
 [0.484717   0.10766827 0.46491766 ... 0.5167105  0.08248363 0.48040324]], shape=(1125, 12), dtype=float32)
mean
tf.Tensor(
[[-0.02861744  0.18378928 -0.05366943 ... -0.02447352  0.17147405
   0.01995782]
 [-0.04386492  0.17371853  0.02725532 ... -0.0394089   0.17304677
   0.01975062]
 [-0.03592782  0.17870577 -0.02330839 ... -0.03103968  0.17335878
   0.02262394]
 ...
 [-0.02743912  0.16893049 -0.04380563 ... -0.01991127  0.15721522
   0.01734988]
 [-0.04017908  0.17598446 -0.00343945 ... -0.03480072  0.17393304
   0.02288158]
 [-0.03186627  0.17773879  0.0448515  ... -0.03889444  0.16466704
   0.00848303]], shape=(1125, 12), dtype=float32)
grads
[<tf.Tensor: shape=(6, 30), dtype=float32, numpy=
array([[ 1.86717010e+00, -2.88743496e+00,  1.82562602e+00,
         1.29697526e+00, -4.53181219e+00,  9.21748161e-01,
        -4.70423937e+00,  6.77133036e+00,  3.37658226e-01,
         1.76863119e-01, -4.85522795e+00,  7.54940510e-01,
        -1.59340262e+00,  1.55078816e+00,  6.81783009e+00,
         3.55590773e+00,  4.06808853e+00, -3.37959146e+00,
         6.05689082e-03, -7.02751696e-01,  6.00992155e+00,
        -7.09383965e-01,  5.41249132e+00, -4.16973114e+00,
        -2.48464489e+00, -4.91460145e-01,  6.25941694e-01,
        -3.13725805e+00,  3.43660641e+00,  1.07112753e+00],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],
       [ 6.30108546e-03, -1.42592359e-02,  1.10610835e-02,
         3.47546116e-03, -1.79253053e-02,  6.60011545e-03,
         5.61812036e-02, -8.85796547e-03,  4.31654230e-03,
         3.54568404e-03,  2.54532788e-02,  5.13241207e-03,
         1.82177313e-02,  2.52909660e-02, -3.18701081e-02,
         2.31136493e-02,  3.57739627e-05,  2.60460190e-03,
         2.59372010e-03,  4.05577011e-05, -3.82718667e-02,
         1.29410457e-02, -2.66076922e-02,  3.94345596e-02,
        -1.82706788e-02,  7.23897945e-03, -5.86805120e-03,
        -2.38609128e-02,  1.24787539e-02,  4.20756545e-03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         0.00000000e+00,  0.00000000e+00,  0.00000000e+00]], dtype=float32)>, <tf.Tensor: shape=(30,), dtype=float32, numpy=
array([ 1.8671700e+00, -2.8874350e+00,  1.8256260e+00,  1.2969755e+00,
       -4.5318122e+00,  9.2174816e-01, -4.7042398e+00,  6.7713299e+00,
        3.3765817e-01,  1.7686313e-01, -4.8552275e+00,  7.5494051e-01,
       -1.5934026e+00,  1.5507883e+00,  6.8178306e+00,  3.5559082e+00,
        4.0680885e+00, -3.3795915e+00,  6.0568880e-03, -7.0275170e-01,
        6.0099211e+00, -7.0938396e-01,  5.4124918e+00, -4.1697307e+00,
       -2.4846447e+00, -4.9146017e-01,  6.2594175e-01, -3.1372576e+00,
        3.4366064e+00,  1.0711274e+00], dtype=float32)>, <tf.Tensor: shape=(30, 60), dtype=float32, numpy=
array([[ 0.09038632, -0.07159574, -0.07451476, ..., -0.08728348,
         0.05274237,  0.05890496],
       [-0.04314782,  0.03166623,  0.03434035, ...,  0.0397197 ,
        -0.02476825, -0.02770865],
       [ 0.14526632, -0.11776868, -0.12108271, ..., -0.14237289,
         0.08520801,  0.09511478],
       ...,
       [-0.09305969,  0.07272407,  0.0762236 , ...,  0.08912166,
        -0.05411097, -0.06044267],
       [ 0.0562605 , -0.0420234 , -0.04513103, ..., -0.05237071,
         0.03240037,  0.03622891],
       [ 0.12487295, -0.10146148, -0.10420051, ..., -0.12254824,
         0.07329879,  0.08182196]], dtype=float32)>, <tf.Tensor: shape=(60,), dtype=float32, numpy=
array([ 0.4819734 , -0.40151927, -0.40705085,  0.37729272, -0.25027382,
       -0.66164225, -0.5833762 , -0.04205909,  0.20359708,  0.5697239 ,
        0.09906723,  0.0279119 ,  0.45990652, -0.42164648, -0.23049968,
        0.04109593, -0.7471991 , -0.38610208,  0.30614555, -0.59242094,
        0.59062433,  0.10083613,  0.2645448 , -0.4220978 ,  0.01161584,
        0.3714008 ,  0.37372163,  0.48319268,  0.47577697, -0.07294841,
       -0.19201037,  0.24700236, -0.57731676, -0.63641775,  0.543112  ,
        0.12468696,  0.31755328, -0.5177456 ,  0.7799584 , -0.61823654,
       -0.57734704,  0.19881125, -0.3668668 , -0.20631665, -0.4815575 ,
        0.01352719, -0.5373248 , -0.69140154, -0.45592034,  0.30763277,
        0.52179956, -0.44414175, -0.12499741, -0.31416336,  0.32755423,
        0.30707452, -0.24642386, -0.4806519 ,  0.2845625 ,  0.31748456],
      dtype=float32)>, <tf.Tensor: shape=(60, 30), dtype=float32, numpy=
array([[ 1.9077107e-02,  3.3955523e-03, -8.0841687e-03, ...,
         2.5528965e-03,  1.5734329e-03, -9.2709390e-03],
       [-1.5407693e-02, -2.8375760e-03,  6.9760988e-03, ...,
        -2.0994889e-03, -1.2734787e-03,  2.9384056e-03],
       [-1.1841880e-02, -2.4492275e-03,  6.3543399e-03, ...,
        -1.7617290e-03, -1.0373853e-03, -5.5937031e-03],
       ...,
       [-1.3888501e-02, -2.5860572e-03,  6.4557204e-03, ...,
        -1.8982057e-03, -1.1420839e-03,  6.5410789e-04],
       [-2.3791953e-03,  5.6607916e-04, -2.9658540e-03, ...,
         1.7852419e-04, -3.9974715e-05,  3.5892494e-02],
       [ 2.0903233e-03,  1.5048452e-03, -5.0277179e-03, ...,
         9.1262278e-04,  4.2915231e-04,  3.1220974e-02]], dtype=float32)>, <tf.Tensor: shape=(30,), dtype=float32, numpy=
array([-0.09522644, -0.01091233,  0.01567606,  0.01024275, -0.01364951,
        0.06424438,  0.03906526,  0.01591265, -0.15569219, -0.01291622,
        0.2059901 ,  0.00841383,  0.1991591 ,  0.01194321, -0.27862397,
        0.01578047,  0.2161983 , -0.01694756, -0.01056605, -0.00457259,
       -0.00711587, -0.12694243,  0.01088527, -0.01528555, -0.01357288,
       -0.02297499,  0.01363145, -0.00978086, -0.00698484,  0.2658219 ],
      dtype=float32)>, <tf.Tensor: shape=(30, 25), dtype=float32, numpy=
array([[ 5.02063012e-05, -1.02557433e-05,  4.23368983e-05,
         6.07133334e-05,  4.28862004e-05,  2.13941967e-05,
         7.30307729e-05,  7.71205014e-05,  6.56468619e-05,
         7.96075983e-05,  2.87681232e-05,  1.08628694e-04,
        -1.14594528e-04, -1.85668650e-05,  5.88871171e-06,
        -3.64358721e-06, -1.92907992e-05,  1.91026375e-05,
        -2.69953234e-05, -1.37523266e-05,  7.61954516e-06,
        -3.55558986e-05,  5.35505897e-05, -1.50927835e-05,
         1.35092239e-04],
       [-4.95151835e-05,  1.92210311e-04,  3.26755748e-04,
         9.68295353e-05,  1.50485557e-05,  4.94548549e-05,
        -2.17377790e-04, -1.89565799e-05,  3.17041995e-05,
         1.06896114e-04,  1.00961915e-04, -8.46461553e-05,
         2.43350660e-04, -1.11875161e-05, -7.83904125e-06,
        -2.05399367e-04,  8.96877973e-05, -5.95086458e-05,
        -1.71143023e-04,  1.21083831e-05,  1.11417277e-04,
         5.79619336e-05, -6.87274151e-05, -1.55739181e-05,
        -1.63957011e-04],
       [ 2.78955686e-05, -1.49184823e-04, -2.45975243e-04,
        -1.14461465e-04, -1.70047351e-05, -4.88616352e-05,
         1.61538599e-04,  2.65769595e-05, -3.99197852e-05,
        -5.46387892e-05, -1.03931954e-04,  5.37253036e-05,
        -1.58584531e-04,  1.25434663e-05,  5.00286615e-06,
         1.48007224e-04, -2.00144896e-05,  4.59483781e-05,
         1.33504160e-04, -7.77711466e-06, -4.95204513e-05,
        -4.44939506e-05,  8.05553573e-05,  2.14172615e-05,
         6.67793647e-05],
       [ 4.98093832e-05, -1.92851570e-04, -3.27876769e-04,
        -9.66481603e-05, -1.50102860e-05, -4.94982050e-05,
         2.18141096e-04,  1.88948143e-05, -3.16305741e-05,
        -1.07543143e-04, -1.00952610e-04,  8.50650104e-05,
        -2.44514114e-04,  1.11676418e-05,  7.87788485e-06,
         2.06167664e-04, -9.05461275e-05,  5.97089129e-05,
         1.71701162e-04, -1.21676258e-05, -1.12173890e-04,
        -5.81638888e-05,  6.86839121e-05,  1.55033831e-05,
         1.65241101e-04],
       [-3.65586166e-05,  1.66230311e-04,  2.78090040e-04,
         1.07363798e-04,  1.62242850e-05,  4.89919039e-05,
        -1.84018834e-04, -2.36017968e-05,  3.64168809e-05,
         7.54797074e-05,  1.02797094e-04, -6.62685052e-05,
         1.92532170e-04, -1.20032910e-05, -6.14044166e-06,
        -1.70995569e-04,  4.79557348e-05, -5.13426094e-05,
        -1.48313644e-04,  9.52719711e-06,  7.43883429e-05,
         4.98656009e-05, -7.56350491e-05, -1.91023464e-05,
        -1.05649277e-04],
       [ 5.54193612e-05,  2.08326128e-05,  1.92501066e-05,
         1.41661672e-04,  3.07152004e-05,  5.65932787e-05,
         8.32472579e-05,  9.64606261e-06,  1.06489970e-04,
        -4.23368328e-05,  7.27869774e-05,  1.10604480e-04,
        -1.46012171e-04, -1.75803962e-05,  6.10260122e-06,
         4.30237633e-05, -1.66088255e-04,  5.41269947e-06,
        -4.99705966e-05, -1.36120116e-05, -1.26663377e-04,
        -1.59373740e-05, -9.03161490e-05, -2.89067830e-05,
         2.31570360e-04],
       [ 2.82754118e-05,  4.96988650e-05,  5.69478216e-05,
         1.56449285e-04,  2.28027329e-05,  5.28918390e-05,
        -8.21726644e-06, -3.49110851e-05,  7.43717465e-05,
        -6.41370571e-05,  1.03916005e-04,  3.86604006e-05,
        -5.85617709e-05, -1.59295705e-05,  2.42072019e-06,
        -2.76393848e-06, -1.51654574e-04, -1.24279577e-05,
        -5.44622962e-05, -4.67171640e-06, -1.06214371e-04,
         9.41339295e-06, -1.14001217e-04, -3.35868099e-05,
         1.73531822e-04],
       [ 2.51953898e-05, -1.43951183e-04, -2.35992804e-04,
        -1.16763695e-04, -1.72308319e-05, -4.88735022e-05,
         1.54535286e-04,  2.75699240e-05, -4.10698776e-05,
        -4.80397139e-05, -1.04332998e-04,  4.98124282e-05,
        -1.48012201e-04,  1.27097519e-05,  4.64859113e-06,
         1.40818156e-04, -1.11644895e-05,  4.42906567e-05,
         1.28954765e-04, -7.23206631e-06, -4.16355360e-05,
        -4.28483690e-05,  8.22671645e-05,  2.21522823e-05,
         5.45997755e-05],
       [ 4.86425852e-05, -3.32600903e-05,  3.47059395e-05,
         3.24310167e-05,  4.76624300e-05,  3.99850387e-06,
         6.67895074e-05,  9.61194892e-05,  4.30650798e-05,
         1.13742761e-04,  1.57096401e-05,  1.02533144e-04,
        -1.09704590e-04, -1.91467607e-05,  5.90020818e-06,
        -1.56456372e-05,  2.87431794e-05,  2.57112533e-05,
        -5.40840210e-06, -1.33042959e-05,  5.36149382e-05,
        -4.31451517e-05,  1.11404079e-04, -1.14372760e-05,
         1.10155066e-04],
       [-4.51325686e-05,  1.83380616e-04,  3.10209231e-04,
         1.00415593e-04,  1.54405679e-05,  4.92780309e-05,
        -2.06136552e-04, -2.05834567e-05,  3.32399686e-05,
         9.61923361e-05,  1.01622725e-04, -7.84877484e-05,
         2.26159667e-04, -1.14638224e-05, -7.26508370e-06,
        -1.93750806e-04,  7.55262954e-05, -5.67407333e-05,
        -1.63343298e-04,  1.12410744e-05,  9.88655811e-05,
         5.52248821e-05, -7.10650420e-05, -1.67830694e-05,
        -1.44184072e-04],
       [-4.80413364e-05,  7.52317865e-05, -8.50965534e-06,
         2.06525146e-06, -5.45693729e-05,  2.19619324e-05,
        -6.11512223e-05, -1.16231975e-04, -9.74670729e-06,
        -1.46498700e-04, -7.22799314e-07, -9.32959229e-05,
         1.15067131e-04,  2.02034971e-05, -6.11680935e-06,
         2.29751240e-05, -8.21409631e-05, -3.61528073e-05,
        -3.41991108e-05,  1.26795867e-05, -1.06698753e-04,
         5.40116671e-05, -1.89631959e-04,  7.80903974e-06,
        -9.20646053e-05],
       [ 5.50365621e-05, -2.03467323e-04, -3.47747642e-04,
        -9.23594635e-05, -1.45466793e-05, -4.97526053e-05,
         2.31487880e-04,  1.68873885e-05, -2.98992854e-05,
        -1.20401295e-04, -1.00117235e-04,  9.23239204e-05,
        -2.65025708e-04,  1.08370359e-05,  8.56163842e-06,
         2.20074551e-04, -1.07471322e-04,  6.30247378e-05,
         1.81137002e-04, -1.31936285e-05, -1.27152452e-04,
        -6.14331148e-05,  6.59382640e-05,  1.40416005e-05,
         1.88885548e-04],
       [-4.81101015e-05,  6.41411971e-05, -1.58804796e-05,
        -6.44220518e-06, -5.28082492e-05,  1.53134079e-05,
        -6.24065142e-05, -1.11416506e-04, -1.82586518e-05,
        -1.38875999e-04, -4.38192410e-06, -9.56408039e-05,
         1.13122034e-04,  1.99236038e-05, -6.05123478e-06,
         2.15721084e-05, -6.92507529e-05, -3.34480428e-05,
        -2.37339245e-05,  1.28342226e-05, -9.37776058e-05,
         5.12390397e-05, -1.69925683e-04,  8.65861693e-06,
        -9.58505407e-05],
       [ 1.78927457e-05, -1.29934691e-04, -2.09648453e-04,
        -1.22516416e-04, -1.79371636e-05, -4.89142803e-05,
         1.35258626e-04,  2.95857026e-05, -4.44628713e-05,
        -3.11888580e-05, -1.04947714e-04,  3.87804976e-05,
        -1.19463701e-04,  1.31626248e-05,  3.68619362e-06,
         1.21583558e-04,  1.19183460e-05,  3.97824988e-05,
         1.17095566e-04, -5.71057626e-06, -2.09796908e-05,
        -3.82908293e-05,  8.63315436e-05,  2.39743440e-05,
         2.23219067e-05],
       [ 4.79101000e-05, -1.17740958e-04, -2.08584916e-05,
        -3.39327562e-05,  6.11434079e-05, -4.73023028e-05,
         5.61890120e-05,  1.34031288e-04, -2.31120830e-05,
         1.74696965e-04, -1.28418169e-05,  8.41192814e-05,
        -1.22659549e-04, -2.12785253e-05,  6.39059635e-06,
        -2.78871321e-05,  1.30343542e-04,  4.64725163e-05,
         7.46410660e-05, -1.20744262e-05,  1.55113856e-04,
        -6.45241162e-05,  2.64412578e-04, -4.83430586e-06,
         7.87469180e-05],
       [ 3.36962694e-05, -1.60492928e-04, -2.67172232e-04,
        -1.09873086e-04, -1.64492958e-05, -4.89177546e-05,
         1.76735281e-04,  2.48489741e-05, -3.74209776e-05,
        -6.82005484e-05, -1.03355116e-04,  6.23235683e-05,
        -1.81285664e-04,  1.21814155e-05,  5.76630737e-06,
         1.63315708e-04, -3.84145678e-05,  4.95591376e-05,
         1.43188110e-04, -8.96960591e-06, -6.59402867e-05,
        -4.81236602e-05,  7.74209620e-05,  1.99332535e-05,
         9.25417626e-05],
       [-4.78248476e-05,  7.08778680e-05, -1.24891958e-05,
         5.47429750e-07, -5.40273650e-05,  2.00143495e-05,
        -6.08387163e-05, -1.15825474e-04, -1.20178620e-05,
        -1.46668259e-04, -1.28096372e-06, -9.38545563e-05,
         1.12582100e-04,  2.00897739e-05, -6.07353786e-06,
         2.40549416e-05, -8.07918768e-05, -3.52702591e-05,
        -3.02153348e-05,  1.27046351e-05, -1.04976018e-04,
         5.32318845e-05, -1.84905351e-04,  7.86938290e-06,
        -9.06123896e-05],
       [-3.97535659e-05,  1.72572050e-04,  2.89865769e-04,
         1.04901330e-04,  1.59064602e-05,  4.90897910e-05,
        -1.92358086e-04, -2.26643460e-05,  3.51434937e-05,
         8.29203345e-05,  1.02490536e-04, -7.09536180e-05,
         2.05049859e-04, -1.18015296e-05, -6.56084376e-06,
        -1.79417504e-04,  5.79988227e-05, -5.33585408e-05,
        -1.53776695e-04,  1.01796486e-05,  8.33309605e-05,
         5.18900706e-05, -7.40758042e-05, -1.82847907e-05,
        -1.19824879e-04],
       [-5.25017967e-05,  1.98290625e-04,  3.38005542e-04,
         9.45051288e-05,  1.47577684e-05,  4.96224529e-05,
        -2.25070413e-04, -1.79612598e-05,  3.06862967e-05,
         1.14018636e-04,  1.00593017e-04, -8.88775394e-05,
         2.55073915e-04, -1.09970242e-05, -8.23086066e-06,
        -2.13301595e-04,  9.91439738e-05, -6.14183227e-05,
        -1.76483067e-04,  1.27037683e-05,  1.19797427e-04,
         5.98615479e-05, -6.73415707e-05, -1.47758728e-05,
        -1.77322945e-04],
       [-6.01778338e-05,  2.14017549e-04,  3.67412926e-04,
         8.81802116e-05,  1.40845204e-05,  5.00650349e-05,
        -2.44569674e-04, -1.48954732e-05,  2.83063673e-05,
         1.33064837e-04,  9.92838613e-05, -9.93941285e-05,
         2.85211194e-04, -1.05098898e-05, -9.23362768e-06,
        -2.33750790e-04,  1.24070109e-04, -6.63100218e-05,
        -1.90557737e-04,  1.41963865e-05,  1.41819590e-04,
         6.46678018e-05, -6.33664094e-05, -1.25956485e-05,
        -2.12155079e-04],
       [-5.68923570e-05,  2.07268604e-04,  3.54861608e-04,
         9.08245129e-05,  1.43850693e-05,  4.98584013e-05,
        -2.36199427e-04, -1.61394491e-05,  2.93246958e-05,
         1.25016071e-04,  9.97959651e-05, -9.48630404e-05,
         2.72311852e-04, -1.07193409e-05, -8.80402695e-06,
        -2.25020485e-04,  1.13507536e-04, -6.42066079e-05,
        -1.84541510e-04,  1.35541122e-05,  1.32484987e-04,
         6.25935863e-05, -6.49685317e-05, -1.35128885e-05,
        -1.97313260e-04],
       [ 4.95197819e-05, -2.44200673e-05,  3.62986248e-05,
         4.54368055e-05,  4.56350645e-05,  1.14478789e-05,
         7.00697783e-05,  8.70349832e-05,  5.29357385e-05,
         9.68811291e-05,  2.17823817e-05,  1.05236126e-04,
        -1.13544578e-04, -1.89245475e-05,  5.91611479e-06,
        -9.13801705e-06,  5.92607239e-06,  2.29600264e-05,
        -1.38047199e-05, -1.35115024e-05,  3.20093750e-05,
        -3.98563716e-05,  8.56337792e-05, -1.31729776e-05,
         1.23172678e-04],
       [ 5.23668350e-05, -1.97986112e-04, -3.37487843e-04,
        -9.45768261e-05, -1.47776082e-05, -4.95959212e-05,
         2.24719261e-04,  1.79810086e-05, -3.07131668e-05,
        -1.13734910e-04, -1.00590951e-04,  8.86853086e-05,
        -2.54538434e-04,  1.10064138e-05,  8.21299454e-06,
         2.12951811e-04, -9.87680251e-05,  6.13234661e-05,
         1.76218862e-04, -1.26764871e-05, -1.19467149e-04,
        -5.97652725e-05,  6.73388349e-05,  1.48063782e-05,
         1.76741771e-04],
       [-3.13469063e-05,  1.55979593e-04,  2.58722750e-04,
         1.11686131e-04,  1.66828831e-05,  4.89213417e-05,
        -1.70524698e-04, -2.54564457e-05,  3.85087478e-05,
         6.28466660e-05,  1.03526327e-04, -5.87602408e-05,
         1.72111322e-04, -1.23276295e-05, -5.45656712e-06,
        -1.57147835e-04,  3.10569922e-05, -4.81032039e-05,
        -1.39378826e-04,  8.47843239e-06,  5.93543919e-05,
         4.66469282e-05, -7.86692108e-05, -2.05110846e-05,
        -8.22042493e-05],
       [-4.10788089e-05,  1.75257301e-04,  2.94992991e-04,
         1.03708895e-04,  1.58089915e-05,  4.91356477e-05,
        -1.95694476e-04, -2.20305319e-05,  3.47212299e-05,
         8.63717287e-05,  1.02192324e-04, -7.27312436e-05,
         2.10260245e-04, -1.17189902e-05, -6.73357317e-06,
        -1.82988093e-04,  6.24745298e-05, -5.41864865e-05,
        -1.56209688e-04,  1.04328283e-05,  8.72830860e-05,
         5.26915901e-05, -7.32265471e-05, -1.78852315e-05,
        -1.25947015e-04],
       [-1.97227782e-05, -6.30676659e-05, -8.23412047e-05,
        -1.51120417e-04, -2.17482611e-05, -5.15988977e-05,
         3.28168753e-05,  3.58269754e-05, -6.73399336e-05,
         4.94308188e-05, -1.05360923e-04, -2.27280525e-05,
         2.61904934e-05,  1.54291920e-05, -1.28559941e-06,
         2.39851215e-05,  1.27828258e-04,  1.73608278e-05,
         6.37262856e-05,  2.58733962e-06,  8.40733628e-05,
        -1.49204534e-05,  1.09293418e-04,  3.22661508e-05,
        -1.39403710e-04],
       [ 4.03277736e-05, -1.73775217e-04, -2.92171055e-04,
        -1.04355255e-04, -1.58700223e-05, -4.91252395e-05,
         1.93768399e-04,  2.23311035e-05, -3.50073387e-05,
        -8.45033646e-05, -1.02321515e-04,  7.16729555e-05,
        -2.07318604e-04,  1.17655572e-05,  6.63531182e-06,
         1.80984411e-04, -5.99986670e-05,  5.37209744e-05,
         1.54900190e-04, -1.02841841e-05, -8.50849538e-05,
        -5.22329101e-05,  7.36918955e-05,  1.80968746e-05,
         1.22537851e-04],
       [-5.17785302e-05,  1.96815497e-04,  3.35315242e-04,
         9.50290414e-05,  1.48360305e-05,  4.95754939e-05,
        -2.23187439e-04, -1.81548967e-05,  3.09418319e-05,
         1.12364767e-04,  1.00648940e-04, -8.78269202e-05,
         2.52235768e-04, -1.10436395e-05, -8.13565202e-06,
        -2.11402803e-04,  9.69213870e-05, -6.09513663e-05,
        -1.75207038e-04,  1.25567512e-05,  1.17823954e-04,
         5.93916993e-05, -6.76210839e-05, -1.49577318e-05,
        -1.74132234e-04],
       [-5.64587899e-05,  2.06371798e-04,  3.53151874e-04,
         9.12188407e-05,  1.44154383e-05,  4.98334703e-05,
        -2.35124535e-04, -1.63645800e-05,  2.94375877e-05,
         1.23863807e-04,  9.99066542e-05, -9.43040723e-05,
         2.70608172e-04, -1.07465667e-05, -8.74779107e-06,
        -2.23850366e-04,  1.12033820e-04, -6.39325081e-05,
        -1.83713477e-04,  1.34735037e-05,  1.31189285e-04,
         6.23304586e-05, -6.52384842e-05, -1.36490435e-05,
        -1.95295957e-04],
       [-4.88049845e-05,  1.63324323e-04,  5.69538824e-05,
         6.08701666e-05, -6.75617921e-05,  7.18598822e-05,
        -5.40626897e-05, -1.47349769e-04,  5.42820053e-05,
        -1.92655774e-04,  2.39718229e-05, -7.57834860e-05,
         1.37519237e-04,  2.24557753e-05, -6.76620539e-06,
         2.66555471e-05, -1.67464808e-04, -5.68432661e-05,
        -1.17574302e-04,  1.15805569e-05, -1.93809246e-04,
         7.46180158e-05, -3.32947733e-04,  2.80614586e-06,
        -7.62303753e-05]], dtype=float32)>, <tf.Tensor: shape=(25,), dtype=float32, numpy=
array([ 6.49254944e-05, -2.23849434e-04, -3.85759515e-04, -8.42656300e-05,
       -1.36709568e-05, -5.03949486e-05,  2.56558618e-04,  1.29297205e-05,
       -2.69584107e-05, -1.44936639e-04, -9.84245999e-05,  1.05797299e-04,
       -3.03857640e-04,  1.02071317e-05,  9.85278621e-06,  2.46409065e-04,
       -1.39508222e-04,  6.93549737e-05,  1.99424016e-04, -1.51097483e-05,
       -1.55431975e-04, -6.76497148e-05,  6.09778799e-05,  1.12257003e-05,
        2.33759041e-04], dtype=float32)>, <tf.Tensor: shape=(25, 12), dtype=float32, numpy=
array([[-1.67334816e-04,  1.25632112e-04, -2.50006357e-04,
        -1.11096000e-04, -5.32482845e-05,  1.25707680e-04,
         2.64479459e-04, -1.57796312e-04, -3.75441159e-04,
         5.89613082e-05, -2.46321724e-04,  2.14658103e-05],
       [-9.10782328e-05,  8.42280424e-05, -4.22092926e-05,
        -3.04332843e-06,  2.54725110e-05,  5.34491555e-05,
         5.64094807e-05, -6.71526941e-05, -5.04317068e-05,
         4.32314991e-05, -9.81042394e-05,  3.19850842e-05],
       [-3.29016120e-06,  7.28018858e-06, -2.11495819e-04,
        -4.09763379e-05, -3.59130172e-06,  1.57356881e-05,
         1.90263774e-04, -4.48332539e-05, -2.28862496e-04,
         9.80727345e-05, -2.17103716e-05, -9.21213286e-05],
       [ 1.81941039e-04, -1.12745343e-04,  2.91393138e-04,
         1.41091703e-04,  2.77621712e-05, -6.91773821e-05,
        -2.64399598e-04,  1.23586055e-04,  3.98922944e-04,
        -6.29266069e-05,  2.62315822e-04,  2.14687643e-05],
       [ 1.65361300e-04, -1.89161670e-04,  1.54794892e-04,
         2.01354014e-05,  6.13230295e-05, -2.48398108e-04,
        -2.58864398e-04,  2.44566792e-04,  2.84902926e-04,
        -8.85073096e-05,  2.23837123e-04, -1.13925533e-04],
       [ 1.27982639e-04, -1.11015186e-04,  1.43809099e-04,
         5.57918247e-05,  3.41050545e-05, -1.17408541e-04,
        -1.75335837e-04,  1.32584712e-04,  2.31294136e-04,
        -4.55166592e-05,  1.78074406e-04, -4.43529716e-05],
       [-8.00017806e-05,  1.41928045e-04, -2.10674334e-05,
         3.00379543e-05, -1.07838110e-04,  2.72493868e-04,
         1.71039166e-04, -2.26751697e-04, -1.63181263e-04,
         2.83658865e-05, -1.27285544e-04,  1.62556084e-04],
       [-1.90682404e-04,  1.00052552e-04, -2.82370776e-04,
        -1.45279191e-04,  2.11767547e-05, -3.20756953e-07,
         2.14211148e-04, -7.27040315e-05, -3.42254265e-04,
         6.58524805e-05, -2.54912418e-04, -5.53878417e-05],
       [ 8.33214508e-05, -2.55158757e-05,  1.28239306e-04,
         7.51340776e-05, -3.71940114e-05,  5.58077663e-05,
        -6.51618539e-05, -8.60223008e-06,  1.27642270e-04,
        -2.59640601e-05,  1.04574145e-04,  5.73377984e-05],
       [-1.14234776e-04,  9.60650941e-05, -7.34860660e-05,
        -2.64024020e-05,  1.18458584e-05,  6.52672024e-05,
         8.67958588e-05, -8.35487444e-05, -1.05866508e-04,
         3.89200795e-05, -1.36378076e-04,  3.65956039e-05],
       [-6.75109914e-05, -1.23005284e-05, -1.41280761e-04,
        -1.00299410e-04,  4.64367040e-05, -1.18420605e-04,
         4.10469802e-05,  5.86709903e-05, -1.27205567e-04,
         7.94795596e-06, -8.94024706e-05, -9.73714050e-05],
       [ 1.39566706e-04, -1.11343179e-04,  1.91034429e-04,
         9.27053770e-05,  7.43768032e-05, -1.39561875e-04,
        -2.26743548e-04,  1.55062997e-04,  3.27535556e-04,
        -2.88350202e-05,  2.16932574e-04, -4.81564130e-05],
       [-7.71918621e-06,  2.73084843e-05,  5.01718932e-05,
         2.67554569e-05, -1.74962042e-05,  5.62899368e-05,
        -1.03835855e-05, -3.72558134e-05,  3.31175761e-05,
        -8.22684979e-06, -7.13893041e-06,  5.49374381e-05],
       [ 1.76016547e-04, -1.62930170e-04,  2.20235757e-04,
         7.25458958e-05,  5.42644484e-05, -1.86199992e-04,
        -2.75367289e-04,  2.05271688e-04,  3.49693175e-04,
        -8.02334180e-05,  2.47485761e-04, -6.20500577e-05],
       [ 1.28296510e-04, -1.14558257e-04,  1.67945705e-04,
         7.78658577e-05,  9.04625340e-05, -1.67820632e-04,
        -2.25511423e-04,  1.71343330e-04,  3.16182995e-04,
        -2.33259016e-05,  2.05919889e-04, -6.97135401e-05],
       [ 3.23566455e-05, -1.04542924e-04, -1.01478843e-04,
        -8.07476245e-05,  6.50600632e-05, -2.11661172e-04,
        -3.70783237e-05,  1.56808499e-04, -2.67118448e-05,
        -4.37717790e-06,  4.07283587e-05, -1.62306460e-04],
       [-3.58150501e-05, -1.07088554e-05, -1.69982188e-04,
        -5.91162607e-05,  7.46017176e-05, -1.06311796e-04,
         7.53594140e-05,  4.96242355e-05, -1.12195215e-04,
         7.59745162e-05, -3.17116719e-05, -1.28533458e-04],
       [-1.16217074e-04,  1.06727763e-04, -1.60695403e-04,
        -6.66750129e-05, -7.62719865e-05,  1.53588713e-04,
         2.12077721e-04, -1.57137561e-04, -2.87450966e-04,
         3.34634424e-05, -1.82858348e-04,  5.88043185e-05],
       [ 1.43149373e-05,  1.79499948e-05,  1.10027177e-04,
         4.46598606e-05, -3.85152380e-05,  7.35229842e-05,
        -4.70149389e-05, -3.86660686e-05,  7.92395076e-05,
        -3.89863162e-05,  1.52026041e-05,  8.54480968e-05],
       [-1.92859094e-04,  1.38253235e-04, -2.89360149e-04,
        -1.28867730e-04, -4.26715706e-05,  1.18358672e-04,
         2.90051568e-04, -1.62427401e-04, -4.14484384e-04,
         7.27540246e-05, -2.77045125e-04,  8.18105400e-06],
       [ 7.04120903e-05, -3.09675925e-05,  5.63046451e-05,
         5.29926983e-05, -1.40366740e-06,  7.67350139e-06,
        -3.44944929e-05,  1.43431653e-05,  8.45719333e-05,
         9.69772373e-06,  9.56581353e-05, -9.96192284e-07],
       [-1.05530155e-04,  1.04639235e-04, -1.41737837e-04,
        -5.72025383e-05, -8.64762551e-05,  1.66997284e-04,
         2.05429053e-04, -1.62634125e-04, -2.74500228e-04,
         2.66830084e-05, -1.71629275e-04,  7.12820183e-05],
       [ 4.36415430e-05, -1.32667701e-04, -8.93964170e-05,
        -8.90348310e-05,  8.91473537e-05, -2.72146746e-04,
        -8.21240683e-05,  2.07148754e-04,  1.52026205e-05,
        -1.64207213e-05,  6.14698365e-05, -1.91851723e-04],
       [-1.36993141e-04,  1.18603668e-04, -1.69483625e-04,
        -6.96884090e-05, -5.43421193e-05,  1.39569311e-04,
         2.08764570e-04, -1.53055633e-04, -2.81794695e-04,
         4.42913733e-05, -1.99819362e-04,  5.14081039e-05],
       [ 6.19915954e-05,  8.18448098e-06,  1.03641156e-04,
         6.53444440e-05, -9.07800568e-05,  1.40443051e-04,
         3.48870844e-06, -7.90475606e-05,  3.78094519e-05,
        -3.12718585e-05,  5.62996538e-05,  1.03857332e-04]], dtype=float32)>, <tf.Tensor: shape=(12,), dtype=float32, numpy=
array([ 1.99448521e-04, -1.41628974e-04,  3.00685992e-04,  1.33249472e-04,
        3.91305002e-05, -1.16097544e-04, -2.97395803e-04,  1.63474848e-04,
        4.24756669e-04, -7.76251254e-05,  2.84573733e-04, -3.89550041e-06],
      dtype=float32)>, <tf.Tensor: shape=(30, 25), dtype=float32, numpy=
array([[-8.95210251e-05,  2.86602008e-05, -6.56231132e-05,
         1.15647463e-05,  5.50504046e-05, -1.24980113e-04,
         1.40076765e-04, -6.97356518e-05, -3.03190409e-05,
         6.58093340e-05,  3.91393678e-06, -2.28140598e-05,
         2.87622925e-06,  1.50162581e-04, -5.02697112e-05,
        -1.92254083e-05, -3.00879801e-05, -2.57821266e-05,
         3.31093133e-06,  9.73165152e-05, -6.85207669e-06,
         3.02817407e-05, -6.65062107e-05,  1.34374068e-05,
        -3.18371458e-05],
       [ 9.67499545e-06,  8.34879174e-05, -1.70701125e-04,
         1.02741986e-04, -8.26612813e-05, -2.86315393e-04,
        -1.61225180e-04, -2.84709124e-04,  1.16251467e-04,
         3.22269188e-05, -8.05442687e-05,  1.15962612e-05,
        -1.95230114e-05,  8.57834530e-05, -1.39343974e-04,
        -4.18703858e-05,  9.41348626e-05, -4.63699544e-05,
         3.48982348e-06, -6.49536014e-05,  1.58268376e-04,
         3.62748069e-05, -5.90841992e-05,  4.93898006e-05,
        -1.02951199e-04],
       [ 5.02581752e-05, -1.05991858e-04,  1.84652396e-04,
        -9.86562809e-05,  7.67345628e-05,  2.42689304e-04,
         1.82989956e-04,  3.10722971e-04, -1.15086164e-04,
        -3.34589822e-05,  5.99571104e-05, -2.50620869e-05,
        -2.06457335e-05, -1.01490790e-04,  1.13794289e-04,
         2.44179573e-05, -1.60148455e-04,  4.31564877e-05,
         1.01830828e-05,  6.19360217e-05, -1.89753730e-04,
        -8.88316572e-05,  5.68704781e-05, -9.52830742e-05,
         1.07936619e-04],
       [-1.04197179e-05, -8.32685982e-05,  1.70643179e-04,
        -1.02835482e-04,  8.27245240e-05,  2.87054980e-04,
         1.60963595e-04,  2.84370559e-04, -1.16351592e-04,
        -3.21943953e-05,  8.09212943e-05, -1.14213544e-05,
         2.00201612e-05, -8.56597981e-05,  1.39761687e-04,
         4.20975666e-05, -9.33087722e-05,  4.63930555e-05,
        -3.64697235e-06,  6.50211150e-05, -1.57995266e-04,
        -3.56135170e-05,  5.91144890e-05, -4.88903934e-05,
         1.03014128e-04],
       [-2.62194917e-05,  9.68098830e-05, -1.78610353e-04,
         1.00324687e-04, -7.95837914e-05, -2.59044435e-04,
        -1.75124471e-04, -3.01195832e-04,  1.15443152e-04,
         3.29666073e-05, -6.74795519e-05,  1.98577618e-05,
         4.62821163e-06,  9.45722786e-05, -1.23518592e-04,
        -3.13284472e-05,  1.34511269e-04, -4.46716731e-05,
        -4.68154667e-06, -6.33614472e-05,  1.76864531e-04,
         6.80983576e-05, -5.77466562e-05,  7.66897938e-05,
        -1.05274870e-04],
       [-2.08206911e-04,  1.53345434e-04, -2.42110604e-04,
         5.25039432e-05,  6.16649631e-05, -2.69745360e-04,
         3.29688592e-05, -1.34100890e-04,  7.65514051e-05,
         4.81254574e-05, -9.73367278e-05,  5.48689604e-06,
         9.72647103e-05,  2.58151675e-04, -1.10509587e-04,
         4.68752387e-06,  1.18336458e-04,  3.90501600e-06,
        -4.04584207e-05,  4.54563815e-05,  2.27898621e-04,
         1.38772040e-04, -5.73627112e-05,  2.06168741e-04,
        -1.94691296e-04],
       [-1.97777219e-04,  1.66406535e-04, -2.40378897e-04,
         8.26313917e-05, -2.71870158e-05, -2.00780327e-04,
        -1.66541169e-04, -3.06148082e-04,  1.12253503e-04,
         3.79536614e-05, -5.14145540e-05,  4.31072222e-05,
         1.12278853e-04,  1.80616247e-04, -8.05400778e-05,
         1.26250607e-05,  2.60498549e-04, -2.07771427e-05,
        -4.38238421e-05, -3.30870607e-05,  2.73025042e-04,
         1.92856824e-04, -5.25298237e-05,  2.15221982e-04,
        -1.57250994e-04],
       [ 5.78641084e-05, -1.08991211e-04,  1.86744321e-04,
        -9.81912744e-05,  7.58153474e-05,  2.37823624e-04,
         1.85495097e-04,  3.13711411e-04, -1.15103314e-04,
        -3.35890873e-05,  5.77680403e-05, -2.66980514e-05,
        -2.57171705e-05, -1.03791201e-04,  1.10872075e-04,
         2.22474318e-05, -1.68244849e-04,  4.26477563e-05,
         1.19433753e-05,  6.15336176e-05, -1.94018707e-04,
        -9.53776616e-05,  5.65959854e-05, -1.01276390e-04,
         1.08974178e-04],
       [-5.33666607e-05, -1.67399267e-05,  7.27654424e-06,
        -3.49264201e-06,  3.81338541e-05, -3.78482655e-05,
         1.42657169e-04, -8.05044401e-05, -7.24161800e-05,
         7.25146019e-05,  6.09899944e-05, -2.40541503e-05,
        -2.38998819e-05,  9.36247234e-05, -1.07734413e-05,
        -2.33193132e-05, -5.25049472e-05, -4.08781452e-05,
         1.79529161e-05,  1.05748753e-04, -9.05446068e-05,
         7.93802428e-06, -6.86832500e-05, -5.57633321e-05,
         4.40619151e-05],
       [-2.50008998e-06,  8.79830768e-05, -1.73281878e-04,
         1.01959071e-04, -8.17932596e-05, -2.76744191e-04,
        -1.66278100e-04, -2.90641095e-04,  1.15986535e-04,
         3.24681496e-05, -7.59028117e-05,  1.44696305e-05,
        -1.12933030e-05,  8.85698100e-05, -1.33834779e-04,
        -3.82664548e-05,  1.08150591e-04, -4.58707655e-05,
         7.17182843e-07, -6.45144610e-05,  1.64547586e-04,
         4.71980275e-05, -5.86274728e-05,  5.86216920e-05,
        -1.03560043e-04],
       [ 1.60943055e-05,  7.21400138e-05, -1.02210601e-04,
         2.46358977e-05, -9.21599258e-06, -9.35543430e-05,
        -1.21882891e-04,  1.15146344e-04,  1.28649524e-04,
        -8.19266788e-05, -1.45139449e-04,  1.90738792e-05,
         4.96744869e-05, -1.43485968e-05, -5.15411812e-05,
         2.47884163e-05,  6.01657084e-05,  6.07690818e-05,
        -3.48431022e-05, -1.09941495e-04,  1.90523308e-04,
         6.71126736e-06,  6.97959549e-05,  1.38174932e-04,
        -1.47979139e-04],
       [-2.49648219e-05, -7.79552356e-05,  1.67752747e-04,
        -1.03733080e-04,  8.34984021e-05,  2.99036299e-04,
         1.54430163e-04,  2.76778010e-04, -1.16686359e-04,
        -3.19139981e-05,  8.68204224e-05, -7.88065154e-06,
         2.99078802e-05, -8.26445103e-05,  1.46589751e-04,
         4.64485420e-05, -7.60905241e-05,  4.68707003e-05,
        -6.95609469e-06,  6.54074611e-05, -1.50577835e-04,
        -2.23703028e-05,  5.96665450e-05, -3.79276898e-05,
         1.02596816e-04],
       [ 2.49115892e-05,  5.84740810e-05, -7.84865188e-05,
         1.92620064e-05, -1.67497165e-05, -5.98569823e-05,
        -1.28192318e-04,  1.05524021e-04,  1.14504030e-04,
        -7.95384040e-05, -1.23647289e-04,  2.06277946e-05,
         4.36770933e-05, -3.44061846e-05, -3.54433832e-05,
         2.46075888e-05,  5.92631077e-05,  5.57985477e-05,
        -3.07294322e-05, -1.09194487e-04,  1.65965525e-04,
         3.75481386e-06,  6.95934577e-05,  1.17956428e-04,
        -1.21773068e-04],
       [ 7.77460082e-05, -1.16704119e-04,  1.92422362e-04,
        -9.65061190e-05,  7.21027827e-05,  2.26599674e-04,
         1.89289247e-04,  3.18942301e-04, -1.14677008e-04,
        -3.40815059e-05,  5.30453217e-05, -3.04344849e-05,
        -3.86727188e-05, -1.10984722e-04,  1.03847240e-04,
         1.67469079e-05, -1.86997408e-04,  4.08364867e-05,
         1.64746125e-05,  5.95001402e-05, -2.04759242e-04,
        -1.11538611e-04,  5.59056352e-05, -1.16818221e-04,
         1.12453883e-04],
       [ 1.63116256e-05, -1.23898993e-04,  1.92959706e-04,
        -4.47912789e-05, -2.05962133e-05,  2.24418836e-04,
         9.50165413e-05, -1.55013273e-04, -1.82588585e-04,
         9.08614020e-05,  2.28304489e-04, -1.24893577e-05,
        -7.18072042e-05, -6.31215735e-05,  1.13942551e-04,
        -2.50458397e-05, -6.03575500e-05, -8.01023998e-05,
         5.02333605e-05,  1.12096212e-04, -2.83734757e-04,
        -1.60172131e-05, -7.08208318e-05, -2.14814208e-04,
         2.48832541e-04],
       [ 3.44124455e-05, -9.99548065e-05,  1.80500138e-04,
        -9.99317635e-05,  7.91953789e-05,  2.52679194e-04,
         1.79011113e-04,  3.05585563e-04, -1.15470189e-04,
        -3.30792973e-05,  6.43948297e-05, -2.18743007e-05,
        -1.02200029e-05, -9.63636048e-05,  1.19865384e-04,
         2.88899391e-05, -1.44311794e-04,  4.43768113e-05,
         6.57883857e-06,  6.32874362e-05, -1.81359574e-04,
        -7.55884394e-05,  5.74326841e-05, -8.30359786e-05,
         1.05761443e-04],
       [ 1.65392612e-05,  6.98979711e-05, -9.75295989e-05,
         2.30946680e-05, -1.14978029e-05, -8.44988390e-05,
        -1.26279032e-04,  1.10772024e-04,  1.25452716e-04,
        -8.12377184e-05, -1.39540760e-04,  2.02827723e-05,
         4.98065820e-05, -1.89513230e-05, -4.67447826e-05,
         2.52602276e-05,  6.24892564e-05,  5.98981060e-05,
        -3.42851345e-05, -1.10658533e-04,  1.86983627e-04,
         7.96056247e-06,  7.01239987e-05,  1.35286595e-04,
        -1.42149802e-04],
       [-1.75650766e-05,  9.36183496e-05, -1.76590795e-04,
         1.01052050e-04, -8.07718461e-05, -2.65041919e-04,
        -1.72692467e-04, -2.98093481e-04,  1.15768438e-04,
         3.27423768e-05, -7.02460602e-05,  1.80486022e-05,
        -1.09223993e-06,  9.20319508e-05, -1.27091131e-04,
        -3.38084319e-05,  1.25596009e-04, -4.52515669e-05,
        -2.73058095e-06, -6.40614962e-05,  1.72474116e-04,
         6.07505135e-05, -5.80593623e-05,  7.01271347e-05,
        -1.04411214e-04],
       [ 1.78169339e-05,  8.05775271e-05, -1.69122650e-04,
         1.03361999e-04, -8.33299564e-05, -2.92878482e-04,
        -1.58076844e-04, -2.80923152e-04,  1.16586365e-04,
         3.20294675e-05, -8.37418265e-05,  9.70692054e-06,
        -2.49996010e-05,  8.39284912e-05, -1.43121288e-04,
        -4.42813798e-05,  8.49422067e-05, -4.67168757e-05,
         5.32042668e-06, -6.53678508e-05,  1.54270121e-04,
         2.90330499e-05, -5.93907607e-05,  4.33268433e-05,
        -1.02661004e-04],
       [ 3.92284164e-05,  7.28406085e-05, -1.65168836e-04,
         1.04617196e-04, -8.40367939e-05, -3.11389507e-04,
        -1.47634622e-04, -2.68914184e-04,  1.17099036e-04,
         3.16313271e-05, -9.29916059e-05,  4.31656736e-06,
        -3.96480100e-05,  7.99973059e-05, -1.53566187e-04,
        -5.07600926e-05,  5.88174516e-05, -4.72285028e-05,
         1.01880196e-05, -6.57022101e-05,  1.43470490e-04,
         9.22030722e-06, -6.02146829e-05,  2.72879861e-05,
        -1.02540005e-04],
       [ 3.01492291e-05,  7.60784824e-05, -1.66792204e-04,
         1.04031853e-04, -8.36602994e-05, -3.03521374e-04,
        -1.51881162e-04, -2.73852056e-04,  1.16803567e-04,
         3.18194070e-05, -8.90625597e-05,  6.57181954e-06,
        -3.34570796e-05,  8.16983666e-05, -1.49119267e-04,
        -4.80180970e-05,  6.97473297e-05, -4.69917468e-05,
         8.13573206e-06, -6.54799951e-05,  1.47954997e-04,
         1.75663117e-05, -5.98661572e-05,  3.40402585e-05,
        -1.02568039e-04],
       [-7.08069201e-05,  4.33177001e-06, -2.60342713e-05,
         2.99684598e-06,  4.52880668e-05, -7.58893439e-05,
         1.39148993e-04, -7.73352076e-05, -5.34883729e-05,
         6.96223724e-05,  3.59110636e-05, -2.28153840e-05,
        -1.06852640e-05,  1.18978671e-04, -2.76032060e-05,
        -2.10745493e-05, -4.04824532e-05, -3.38764075e-05,
         1.10832570e-05,  1.01241443e-04, -5.13155064e-05,
         1.94580025e-05, -6.73770264e-05, -2.33201972e-05,
         9.85745010e-06],
       [-1.74893721e-05, -8.06613461e-05,  1.69124396e-04,
        -1.03311555e-04,  8.33030426e-05,  2.92513694e-04,
         1.58187337e-04,  2.81072746e-04, -1.16524126e-04,
        -3.20474901e-05,  8.35517349e-05, -9.78486787e-06,
         2.47815988e-05, -8.39691784e-05,  1.42917372e-04,
         4.41794764e-05, -8.53045058e-05,  4.67097634e-05,
        -5.25403266e-06,  6.53309398e-05, -1.54365305e-04,
        -2.93247176e-05,  5.93768964e-05, -4.35316542e-05,
         1.02608159e-04],
       [-4.07512161e-05,  1.02383368e-04, -1.82267759e-04,
         9.93669237e-05, -7.79728580e-05, -2.49067351e-04,
        -1.80115632e-04, -3.07174399e-04,  1.15286930e-04,
         3.32470845e-05, -6.28688140e-05,  2.30478909e-05,
         1.43377074e-05,  9.86680316e-05, -1.17607968e-04,
        -2.71382742e-05,  1.50219465e-04, -4.37955023e-05,
        -8.01605529e-06, -6.25885732e-05,  1.84713630e-04,
         8.07130345e-05, -5.72159479e-05,  8.79554646e-05,
        -1.06842701e-04],
       [-1.37265215e-05,  9.21524115e-05, -1.75767316e-04,
         1.01198937e-04, -8.08106706e-05, -2.68251461e-04,
        -1.70586485e-04, -2.95758451e-04,  1.15732961e-04,
         3.27004818e-05, -7.18406154e-05,  1.70453459e-05,
        -3.74394222e-06,  9.13408730e-05, -1.28901345e-04,
        -3.49725196e-05,  1.20742508e-04, -4.53309112e-05,
        -1.83844577e-06, -6.40049548e-05,  1.70367028e-04,
         5.71365526e-05, -5.82094362e-05,  6.71634552e-05,
        -1.04307321e-04],
       [ 1.77397364e-04, -1.57771428e-04,  2.30135542e-04,
        -8.60795990e-05,  3.97352414e-05,  1.97446134e-04,
         1.80800213e-04,  3.18106264e-04, -1.13253554e-04,
        -3.69801419e-05,  4.67390055e-05, -4.31414082e-05,
        -1.00828373e-04, -1.63499004e-04,  8.10111087e-05,
        -8.41527435e-06, -2.56702042e-04,  2.60068555e-05,
         3.92796464e-05,  4.10500033e-05, -2.61716457e-04,
        -1.82589953e-04,  5.29121025e-05, -1.98059075e-04,
         1.45365906e-04],
       [ 1.58521871e-05, -9.29721864e-05,  1.76286994e-04,
        -1.01084130e-04,  8.06458047e-05,  2.66704854e-04,
         1.71460648e-04,  2.96771410e-04, -1.15733848e-04,
        -3.27335147e-05,  7.11074754e-05, -1.75401474e-05,
         2.30759383e-06, -9.18753649e-05,  1.28001382e-04,
         3.43502143e-05, -1.23172082e-04,  4.52305758e-05,
         2.32960815e-06,  6.39415521e-05, -1.71531385e-04,
        -5.90334894e-05,  5.81311178e-05, -6.88160944e-05,
         1.04491293e-04],
       [ 1.58988532e-05,  8.12422659e-05, -1.69480540e-04,
         1.03179584e-04, -8.30993304e-05, -2.91374483e-04,
        -1.58658062e-04, -2.81670626e-04,  1.16461058e-04,
         3.20886393e-05, -8.30190111e-05,  1.01226797e-05,
        -2.37276872e-05,  8.44171227e-05, -1.42244593e-04,
        -4.37210620e-05,  8.69702781e-05, -4.66113852e-05,
         4.89609874e-06, -6.52071394e-05,  1.55161586e-04,
         3.06860093e-05, -5.93197947e-05,  4.47313541e-05,
        -1.02745274e-04],
       [ 2.88879492e-05,  7.65454970e-05, -1.67012695e-04,
         1.03995248e-04, -8.37177940e-05, -3.02323082e-04,
        -1.52703142e-04, -2.74753518e-04,  1.16812342e-04,
         3.18306193e-05, -8.84447945e-05,  6.92933463e-06,
        -3.25707697e-05,  8.18435728e-05, -1.48460560e-04,
        -4.76238165e-05,  7.14700436e-05, -4.69981933e-05,
         7.84326403e-06, -6.55342592e-05,  1.48625142e-04,
         1.88057020e-05, -5.98157894e-05,  3.49976290e-05,
        -1.02521000e-04],
       [-3.89624474e-05,  1.67254722e-04, -2.72407196e-04,
         6.42024243e-05,  5.00471615e-05, -3.48989182e-04,
        -5.85285379e-05,  2.00676557e-04,  2.31483573e-04,
        -9.94338843e-05, -3.06632224e-04,  3.07421033e-06,
         8.57834311e-05,  1.33546419e-04, -1.75101275e-04,
         2.31608174e-05,  4.96522844e-05,  9.66884254e-05,
        -6.25717075e-05, -1.10576817e-04,  3.60092177e-04,
         1.61534936e-05,  7.05410348e-05,  2.77383777e-04,
        -3.39867780e-04]], dtype=float32)>, <tf.Tensor: shape=(25,), dtype=float32, numpy=
array([-5.2481584e-05, -6.8131121e-05,  1.6297202e-04, -1.0535640e-04,
        8.4160187e-05,  3.2353407e-04,  1.4058984e-04,  2.6088080e-04,
       -1.1745302e-04, -3.1391133e-05,  9.9161487e-05, -8.5256397e-07,
        4.8781974e-05, -7.7946534e-05,  1.6034194e-04,  5.4825567e-05,
       -4.2081410e-05,  4.7399790e-05, -1.3194643e-05,  6.5754641e-05,
       -1.3690948e-04,  3.2753487e-06,  6.0732040e-05, -1.7450358e-05,
        1.0285580e-04], dtype=float32)>, <tf.Tensor: shape=(25, 12), dtype=float32, numpy=
array([[-5.43682872e-05, -4.94495580e-05,  1.47375496e-04,
         1.82790573e-05, -2.76790288e-05, -1.07040258e-04,
         6.80110534e-05, -8.96678102e-05, -1.10543988e-04,
         8.99939550e-06, -1.51526008e-04,  1.01415622e-04],
       [ 1.94894369e-07,  3.32479467e-05, -2.60840316e-04,
         1.80846964e-05, -9.37307050e-05,  1.49880128e-04,
        -1.39838376e-05,  1.10033143e-04,  4.56439811e-05,
        -3.07957157e-06,  1.47300234e-04, -6.82868122e-05],
       [ 6.45176769e-05,  5.58651736e-05, -1.52938854e-04,
        -7.08326916e-05,  6.40198850e-05,  5.37728920e-06,
        -5.82797256e-05,  9.11656971e-05,  1.52213252e-04,
         2.18502282e-06,  1.18826414e-04, -9.22916297e-05],
       [ 6.47400084e-05,  9.18950682e-05, -8.84171677e-05,
        -1.35763228e-04,  1.27136707e-04, -2.15598062e-04,
        -4.72048578e-06,  1.84371689e-04,  9.32461553e-05,
         1.03843031e-05,  8.78103892e-05, -1.10876063e-04],
       [ 2.81547659e-06, -3.90326531e-05,  9.97674360e-05,
        -6.16800244e-05,  6.18226113e-05, -1.53175992e-04,
         5.70496331e-06, -1.03858802e-04,  6.83604012e-05,
         2.40267254e-05, -1.43936879e-04,  8.52445664e-05],
       [ 2.78282860e-05, -3.96547257e-05,  1.24893646e-04,
        -1.22166937e-04,  1.27889274e-04, -2.51940568e-04,
        -6.95430663e-06, -1.36755232e-04,  1.48496183e-04,
         3.65131709e-05, -1.90784252e-04,  1.00991150e-04],
       [-2.07050216e-05,  4.60787487e-05, -2.02147494e-04,
         7.90109480e-05, -1.24554936e-04,  1.99716465e-04,
         1.13926235e-05,  1.48115534e-04, -8.58645799e-05,
        -2.35962943e-05,  1.84167890e-04, -9.72182461e-05],
       [ 5.23336203e-05,  6.54327450e-05, -1.25092920e-04,
        -7.59135728e-05,  6.47109191e-05, -5.89750925e-05,
        -2.65301423e-05,  1.22980433e-04,  9.68833774e-05,
         2.24686551e-06,  1.05510138e-04, -9.41399558e-05],
       [ 8.06457501e-06, -4.92268518e-05,  2.37083048e-04,
         4.39077739e-05,  5.97692233e-05,  3.98725788e-05,
        -3.58319230e-05, -1.64299403e-04, -2.03485361e-05,
        -1.16395513e-05, -9.59427489e-05,  5.74863661e-05],
       [ 1.19288401e-04,  1.58042079e-04, -2.90725933e-04,
        -1.88179678e-04,  1.56535621e-04, -1.78599003e-04,
        -5.35615036e-05,  3.23268672e-04,  2.17497509e-04,
         1.04597621e-05,  2.44242896e-04, -2.22639763e-04],
       [-8.33621161e-05, -5.39840112e-05, -2.44324270e-04,
         5.43920069e-05, -2.15165055e-04,  1.26660161e-04,
         5.59977052e-05, -2.27253804e-05, -1.26429723e-05,
         2.12360355e-05, -2.46469663e-05,  8.71846714e-05],
       [ 1.98613216e-05,  2.99771327e-05, -1.33147623e-04,
        -9.00663726e-05,  2.67874148e-05, -1.23375299e-04,
         7.74345426e-06,  7.79204056e-05,  9.93160502e-05,
         2.15636319e-05,  1.62405231e-05, -2.06853256e-05],
       [-4.43055978e-05, -9.07319190e-05,  7.55550500e-05,
         3.87067739e-05, -5.94237172e-05,  3.91961585e-05,
         8.78050741e-06, -1.90007617e-04,  1.13609549e-05,
         1.60325599e-05, -1.57168353e-04,  1.39440526e-04],
       [-4.69002771e-05, -9.65536310e-05,  3.89756518e-04,
         7.28592349e-05,  2.91315628e-05, -4.73233667e-05,
         2.72922844e-05, -2.36683729e-04, -1.43692421e-04,
        -5.38289078e-06, -2.31310638e-04,  1.49915737e-04],
       [-7.84873009e-06, -2.28435783e-05,  1.12504182e-04,
         2.72859470e-05,  1.24148864e-05,  8.28391967e-06,
        -6.64983588e-07, -6.28269481e-05, -4.10965804e-05,
        -5.55996485e-06, -4.85088094e-05,  3.02884491e-05],
       [-3.05500021e-06,  3.03660090e-05, -1.46220336e-04,
        -6.47539928e-05, -1.57897812e-05, -1.09444802e-04,
         3.48435278e-05,  1.00508158e-04,  3.78067489e-05,
         1.82470540e-05,  2.26759403e-05, -1.92218031e-05],
       [ 6.14256205e-05,  6.27044210e-05, -1.88947481e-04,
        -4.56297712e-05,  3.67353132e-05,  6.61218437e-05,
        -6.17666083e-05,  1.11998612e-04,  1.35578870e-04,
        -4.22511448e-06,  1.58749885e-04, -1.12350484e-04],
       [ 6.91988243e-05,  6.76628770e-05,  1.39150332e-04,
        -2.34442305e-05,  1.49871281e-04, -4.10182802e-05,
        -4.83788899e-05,  8.27273398e-05, -1.55874332e-05,
        -2.75827188e-05,  9.58665041e-05, -1.17871088e-04],
       [-1.81621235e-05, -4.50612206e-05,  5.37234446e-05,
         1.00371253e-04, -6.08215632e-05,  2.07555451e-04,
        -3.22013293e-05, -1.09061744e-04, -3.72651484e-05,
        -1.74798624e-05, -1.40374959e-06,  3.20343861e-05],
       [ 5.99720261e-05,  2.47375956e-05,  7.69373873e-05,
        -7.53033164e-05,  1.35655806e-04, -1.09850909e-04,
        -4.38852985e-05, -7.53173163e-06,  9.94531802e-05,
         3.18459183e-06, -8.10717393e-06, -3.15969774e-05],
       [-6.86372960e-06, -1.42574791e-05, -5.12109764e-05,
         4.03217600e-05, -4.83987169e-05,  1.18655473e-04,
        -1.86996931e-05, -2.69288175e-05,  1.18004773e-05,
        -5.07248478e-06,  2.77104664e-05,  3.24950634e-06],
       [-7.13049085e-05, -6.44846295e-05, -6.56238262e-05,
        -1.02812264e-05, -1.08483684e-04, -8.16840839e-05,
         7.30858374e-05, -7.27835577e-05, -9.86309442e-06,
         3.38230420e-05, -1.45858648e-04,  1.32286994e-04],
       [ 1.40134667e-04,  1.26751489e-04, -2.10451413e-04,
        -3.07025737e-04,  2.63110211e-04, -4.23336547e-04,
        -5.56913255e-05,  2.42968759e-04,  3.54858377e-04,
         4.94742599e-05,  8.32892183e-05, -1.36524468e-04],
       [-4.06541440e-05,  2.25359890e-05, -3.13053402e-04,
         1.89835082e-05, -1.59249976e-04,  1.01843347e-04,
         3.96396172e-05,  1.35300346e-04, -5.93996128e-06,
         8.02220529e-06,  1.12883296e-04, -3.31468837e-05],
       [ 3.60448830e-05, -1.32830373e-05,  6.71461021e-05,
        -5.76994134e-05,  8.92640528e-05, -8.06970347e-05,
        -3.48164976e-05, -8.17542168e-05,  1.09381537e-04,
         1.18380767e-05, -6.67513305e-05,  2.81377215e-05]], dtype=float32)>, <tf.Tensor: shape=(12,), dtype=float32, numpy=
array([ 1.0236361e-04,  1.7377641e-04, -3.1538153e-04, -1.0965458e-04,
        9.1439761e-05, -3.1392159e-05, -4.6771849e-05,  3.5995903e-04,
        1.1505943e-04, -1.6132184e-05,  3.3220017e-04, -2.6960039e-04],
      dtype=float32)>, <tf.Tensor: shape=(30, 1), dtype=float32, numpy=
array([[ 13.833496 ],
       [ 13.827963 ],
       [-13.831549 ],
       [-13.828077 ],
       [ 13.830149 ],
       [ 13.819731 ],
       [ 13.833327 ],
       [-13.832193 ],
       [ 13.84932  ],
       [ 13.828808 ],
       [-13.870314 ],
       [-13.827028 ],
       [-13.864774 ],
       [-13.8328285],
       [ 13.903023 ],
       [-13.830891 ],
       [-13.871315 ],
       [ 13.82989  ],
       [ 13.827679 ],
       [ 13.826073 ],
       [ 13.826628 ],
       [ 13.838752 ],
       [-13.827595 ],
       [ 13.831131 ],
       [ 13.829469 ],
       [-13.834417 ],
       [-13.829695 ],
       [ 13.827673 ],
       [ 13.826803 ],
       [-13.933418 ]], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-13.825033], dtype=float32)>, <tf.Tensor: shape=(34, 40), dtype=float32, numpy=
array([[ 1.7311470e-07, -2.0923846e-08, -5.8881540e-09, ...,
         2.9826191e-07,  8.4945619e-08,  2.6934248e-07],
       [-1.8763055e-07,  3.9191761e-08,  2.0798367e-07, ...,
        -1.8354403e-07, -1.3439328e-07,  3.0181715e-07],
       [-8.9746766e-07,  1.9965228e-07,  7.6150371e-07, ...,
         2.5977505e-07, -1.7236468e-07,  2.1377316e-06],
       ...,
       [ 8.8564666e-06,  8.9916165e-07, -1.3065475e-05, ...,
        -1.0937044e-06, -1.8235903e-06, -3.7178790e-05],
       [-5.7557900e-06,  1.1312213e-06, -5.6525951e-06, ...,
        -6.2978716e-06, -1.2340753e-06, -1.3485609e-05],
       [-4.5091992e-06, -1.4867544e-06,  3.5336489e-06, ...,
        -1.6653064e-05, -1.0172924e-05, -1.9051011e-05]], dtype=float32)>, <tf.Tensor: shape=(40,), dtype=float32, numpy=
array([-3.1087245e-07, -2.9484539e-07, -2.4386668e-07,  1.9085100e-07,
       -3.0934336e-06, -2.6263513e-07, -2.9950951e-07,  8.3958025e-07,
       -1.2380035e-06, -1.1526503e-06, -8.9999212e-08,  2.6828332e-07,
        2.5079032e-07,  5.9574398e-07, -3.0934746e-07, -1.6980794e-06,
        8.6567627e-07, -3.4636480e-07,  5.3803592e-07, -3.0181559e-06,
        2.4909781e-07, -3.9441863e-07, -6.2700383e-06, -7.3709612e-07,
       -6.7147153e-07, -2.7299939e-06,  4.3222207e-07,  6.6894063e-07,
        3.0180365e-07,  9.9072520e-07,  2.0485493e-07,  2.8665243e-06,
       -1.6505271e-07, -1.3541669e-06, -8.9515680e-08,  7.0106466e-07,
        1.1891807e-06, -1.1102255e-06, -4.7564353e-07, -3.5048211e-06],
      dtype=float32)>, <tf.Tensor: shape=(40, 80), dtype=float32, numpy=
array([[ 4.1431886e-07, -1.0835282e-07, -4.5849319e-07, ...,
        -1.9857467e-08, -9.6861777e-07, -6.3508236e-07],
       [ 2.4560748e-08, -6.1648748e-08, -1.2679710e-07, ...,
        -6.0625595e-08, -2.0451296e-07,  5.3488480e-07],
       [ 3.1946939e-07, -1.8468478e-07,  2.9772210e-08, ...,
        -2.8671934e-08, -5.0021180e-07, -5.7393174e-07],
       ...,
       [ 7.9645605e-07,  6.6042435e-07, -4.2548302e-07, ...,
        -9.9662785e-08, -4.7067218e-07, -1.1009536e-07],
       [ 1.3424575e-06,  4.9008867e-07, -9.8287137e-07, ...,
        -6.5495163e-07, -1.7884755e-06, -9.1090965e-07],
       [-8.7878504e-08, -2.0913401e-07,  5.1887679e-07, ...,
        -1.3516620e-07,  2.1261401e-06,  1.2944590e-06]], dtype=float32)>, <tf.Tensor: shape=(80,), dtype=float32, numpy=
array([-2.1369601e-07, -3.3785739e-07, -1.4177739e-07,  1.9210441e-07,
        1.8776214e-07,  8.6569764e-07,  9.1157921e-07, -3.4318373e-07,
       -7.3888941e-08,  1.6090632e-07, -7.2201283e-07,  5.2226903e-07,
       -5.5818316e-07,  3.4541972e-07, -6.1454045e-08, -1.0919314e-07,
       -1.1930570e-07,  4.7566701e-07,  9.4833865e-09,  2.4746254e-07,
       -2.0954344e-07, -1.6312396e-07, -1.0952650e-06,  8.5964757e-07,
       -2.7797699e-07,  2.9828686e-07, -2.4099666e-07,  1.2303644e-08,
       -1.1174836e-07, -5.8149055e-07,  4.5174488e-07,  8.2237989e-07,
        9.1576186e-07, -3.2709772e-07,  4.1170324e-08, -8.1465532e-07,
        5.3766939e-08, -6.3264537e-07, -1.3793965e-08,  6.3115669e-07,
       -7.7855225e-07,  1.1375212e-07, -1.7708874e-06,  8.6034834e-07,
       -5.2559443e-08, -1.0839060e-06, -3.8966590e-08, -1.1329189e-06,
       -9.0695761e-07,  9.2272762e-07, -6.9295186e-07, -2.3049779e-07,
       -2.8849246e-07, -7.0194233e-08,  1.0615829e-06, -9.4665097e-07,
       -3.8459677e-07,  5.3892830e-07, -2.9314884e-07,  3.7157400e-07,
        1.7211350e-06, -1.0871312e-06,  3.3389460e-07, -5.0478536e-07,
        9.5282996e-07, -8.2506961e-07,  5.7433510e-07, -1.2317553e-06,
        6.6235316e-07,  5.3990595e-07, -1.1881760e-06,  2.1288406e-07,
       -1.5104652e-07, -6.3136008e-07,  4.7440420e-07, -3.7532345e-07,
        3.4649520e-07, -6.4853066e-07, -1.4041477e-06,  3.9811073e-07],
      dtype=float32)>, <tf.Tensor: shape=(80, 30), dtype=float32, numpy=
array([[-4.7115958e-07, -3.3145713e-07, -1.5643262e-06, ...,
         5.6321426e-07,  5.9751625e-07, -4.6457387e-07],
       [ 7.0069132e-07, -1.2736169e-06, -8.4411866e-08, ...,
         1.0263295e-07, -6.6352646e-07, -1.3939513e-06],
       [-7.8574533e-07,  1.4991664e-06,  6.7822282e-07, ...,
        -6.4504309e-07,  4.2512380e-07,  1.4846739e-06],
       ...,
       [ 3.0204319e-07,  4.2119228e-07,  1.4025757e-06, ...,
        -6.5471983e-07, -6.4956635e-07,  1.0032556e-06],
       [ 6.3724741e-08, -9.1635521e-08,  2.1036541e-08, ...,
        -2.5096151e-07,  2.4066901e-07, -2.4225909e-07],
       [ 5.0285314e-07, -3.7915007e-07,  1.5311984e-06, ...,
        -2.1689704e-07, -5.3591981e-07,  6.9498242e-08]], dtype=float32)>, <tf.Tensor: shape=(30,), dtype=float32, numpy=
array([-4.1035437e-07,  2.4565825e-06,  4.0978807e-07,  1.1335687e-06,
        8.6225862e-07, -2.2132070e-07,  2.3359883e-07, -7.5216337e-07,
        1.7130097e-07, -2.6325625e-07, -5.9659214e-08,  5.0178494e-07,
        3.8178018e-07,  9.4817244e-07, -1.2307705e-06,  2.0268471e-06,
       -1.9900315e-07,  5.5467302e-07, -7.8709803e-08, -1.6610363e-07,
       -5.6744307e-07, -6.8968393e-07,  1.8342860e-08, -1.9190762e-07,
       -9.8579721e-07,  3.4144449e-07, -5.4818975e-07,  1.9492595e-06,
       -5.0359176e-08, -7.4359832e-07], dtype=float32)>, <tf.Tensor: shape=(30, 30), dtype=float32, numpy=
array([[-5.17171259e-07, -8.83387429e-07, -1.12643886e-06,
         3.62702366e-07,  4.31327976e-07, -1.80125312e-07,
        -1.89343268e-06, -1.02547779e-06,  7.51455900e-07,
        -1.94838321e-07, -9.37005495e-08, -1.01399849e-08,
        -4.24711743e-07, -3.16460671e-07, -1.92644234e-06,
        -7.37907385e-07,  3.44616382e-07, -6.88860894e-07,
        -8.45909881e-07, -4.91771289e-07,  1.37651386e-06,
        -1.12331600e-06,  5.58278430e-07,  7.47438889e-07,
        -1.37718487e-07,  1.18709568e-06,  6.81798952e-07,
        -5.75649722e-07, -6.17951059e-07,  1.04266451e-06],
       [ 5.63495490e-08, -1.20763374e-07, -2.38857274e-06,
         5.76724744e-07,  1.08923700e-06, -1.36231529e-06,
         5.55763961e-07,  1.04009905e-06,  1.42178624e-06,
        -1.84201497e-06, -2.62252343e-07, -2.29805366e-07,
         3.49677549e-07,  4.27010207e-07,  1.77610616e-07,
         1.32956160e-07,  4.98316808e-07, -4.84386590e-07,
         3.34142356e-07, -1.92715873e-07, -4.27658620e-08,
        -5.15859028e-07,  5.18587967e-07, -1.30488985e-07,
         2.28015750e-07,  1.21544389e-07, -4.50014511e-07,
        -1.06163498e-07,  4.81116786e-07,  2.01582395e-09],
       [ 3.79952866e-07, -1.73315783e-07,  2.23437723e-07,
         1.68843542e-06, -4.99437022e-07,  5.50284653e-07,
         4.22979809e-07, -1.05272102e-06, -8.32395699e-07,
        -5.26361191e-07,  1.51531623e-07,  6.08722587e-07,
        -2.77181584e-07, -2.76921753e-07, -1.42048202e-06,
        -5.31564467e-07,  7.58695762e-07, -1.85230746e-08,
         3.67921416e-07, -2.35636350e-07, -5.20211756e-07,
        -8.72103897e-07, -1.00357670e-06,  8.09593672e-08,
        -1.46835575e-08, -4.12195249e-07,  3.72342981e-07,
        -1.03271248e-07,  1.42676356e-07, -1.11661514e-07],
       [-6.39939913e-07, -1.46952868e-06, -2.13310591e-06,
         1.26242116e-06,  5.76934667e-07, -2.46900697e-07,
        -4.92196534e-07, -7.93927825e-07,  2.68998178e-07,
        -6.06667754e-07, -2.89467380e-07, -5.24375025e-07,
        -1.19308129e-06, -2.89130014e-07, -1.97615077e-06,
        -4.78475727e-07,  9.88244892e-07,  1.32828507e-06,
        -4.37968708e-07, -9.38478195e-07,  5.12700638e-07,
        -1.75704315e-06,  2.95589473e-07,  1.02256024e-06,
        -1.86904145e-07, -1.65343153e-07,  1.74064689e-06,
         2.24348753e-07, -9.29508246e-07,  5.28049100e-07],
       [ 3.32820434e-07, -5.30979605e-07, -3.13536077e-07,
         6.79373329e-07,  1.56112350e-07,  3.37595935e-07,
         5.06526305e-07, -1.06160769e-08, -4.26031249e-07,
        -5.24544362e-07,  1.75281158e-07, -1.21165073e-07,
        -3.36212509e-07, -2.63654698e-07, -7.91463719e-07,
        -2.75901698e-07,  3.58636100e-07,  5.08746666e-07,
         1.19026453e-07, -6.32806973e-07, -6.00406963e-07,
        -8.58802423e-07, -6.32638830e-07,  3.95359336e-07,
        -5.32090603e-08, -3.67968198e-07,  8.55308826e-07,
         5.14066585e-08, -1.66055344e-07, -2.09135806e-07],
       [-3.19295765e-07, -8.33569743e-07, -2.53369944e-06,
         3.08332233e-07,  3.83475395e-07, -6.18588103e-07,
        -1.38589508e-06, -6.08275059e-07,  8.60652563e-07,
        -8.17459181e-07, -2.04917569e-07, -8.86842599e-07,
        -5.02392027e-07,  1.06416849e-07, -1.30897251e-06,
        -4.78695597e-07,  6.59621264e-08, -8.05460331e-07,
        -5.25331757e-07, -3.31280091e-07,  8.52596486e-07,
        -1.21774337e-06,  2.31089757e-07,  5.89314482e-07,
         1.93025002e-07,  1.02839215e-06,  3.35160934e-07,
        -1.74339448e-07, -3.80417475e-07,  6.10555730e-07],
       [-7.24485631e-07, -8.26622681e-07, -6.60092553e-07,
         4.27339643e-07,  3.29463091e-07,  1.72487603e-07,
        -8.59259330e-09, -5.24855523e-07, -1.33347839e-08,
         4.70081488e-07, -2.70030512e-07, -3.99739918e-07,
        -8.16898705e-07, -6.75449598e-08, -1.25862914e-06,
        -6.59124566e-07,  7.75141530e-07,  1.98057478e-06,
        -2.67064109e-07, -3.65911262e-07,  5.20207209e-07,
        -7.19811737e-07,  6.40896474e-07,  5.64630682e-07,
        -2.68606527e-07, -5.77818355e-07,  1.55025077e-06,
         7.85416319e-07, -4.33466028e-07,  3.78599736e-07],
       [-2.45658754e-07, -7.03453793e-07,  8.51691823e-07,
         1.35194477e-06, -6.42577902e-07,  4.58771297e-07,
        -2.15960782e-07, -1.61882008e-06, -8.58350120e-07,
        -2.42245818e-07, -2.49547838e-07,  1.31769173e-06,
        -9.18696855e-07, -6.96810503e-07, -1.18809749e-06,
         4.45741506e-07,  9.93750746e-07,  1.08235986e-06,
        -5.40334071e-08, -9.02592376e-07,  1.07099311e-07,
        -1.02606407e-06, -2.20621445e-07,  6.81744893e-07,
        -1.15913750e-07, -5.57747114e-07,  1.32240507e-06,
        -3.98487060e-07, -9.26808639e-07,  3.63984356e-07],
       [-5.75179513e-07, -7.86217413e-07, -1.04675928e-06,
         1.08587436e-07,  5.94836820e-07, -2.67534318e-07,
        -6.10640427e-08, -6.66880737e-07,  2.54843144e-07,
        -8.05637114e-07, -6.23831852e-07, -5.74761430e-08,
        -8.10740573e-07,  3.17474132e-07, -1.20862387e-06,
         2.88607112e-07,  6.33880177e-07,  1.22342487e-06,
         3.39917278e-07, -3.03356614e-07,  3.75754325e-07,
        -1.15175885e-06,  9.39232734e-07,  5.07540562e-07,
         2.55322220e-07, -1.70700815e-07,  1.17768275e-06,
         2.51733269e-07, -4.40863573e-07,  7.00292446e-07],
       [-8.85677821e-07, -1.04175911e-06, -2.24330279e-08,
         7.44240509e-08, -5.34315348e-07,  5.68098187e-07,
        -1.10299754e-06, -1.43186844e-06, -5.76614582e-07,
         1.69713462e-06, -3.36548254e-07, -8.01753345e-07,
        -1.24806661e-06, -4.67551786e-07, -1.92417019e-06,
        -9.59176987e-07,  2.26394036e-07,  1.64809512e-06,
        -1.00646287e-06, -4.86553063e-07,  9.54130201e-07,
        -8.62931415e-07,  1.82476583e-07,  8.74932653e-07,
        -3.64152214e-07, -1.50454525e-07,  1.74935406e-06,
         6.43679300e-07, -1.21290179e-06,  6.14078601e-07],
       [-5.90073114e-07, -1.11574127e-06, -9.63294838e-07,
         2.66910035e-07, -2.73351873e-07,  5.25966925e-07,
        -1.91240474e-07, -9.15818362e-07, -7.13147642e-07,
         1.52577138e-06, -2.45428140e-07, -1.39912549e-06,
        -1.15282853e-06, -4.01456703e-07, -1.22138181e-06,
        -8.72202918e-07,  1.79086840e-07,  2.19617914e-06,
        -5.61008562e-07, -4.12218526e-07,  2.09673658e-07,
        -1.05547406e-06, -5.26297299e-08,  7.62098182e-07,
         1.59948826e-07, -6.13398015e-07,  1.74602246e-06,
         8.87089357e-07, -8.01310875e-07,  2.93120308e-07],
       [ 6.81311178e-07,  2.20000018e-08,  1.65316521e-07,
        -1.64490785e-07,  1.69393303e-07, -2.04075718e-07,
         1.90625053e-06,  7.41594533e-07, -4.77152639e-07,
        -8.72101282e-07, -2.70002715e-07,  2.06643108e-07,
         4.83272942e-08,  1.55858260e-07,  8.18968147e-07,
         7.98791348e-07,  3.20079721e-07,  7.17251851e-07,
         7.35320214e-07, -9.91188926e-08, -1.34897812e-06,
         1.64915264e-07, -5.21131483e-07, -5.66900326e-07,
         8.21647177e-08, -1.05523384e-06,  2.65829954e-08,
         5.73988132e-07,  6.79704684e-08, -4.62257901e-07],
       [-7.63313039e-08, -8.46265834e-07, -3.48086996e-06,
         5.93209961e-07,  1.32814739e-06, -9.43090527e-07,
        -8.76838044e-07,  4.14291492e-07,  1.52462144e-06,
        -9.35787625e-07, -2.49311682e-09, -1.16223998e-06,
        -2.05270595e-07,  1.24792038e-07, -1.40341103e-06,
        -1.01332103e-06,  3.03041531e-07, -3.26260619e-07,
        -4.29746933e-07, -3.60790466e-07,  7.69248800e-07,
        -1.05836330e-06,  5.86398414e-07,  4.86052045e-07,
         6.74526888e-08,  8.13412214e-07,  3.57401206e-07,
         3.93047799e-08, -2.53259913e-07,  5.43000510e-07],
       [-1.01031196e-06, -9.76140058e-08, -3.31678564e-07,
         3.74257070e-07,  4.66506776e-07,  2.31993624e-07,
        -6.95009021e-07,  7.35845333e-07,  9.19415868e-07,
         6.75429590e-07,  3.59559280e-08, -6.44461409e-08,
         3.58104728e-07,  3.68641679e-07,  5.51942207e-08,
        -9.75760031e-07, -2.08841044e-07, -1.61593647e-07,
        -1.53320400e-07,  9.13505360e-08,  1.53652536e-06,
         9.12237624e-07,  1.21100766e-06, -2.33812614e-07,
         3.79871949e-07,  2.71994907e-07,  8.70085444e-08,
         6.52668575e-07,  8.27007113e-07,  1.82197851e-07],
       [-1.66874145e-07, -6.72300416e-07,  1.67907160e-06,
        -5.95262577e-08,  2.58851571e-08,  5.90472382e-07,
         1.33719823e-07, -4.92207164e-07, -7.43071723e-07,
        -1.26028056e-07, -1.52914481e-08,  1.52036341e-06,
        -7.10128575e-07, -5.77558922e-07,  1.39844474e-08,
         1.21988796e-06,  4.75595982e-07,  1.63309585e-06,
         4.00016660e-08, -9.02147463e-07, -7.05478556e-08,
        -2.31382799e-07,  4.35605187e-07,  5.13698240e-07,
        -2.18322782e-07, -7.03440662e-07,  1.33033814e-06,
        -2.20145068e-07, -1.09991572e-06,  5.80550079e-08],
       [ 3.06099935e-07, -6.31626904e-07,  2.04166213e-07,
        -1.27759870e-06, -4.36130563e-08,  4.71828059e-07,
         5.16500506e-07, -6.80514745e-07, -8.92850721e-07,
        -2.97652491e-07, -5.22462699e-07, -8.15188571e-07,
        -8.78221215e-07,  4.52621691e-08, -3.23231205e-07,
         9.33355068e-07, -2.80211680e-07,  8.67843937e-07,
         3.36897784e-07, -8.24487216e-08, -9.10300741e-07,
        -8.53680262e-07, -4.58270904e-07,  4.74244814e-07,
         1.49758733e-07, -3.62458366e-07,  8.74994100e-07,
         3.73329215e-07, -6.98799113e-07,  1.67029526e-07],
       [-3.32832997e-07,  1.05675065e-06, -1.43495231e-06,
        -4.94535357e-07,  1.13770375e-06, -8.62110255e-07,
        -7.58223052e-07,  2.13822113e-06,  2.13963381e-06,
         4.13953387e-07,  3.94924086e-07, -1.05244442e-06,
         1.39371093e-06,  6.74204671e-07,  8.55984922e-07,
        -1.23359052e-06, -8.27669851e-07, -1.54179179e-06,
        -2.45487286e-07,  9.18883075e-07,  1.13766282e-06,
         1.38838254e-06,  1.11028476e-06, -5.37362098e-07,
        -3.02332275e-08,  1.20500249e-06, -1.54663257e-06,
         4.91899641e-08,  1.14508350e-06, -6.95876565e-08],
       [ 1.10391539e-07, -1.08629467e-06, -3.33838955e-07,
        -3.46407830e-07, -6.81322547e-07,  8.66060304e-07,
         1.46495813e-06, -1.27451676e-06, -1.65757706e-06,
        -5.21958327e-07, -8.80850394e-07, -1.42696979e-06,
        -1.42330828e-06,  2.55109939e-07, -7.79725667e-07,
         1.03450270e-06, -2.02647925e-08,  2.19411368e-06,
         7.70145107e-07, -3.81048636e-07, -1.51372387e-06,
        -1.68481881e-06, -8.96481254e-07,  5.86912563e-07,
         4.65781312e-07, -1.47125752e-06,  1.72085265e-06,
         1.04127821e-06, -5.38100494e-07,  3.19022320e-09],
       [-8.53419465e-07,  7.47492209e-07,  5.58125294e-07,
        -3.58082502e-07,  2.29206563e-07, -1.03841387e-07,
        -3.72740146e-07,  9.37562163e-07,  7.86189958e-07,
         1.19924198e-06,  9.36034255e-08,  1.08869095e-07,
         6.93232153e-07,  3.97721408e-07,  9.95586333e-07,
        -6.34857201e-07, -4.07277184e-07, -1.27031569e-07,
        -1.17611648e-07,  7.30401212e-07,  1.09297821e-06,
         1.55779605e-06,  1.20396976e-06, -5.05043829e-07,
         4.78498805e-08,  2.58164363e-07, -6.84072347e-07,
         4.02659083e-07,  8.56240490e-07,  6.90905608e-08],
       [-3.15009316e-07, -9.42292559e-07, -1.03109824e-06,
         6.32866715e-07, -1.39624149e-07,  1.81700244e-07,
         1.11880468e-07, -7.41939289e-07, -5.59756131e-07,
         3.80594457e-07, -1.49381236e-07, -1.22806546e-06,
        -9.90331046e-07, -5.35297033e-08, -1.63797745e-06,
        -5.11946212e-07,  3.52131167e-07,  1.81890334e-06,
        -3.45608242e-07, -3.83737529e-07, -1.78533526e-07,
        -1.13473550e-06, -2.63967848e-07,  6.29833892e-07,
        -3.34980029e-07, -6.47046647e-07,  1.38339226e-06,
         7.23495646e-07, -8.92661035e-07, -1.44666856e-08],
       [-1.56626641e-08, -1.04527726e-06, -2.83313921e-06,
         1.74542356e-06,  4.78621985e-07, -7.53972415e-07,
        -1.29166699e-06, -1.02816625e-06,  4.85409601e-07,
        -7.88730858e-07, -1.54659290e-08,  3.27340615e-07,
        -6.14987584e-07, -3.81560511e-07, -2.04334629e-06,
        -7.60098089e-07,  1.17095954e-06, -1.11392666e-07,
        -6.08645337e-07, -7.54468772e-07,  9.17268039e-07,
        -1.75068965e-06,  1.92505922e-07,  6.67986683e-07,
         1.39057789e-07,  8.03174999e-07,  8.14045734e-07,
        -6.67623283e-07, -7.51496657e-07,  8.42517693e-07],
       [ 1.86779630e-07, -3.24957142e-07,  3.70022190e-06,
        -6.44183615e-07, -1.86697469e-06,  1.62054653e-06,
         6.84626343e-07, -1.92053608e-06, -2.64278106e-06,
         9.11039592e-07, -3.40941483e-07,  6.01881311e-07,
        -1.06197092e-06, -6.22620234e-07,  9.79609638e-08,
         1.37074812e-06, -2.54954017e-07,  1.53294309e-06,
         9.23013488e-08, -5.09103074e-07, -1.16202443e-06,
        -2.94582776e-07, -1.28187457e-06,  4.31285429e-07,
        -1.95564695e-07, -1.27110957e-06,  1.29613829e-06,
         2.29667876e-07, -1.08314373e-06, -3.28332163e-07],
       [-5.69234714e-07, -1.61547817e-07,  7.53224299e-07,
        -6.69420444e-07,  1.56499482e-07,  2.24732332e-07,
         7.90818262e-07,  2.09607094e-07, -2.24786220e-07,
         3.67743212e-07, -3.14446652e-07, -3.57584526e-07,
        -2.38386363e-07,  2.77315934e-07,  4.07746086e-07,
         2.53741121e-07, -1.27608530e-07,  1.55048167e-06,
         1.46347247e-07,  2.83754815e-08, -1.23530725e-07,
         3.69972611e-07,  5.70351233e-07, -4.67288324e-08,
        -1.88879397e-07, -8.88422164e-07,  8.35991727e-07,
         1.01900821e-06, -1.11191433e-07, -4.28255404e-08],
       [ 4.72050488e-09, -2.05497400e-07, -7.83459541e-07,
         8.58722842e-07,  5.32836282e-07, -3.66707042e-07,
        -9.06212506e-07,  5.24375707e-08,  8.20631044e-07,
        -1.46130986e-07,  4.47287391e-07,  6.47311651e-07,
         1.95869632e-07, -5.20713343e-07, -5.41552140e-07,
        -8.36534355e-07,  5.83612973e-07, -9.72008934e-07,
        -4.87639966e-07, -3.79959573e-07,  7.72129511e-07,
        -2.83486486e-07,  1.47010780e-07,  3.29915707e-07,
        -9.62739861e-08,  6.92781555e-07,  3.62865009e-08,
        -7.25955886e-07, -2.81684009e-09,  2.90945309e-07],
       [ 4.89525632e-07,  2.08864364e-07,  1.44712840e-06,
        -5.74160538e-07, -1.31108607e-07,  1.97920343e-07,
         1.52576445e-06,  6.01733518e-07, -9.47053081e-07,
        -1.16879505e-07, -1.31235780e-07,  4.93136611e-07,
        -6.84125325e-08, -1.82351556e-08,  1.18520813e-06,
         9.88005922e-07,  1.78531110e-07,  1.45297827e-06,
         4.79021651e-07, -1.37979569e-07, -1.19057154e-06,
         3.94646975e-07, -2.16963471e-07, -2.13735802e-07,
        -9.63821805e-08, -1.11847214e-06,  2.57372051e-07,
         3.42828855e-07, -2.15687052e-07, -5.55987242e-07],
       [-7.64467245e-07,  9.38772757e-08,  1.63684217e-06,
         7.11562791e-07, -9.96423296e-07,  8.98328835e-07,
        -5.31003366e-07, -1.00203920e-06, -8.33599302e-07,
         1.61489652e-06, -2.75375953e-08,  6.45548653e-07,
        -2.24489611e-07, -2.02247520e-07, -1.09814573e-06,
        -5.85926841e-07, -1.63175059e-07,  7.18410888e-07,
        -1.60567524e-07,  1.41909368e-07,  7.20954176e-07,
         8.48120294e-08,  7.24410540e-08, -3.43363808e-08,
         2.04473068e-07, -1.98680311e-07,  4.02831375e-07,
         1.66906887e-07,  3.46447564e-08,  2.62344855e-07],
       [-1.20193704e-08,  1.19844947e-06,  2.78019479e-06,
        -2.10853477e-06, -6.37450512e-07,  1.04694834e-06,
         5.15685088e-07,  1.18189223e-06, -5.12042106e-07,
         2.19115236e-06,  2.33830974e-07, -7.86513795e-07,
         7.38089000e-07,  2.91393746e-07,  2.04725393e-06,
        -7.93435788e-08, -1.65161782e-06, -1.54102452e-07,
         6.40119993e-08,  9.66209541e-07, -3.42100975e-07,
         2.37861263e-06, -2.04115679e-07, -7.02558964e-07,
        -9.07553357e-08, -2.35911600e-07, -9.82659913e-07,
         9.22845288e-07,  8.27318445e-07, -7.24201300e-07],
       [ 1.11697318e-10,  9.47528122e-07, -1.51565166e-06,
        -5.09596987e-07,  1.46274920e-06, -1.35424739e-06,
        -1.22464229e-07,  1.83624218e-06,  2.04615526e-06,
        -1.24434814e-06,  3.68171698e-08,  3.58445021e-07,
         1.14937268e-06,  9.30132046e-07,  1.26503346e-06,
         4.05458422e-08,  3.41971713e-08, -1.56927490e-06,
         5.55945007e-07,  7.89282410e-07,  7.36490279e-07,
         1.04597734e-06,  1.51991753e-06, -9.25586562e-07,
         1.62509281e-07,  9.56601980e-07, -1.71011175e-06,
        -3.06462567e-07,  9.62298941e-07,  1.79045969e-07],
       [-4.65421977e-08,  4.29142460e-07,  2.41820908e-06,
         6.30870261e-07, -1.09684765e-06,  8.56554891e-07,
         4.87137811e-07, -8.51995708e-07, -1.26151019e-06,
         7.57201974e-07, -6.54442829e-08,  1.41564306e-06,
         6.58818919e-08, -2.03239466e-07,  2.38128415e-08,
         2.60002935e-07,  2.94300946e-07,  3.96590792e-07,
         3.86398540e-07,  1.25646864e-07, -2.69877717e-07,
         3.80017411e-07, -3.80889162e-07, -2.49460470e-07,
         8.18403052e-08, -6.90514071e-07,  1.00925369e-07,
        -1.39917574e-07,  1.83153730e-07, -1.30593776e-07],
       [-4.89248464e-07,  3.62966333e-08,  4.61759726e-07,
        -7.87093768e-07, -1.34767220e-07,  3.09103029e-07,
         4.85999294e-08,  3.97133022e-07, -2.35413438e-07,
         1.98426505e-06,  1.71420140e-07, -1.24396433e-06,
        -4.61961065e-08, -1.15046767e-07,  2.47389124e-07,
        -9.23599544e-07, -3.40450754e-07,  1.12644796e-06,
        -7.90361696e-07,  2.09658026e-07,  2.36807608e-07,
         7.03238641e-07,  8.26301516e-09,  1.19730245e-07,
        -4.78422180e-07, -3.32036279e-07,  5.28158466e-07,
         8.81162180e-07, -2.86287332e-07, -8.38460323e-08]], dtype=float32)>, <tf.Tensor: shape=(30, 30), dtype=float32, numpy=
array([[ 5.05866581e-07,  1.25635654e-06,  6.86171461e-07,
        -1.24033500e-06, -3.74684930e-07, -2.19349147e-07,
        -4.88527974e-07,  1.37570544e-06,  4.13681505e-07,
         1.22990980e-06,  4.38501957e-07, -6.08039386e-07,
         1.41653379e-06,  8.30660625e-08,  2.27435066e-06,
        -5.37597259e-07, -1.37484926e-06, -2.64430241e-06,
        -5.48316848e-07,  9.44987391e-07,  4.43279191e-08,
         2.09751306e-06, -6.85442274e-07, -6.58952843e-07,
        -1.09941823e-07,  1.13921101e-06, -1.92702282e-06,
        -3.47284953e-07,  9.06457501e-07, -3.87366100e-07],
       [ 1.08570214e-06, -9.77985792e-07, -1.27424755e-07,
         2.54581920e-07,  1.09268683e-08, -1.57322688e-09,
         7.86406247e-07, -1.19844913e-06, -1.06947050e-06,
        -1.49825780e-06, -6.02844068e-08,  2.63207738e-07,
        -9.99642680e-07, -6.46040974e-07, -1.38663188e-06,
         9.41646931e-07,  6.81939582e-07,  7.83971814e-07,
         1.95489434e-07, -9.40425139e-07, -1.44810338e-06,
        -1.82393990e-06, -1.13639362e-06,  6.15045280e-07,
        -3.32023262e-07, -5.50375546e-07,  1.11712257e-06,
        -4.19268247e-07, -1.38515156e-06,  1.19942257e-07],
       [-3.88958199e-09,  2.96028645e-07,  3.22704773e-06,
        -5.75210606e-07, -9.51942525e-07,  7.09507503e-07,
         4.26384247e-07, -7.39421807e-07, -1.05083200e-06,
         3.40011468e-07, -6.06930598e-07,  1.53013343e-06,
        -1.52125295e-07, -2.93264804e-07,  1.34728145e-06,
         1.26145858e-06,  2.03835071e-07,  3.21176913e-07,
         2.68084591e-07, -2.04690593e-07, -1.47898987e-07,
         3.04509285e-07, -1.20652999e-07,  4.75385633e-08,
         1.09348406e-08, -7.14105909e-07,  4.18453340e-07,
        -4.79395680e-07, -3.61412788e-07, -7.65002994e-08],
       [ 2.28473851e-07,  1.23722444e-07,  2.77590630e-07,
         6.89668354e-07, -5.81050585e-08, -1.48131818e-07,
        -1.44910211e-06, -4.05123245e-07,  3.87483254e-07,
         3.33037121e-07,  4.99831515e-07,  9.97870984e-07,
         2.38072829e-07, -7.19659681e-07, -7.43736678e-07,
        -5.80707820e-07,  2.13801599e-07, -1.41927615e-06,
        -7.53806034e-07, -2.83068232e-07,  8.04877516e-07,
         1.16121129e-07, -1.43746760e-07,  2.22788998e-07,
        -1.52949951e-07,  1.10765791e-06, -4.78998800e-07,
        -1.07910500e-06, -4.25719065e-07,  4.00149275e-07],
       [ 2.07818800e-07,  9.58718829e-08,  1.86697639e-06,
         6.37752180e-08, -5.99323585e-07,  4.64423408e-07,
        -1.78431080e-07, -8.84216036e-07, -9.21721835e-07,
         3.82689251e-07, -4.75664308e-09,  1.12941859e-06,
        -2.71494685e-07, -4.46045874e-07,  2.01528948e-07,
         5.09111885e-07,  4.00007167e-07,  5.39936536e-07,
        -3.78794240e-08, -2.53496012e-07, -3.36472823e-08,
        -1.38164381e-07, -1.41223808e-07,  1.76263683e-07,
        -4.95344352e-08, -1.74818695e-07,  3.30547522e-07,
        -4.14237604e-07, -6.40468443e-07, -3.53733576e-08],
       [ 6.51033019e-07,  7.49563014e-07,  7.11267944e-07,
        -4.63047627e-07, -7.16269767e-07,  3.79474898e-07,
         2.57315918e-07,  5.96637335e-07, -2.25304689e-07,
         8.89185799e-07,  2.78528631e-07, -9.59456884e-07,
         7.72938904e-07, -5.80684798e-08,  7.35664401e-07,
        -8.48400873e-07, -1.02315425e-06, -1.66638301e-06,
        -2.52907910e-07,  4.85105772e-07, -4.96935002e-07,
         1.00382601e-06, -1.27789008e-06, -5.22847984e-07,
        -8.30987403e-08,  2.92771517e-07, -1.08062864e-06,
         3.09767415e-07,  6.73961779e-07, -5.02735134e-07],
       [-6.22042535e-07, -3.18329626e-07, -2.15391265e-06,
         1.49398556e-06,  7.11520102e-07, -9.08079528e-07,
        -2.05828292e-06, -1.50258998e-07,  1.41082080e-06,
        -1.64254232e-07,  1.48366780e-07,  9.11280210e-07,
         2.58070543e-09, -2.55344844e-07, -1.14861496e-06,
        -8.45884358e-07,  9.27874112e-07, -1.29353793e-06,
        -9.37068876e-07, -2.69119056e-07,  1.68028441e-06,
        -3.67453367e-07,  8.34597301e-07,  4.11347514e-07,
        -1.25201609e-08,  1.39894064e-06, -1.96369783e-07,
        -1.12645489e-06, -1.49216078e-07,  7.76278739e-07],
       [-1.27785086e-07,  5.89228137e-07,  1.64894095e-06,
        -9.04279830e-07, -1.33887966e-07,  9.79358035e-08,
        -1.08103950e-06, -3.98768982e-08,  3.70483235e-07,
         6.51791879e-07,  1.01183133e-07,  8.43234147e-07,
         4.52058771e-07, -1.98886625e-07,  8.55452015e-07,
         2.28626050e-07, -6.12893871e-07, -7.76276408e-07,
        -2.92960976e-07,  8.68325003e-08,  9.01403610e-07,
         9.25622601e-07,  7.52201231e-07,  3.77291975e-08,
         3.58271919e-08,  7.09725612e-07, -5.01284944e-07,
        -6.32428282e-07, -5.94850569e-08,  2.58987200e-07],
       [-3.82691667e-07, -6.22561515e-07, -2.15544333e-06,
        -5.84451527e-08,  5.84679015e-07, -6.16205455e-07,
        -1.12243390e-06,  2.01092234e-07,  1.17554146e-06,
        -7.04501417e-08, -5.52463746e-08, -1.61475714e-06,
        -1.88835543e-07, -1.55581716e-08, -1.13293527e-06,
        -1.25063252e-06, -6.54193855e-08, -1.03608636e-06,
        -8.49006938e-07, -2.10125435e-07,  8.34062291e-07,
        -7.46796673e-07, -8.94092977e-08,  5.35057723e-07,
        -1.11778220e-07,  9.94858738e-07,  2.08975607e-07,
         1.16922138e-07, -2.48558791e-07,  3.00292612e-07],
       [-9.02971351e-07,  1.03029060e-06, -7.62956859e-07,
         6.82288146e-07,  7.30877559e-07, -4.34402949e-07,
        -2.49440427e-06,  1.32207367e-06,  2.12055352e-06,
         1.61001412e-06,  9.58307282e-07,  4.57488397e-07,
         1.50869869e-06, -2.89697653e-08,  2.27226678e-07,
        -1.93741198e-06, -7.18389515e-07, -2.56084013e-06,
        -1.12939074e-06,  8.25457619e-07,  2.41997486e-06,
         1.90348669e-06,  1.34664504e-06, -4.50522577e-07,
        -1.21220651e-07,  2.08971869e-06, -1.89793661e-06,
        -9.33901219e-07,  1.29369869e-06,  4.17609840e-07],
       [-1.04549792e-06,  3.07318231e-07, -1.19708318e-06,
         1.64370999e-06,  3.23679785e-07, -6.95603262e-07,
        -2.30862975e-06, -4.27832703e-08,  1.50828157e-06,
         8.63611945e-07,  7.30547470e-07,  1.03397542e-06,
         5.62609102e-07, -2.08145920e-07, -1.50081678e-06,
        -1.70868452e-06,  2.91066812e-07, -1.46597677e-06,
        -1.03855427e-06,  2.04076983e-07,  2.25753752e-06,
         4.91815058e-07,  1.07103381e-06, -2.72374081e-07,
         8.57039595e-08,  1.54159716e-06, -9.00806526e-07,
        -6.90488719e-07,  5.93931929e-07,  6.19191383e-07],
       [ 4.17636485e-08, -1.04454887e-06, -2.34397547e-07,
         6.52321091e-07, -1.70169955e-07,  2.31374031e-08,
         2.84677810e-08, -1.45403101e-06, -9.33455794e-07,
        -4.96379016e-07, -2.62009706e-07,  4.95592076e-07,
        -1.22103620e-06, -2.54596671e-07, -1.67849703e-06,
         4.83767565e-07,  9.89238970e-07,  2.18071705e-06,
         1.60662793e-07, -7.25786776e-07, -9.25471113e-08,
        -1.95472603e-06,  2.19081301e-07,  7.43995031e-07,
        -4.45031532e-08, -6.91266109e-07,  1.73375372e-06,
        -7.16471504e-08, -1.26243674e-06,  5.21886534e-07],
       [ 7.86610485e-07,  1.79938212e-07, -1.91855975e-07,
         6.36088942e-07, -5.08490473e-07,  1.83734755e-07,
        -8.93357708e-08, -2.53866233e-07, -4.99023884e-07,
        -1.05307095e-07,  3.99239156e-07, -1.91036889e-07,
         2.86497965e-07, -2.38574586e-07, -7.95309404e-07,
        -8.05412128e-07, -6.70877682e-08, -9.75169314e-07,
        -7.43438306e-08, -6.04819377e-08, -5.40440112e-07,
        -6.13240957e-07, -1.22728329e-06, -9.62038129e-08,
         1.32254314e-07,  3.07952462e-07, -4.29491820e-07,
        -3.56210847e-07,  1.80212282e-07, -2.28519625e-07],
       [ 6.41364863e-07,  1.46923071e-07, -1.40448299e-06,
         7.93399266e-08,  1.05203617e-06, -1.20961113e-06,
         1.50141801e-07,  1.00936336e-06,  1.13643569e-06,
        -1.29944362e-06, -1.60813656e-07, -1.62963559e-07,
         6.40861117e-07,  3.85179078e-07,  4.02665535e-07,
        -4.53919029e-07,  5.10422012e-07, -9.79595143e-07,
         1.50837849e-08, -9.99757575e-08,  2.68495427e-07,
        -5.33408070e-07,  4.15897091e-07, -2.38266011e-07,
         8.19301818e-08,  5.15670479e-07, -4.29029910e-07,
        -3.06785722e-07, -4.44601653e-08,  9.42426936e-08],
       [-1.28638121e-07,  5.39198624e-08, -8.80743528e-07,
        -3.43209678e-07,  5.02909188e-07, -4.75559858e-07,
        -1.35539449e-06, -7.03745968e-08,  7.64388687e-07,
         1.92106995e-07,  4.00030729e-08, -4.97926749e-08,
         1.22472940e-07,  1.25418694e-07, -2.71668227e-07,
        -3.86110059e-07, -1.29739249e-07, -3.21798495e-07,
        -3.83186830e-07,  2.30789794e-07,  1.19030574e-06,
         6.97448002e-08,  1.03308150e-06,  8.38035561e-08,
         1.47733047e-07,  1.06309108e-06, -2.68934940e-07,
        -1.38636779e-07, -3.76511537e-07,  6.46079911e-07],
       [-7.86489068e-07, -3.94779477e-07, -1.23449718e-06,
        -1.91690219e-07, -2.05263746e-07,  1.01769601e-07,
        -5.61848196e-07,  2.32880410e-07,  1.90374394e-07,
         1.72108457e-06,  1.44121657e-08, -2.10240387e-06,
        -1.58457638e-07,  2.35703823e-07, -5.58786382e-07,
        -1.59513002e-06, -3.37058509e-07, -1.06077778e-07,
        -9.29826399e-07,  3.08689550e-07,  7.30620741e-07,
         3.98409981e-07, -2.24726051e-07, -4.03811171e-08,
        -2.42575737e-09,  3.26043732e-07,  2.21006772e-07,
         1.12883970e-06,  2.22033080e-07, -3.31345333e-08],
       [ 8.02518628e-07, -2.96890192e-07, -1.59359035e-06,
         6.50162860e-07,  5.79490802e-07, -5.78454092e-07,
         1.13432259e-06,  2.50209098e-07,  1.25719168e-07,
        -1.84723638e-06,  2.40226882e-07, -1.51035579e-07,
        -6.10987314e-08, -4.44610855e-08, -4.87492230e-07,
         4.03129263e-07,  7.13320105e-07,  6.06909225e-08,
         4.67517395e-07, -4.04656049e-07, -1.26702855e-06,
        -9.85383167e-07, -7.52648589e-07,  3.36629995e-08,
        -9.12278324e-08, -3.65529388e-07, -1.52997330e-07,
        -2.36922347e-07,  2.93195484e-07, -3.28566927e-07],
       [-1.02561626e-06, -8.23707978e-07, -2.83903717e-07,
         2.43884926e-07,  3.86897483e-08,  4.50028153e-07,
        -1.10614292e-06, -3.97553492e-07,  2.11335987e-07,
         1.70147473e-06,  1.50021648e-07, -1.20973516e-06,
        -5.05284561e-07, -3.94953418e-07, -1.51697145e-06,
        -1.78199912e-06, -1.48729669e-07,  7.44730812e-07,
        -1.09368864e-06, -3.86669115e-07,  1.28964143e-06,
        -3.26965733e-07,  2.58285439e-07,  6.83132441e-07,
        -2.38963025e-07,  2.50359165e-07,  1.29653029e-06,
         7.64497202e-07, -3.38557754e-07,  3.49350302e-07],
       [-7.13658892e-08, -3.76659102e-07, -2.67401811e-06,
         6.27800432e-07,  1.41556552e-06, -1.30878311e-06,
         7.22351956e-09,  7.88538728e-07,  1.58483124e-06,
        -1.70823364e-06, -1.04844624e-07, -1.54287818e-07,
         5.27935242e-08,  3.11407888e-07, -4.86777822e-07,
        -1.84786558e-07,  7.99078975e-07, -5.46516162e-07,
         1.05702263e-07, -2.46574189e-07,  3.02749640e-07,
        -6.79457344e-07,  6.90903732e-07,  1.19792773e-07,
        -3.91711836e-08,  3.79949370e-07, -1.34798768e-07,
        -2.63047099e-07,  1.13171751e-07,  2.43352559e-07],
       [-6.28530586e-07, -2.14024567e-08, -2.24610233e-07,
         1.53413657e-06,  1.38856322e-07,  2.96964942e-09,
        -1.65247877e-06, -6.04983256e-07,  6.77553317e-07,
         4.25145174e-07,  5.42264161e-07,  1.39726910e-06,
         1.27029537e-07, -6.29539443e-07, -1.48597144e-06,
        -9.17548164e-07,  5.70923362e-07, -6.21864444e-07,
        -7.91494017e-07, -2.07820179e-07,  1.49460641e-06,
         2.39839409e-07,  6.01516888e-07,  1.49625663e-07,
        -3.21590335e-07,  7.68185544e-07, -4.71413841e-08,
        -8.36128720e-07,  2.20294538e-08,  6.60900128e-07],
       [ 3.91799574e-07,  1.02581248e-06,  1.54895451e-06,
        -4.26860254e-08, -1.04236938e-06,  3.99293242e-07,
        -5.05314063e-07, -4.75751051e-07, -4.20342928e-07,
         6.46835531e-07,  1.89291882e-07,  5.14981309e-07,
         7.03545425e-07, -1.91648425e-07,  4.47529487e-07,
        -1.86622998e-07, -8.95720746e-07, -1.77182665e-06,
        -6.55118981e-09,  4.95987479e-07, -6.03731110e-09,
         4.47203888e-07, -8.00137855e-07, -3.41396174e-07,
         3.57133615e-07,  6.58495878e-07, -1.20025561e-06,
        -6.56162911e-07,  5.14054250e-07, -1.23186709e-07],
       [-1.07547078e-06,  6.16151510e-07,  1.11078134e-06,
        -1.00671070e-06,  2.57457231e-07,  3.01105047e-07,
        -1.06921198e-06,  7.98988651e-07,  8.64617732e-07,
         2.24733230e-06,  1.70517097e-07, -5.23369010e-08,
         5.35789809e-07,  1.00021481e-07,  1.04056710e-06,
        -6.76111767e-07, -7.82176016e-07,  3.08534709e-09,
        -7.11079110e-07,  6.91793787e-07,  1.70112423e-06,
         2.19720300e-06,  1.49545735e-06, -2.99101885e-07,
        -2.67995460e-07,  5.39170571e-07, -4.28963290e-07,
         5.62390710e-07,  3.84568921e-07,  3.37023209e-07],
       [-4.04017328e-07, -7.41103349e-07, -2.56681415e-06,
         9.54959432e-07,  1.29379077e-06, -9.84662847e-07,
        -1.20804623e-06,  2.53926146e-07,  1.46579407e-06,
        -5.65369248e-07,  1.13361040e-07,  3.12598161e-08,
        -3.95301811e-07, -2.34900511e-07, -1.55454995e-06,
        -8.49064804e-07,  1.02835440e-06, -2.16787726e-08,
        -6.02896250e-07, -4.54841739e-07,  1.12399164e-06,
        -6.72197530e-07,  9.18551564e-07,  6.61865556e-07,
        -4.32805052e-07,  7.94601249e-07,  5.77885658e-07,
        -3.85895191e-07, -6.60049295e-07,  7.20780690e-07],
       [ 7.31459750e-07,  7.39473876e-07,  1.62236165e-06,
        -6.45342880e-07, -7.10096629e-07,  6.15556814e-08,
         1.53023109e-07,  3.86901888e-09, -7.00281248e-07,
        -1.84092329e-07,  2.92910940e-08,  8.87414558e-07,
         4.75470358e-07,  1.07706498e-07,  1.52034409e-06,
         9.91138450e-07, -3.85119677e-07, -7.63659614e-07,
         4.33717332e-07,  1.75458666e-07, -5.56455689e-07,
         4.34293725e-07, -3.55888972e-07, -4.48120602e-07,
         2.58352657e-07,  2.72380021e-08, -8.18286821e-07,
        -6.03128512e-07,  3.74774345e-09, -5.20777746e-07],
       [-4.69646778e-07, -8.97886594e-07, -1.43539387e-06,
         9.59123781e-07,  4.67301220e-07, -3.52671805e-07,
        -5.24836537e-07, -1.22161362e-06,  9.51679340e-08,
        -3.92033144e-07, -1.75480380e-07,  4.61657692e-07,
        -1.04619789e-06, -2.45450110e-07, -1.96471615e-06,
         1.40163110e-07,  1.17718821e-06,  1.41723069e-06,
        -1.66061810e-07, -4.99844475e-07,  5.11245673e-07,
        -1.27429325e-06,  7.61590400e-07,  6.86986368e-07,
        -6.45023164e-08, -1.33157982e-08,  1.10464293e-06,
        -4.72827466e-08, -9.03510738e-07,  8.09688913e-07],
       [-8.02599914e-07,  7.21108620e-07,  3.98636985e-07,
        -4.63536907e-07,  9.46303544e-07, -6.72115732e-07,
        -9.31352247e-07,  1.12684938e-06,  1.74327579e-06,
         1.75050758e-07,  2.48845907e-07,  5.64335096e-07,
         9.45468230e-07,  2.24176517e-07,  6.46954618e-07,
        -3.82033278e-07, -1.75107971e-07, -1.33728747e-06,
        -3.43603546e-07,  4.69688644e-07,  1.47834726e-06,
         1.45183003e-06,  1.41832015e-06, -3.72625010e-07,
        -2.45969034e-07,  9.37434493e-07, -9.77157015e-07,
        -4.05024934e-07,  6.35456388e-07,  3.45459455e-07],
       [-6.37069434e-07, -2.28204897e-07, -2.05456149e-06,
         4.70988340e-07,  1.12217754e-06, -7.41946565e-07,
         2.74127899e-07,  1.33004062e-06,  1.15025171e-06,
         6.97792757e-08,  1.01859712e-07, -6.57375551e-07,
         2.12489070e-07,  6.24474808e-07, -1.57357107e-07,
        -8.50406764e-07,  6.53671748e-07,  8.45035402e-07,
         4.81819171e-08,  2.75170066e-07,  6.12762506e-07,
         2.96490214e-07,  1.14480179e-06, -3.39852079e-07,
        -1.01618198e-08, -1.31721137e-07, -1.18355814e-08,
         6.89645503e-07,  6.19834964e-07, -6.24054621e-08],
       [ 8.05295144e-07, -1.05980530e-06, -2.56216276e-06,
         2.44520777e-07,  1.53318439e-07, -3.32665593e-07,
         1.41831606e-06, -4.68914550e-07, -5.37524670e-07,
        -2.27720739e-06, -7.06514811e-07, -1.80785241e-06,
        -8.80517064e-07,  4.38706650e-07, -7.74599471e-07,
         6.92138656e-07,  3.95050279e-07,  7.55209783e-07,
         7.12652252e-07, -4.93970106e-07, -1.78532491e-06,
        -2.53226426e-06, -1.21062999e-06,  5.68581072e-07,
         3.37325332e-07, -8.21620347e-07,  9.66100515e-07,
         4.06213047e-07, -3.48531756e-07, -2.12985299e-07],
       [-6.81686743e-07,  1.69389097e-07,  8.61137664e-07,
        -8.21518483e-07,  3.94216784e-07, -1.82820060e-07,
        -1.23911988e-07,  1.86082985e-07,  6.61967533e-07,
        -1.82575945e-08, -4.66513711e-07,  4.96539201e-07,
         6.72688643e-08,  1.92820039e-07,  1.01499586e-06,
         7.99844884e-07, -1.28113555e-07,  2.07550599e-07,
         9.99145584e-08,  1.33470280e-07,  7.35988465e-07,
         4.98571865e-07,  1.13503631e-06,  1.29266624e-08,
        -5.56310908e-09,  1.03164552e-08,  5.75913930e-08,
        -6.91635904e-08, -1.14207666e-07,  3.12036860e-07],
       [-7.49450066e-07,  1.98778324e-07, -1.64090511e-06,
         1.59327283e-06,  8.51287268e-07, -6.53926463e-07,
        -1.44090677e-06,  4.31520192e-07,  1.56024862e-06,
         2.46714933e-07,  6.65436573e-07,  1.19634296e-06,
         2.72898149e-07, -2.53183771e-07, -7.32890669e-07,
        -9.53780045e-07,  7.32561944e-07, -4.98594602e-07,
        -4.67791750e-07, -1.44711692e-08,  1.47558126e-06,
         5.12953534e-07,  1.24545670e-06,  5.28033510e-08,
        -2.39221436e-07,  9.05707395e-07, -6.33613581e-07,
        -7.66643495e-07,  4.50185524e-07,  2.64680637e-07]], dtype=float32)>, <tf.Tensor: shape=(60,), dtype=float32, numpy=
array([-2.7359673e-04,  1.2924130e-03, -4.9875327e-04,  2.5161766e-03,
        2.5829857e-05,  6.9071498e-04, -2.8913731e-03, -5.3905137e-04,
        1.0778354e-03,  1.6138881e-03,  2.1663285e-03,  1.9104370e-03,
        8.2717743e-04, -9.9573424e-04, -1.9216767e-03, -7.4219360e-04,
        1.0867211e-03, -3.9693975e-04, -1.2067553e-03,  5.5503839e-04,
        1.8374871e-03,  3.6423407e-03,  1.8075479e-03,  3.3938966e-04,
       -8.0773904e-04,  1.3716634e-03, -1.6372813e-03, -1.5850263e-03,
        1.4423602e-04,  1.6709835e-03, -6.0945010e-04, -1.8726415e-03,
        8.0843421e-04, -1.8565138e-03,  4.3614532e-04, -5.6250941e-04,
        2.3433275e-03, -1.5008751e-04, -9.5671276e-04, -5.0203881e-04,
       -4.5915606e-04, -5.2135816e-04, -1.5860107e-03, -1.5938141e-03,
        5.3010765e-04,  9.7149989e-04, -7.1330648e-04,  1.2900550e-03,
       -8.0413971e-04, -1.1226129e-03, -2.3379878e-03, -7.2413724e-04,
       -1.2445255e-03,  9.9976244e-04, -1.9667877e-03, -1.4202785e-03,
        2.6135019e-03,  6.5588334e-05, -2.8302553e-03, -3.9733859e-04],
      dtype=float32)>, <tf.Tensor: shape=(30, 45), dtype=float32, numpy=
array([[ 2.3505227e-06,  1.5696194e-06,  1.1616452e-07, ...,
        -3.5431324e-07,  1.1018675e-06,  1.5192846e-07],
       [-1.9632498e-06, -3.3314504e-07,  9.5591770e-07, ...,
        -9.0150735e-07,  2.7642309e-07, -1.0889354e-06],
       [-1.2042150e-06, -2.6277457e-06, -4.8964148e-07, ...,
        -1.1789244e-06, -1.3815621e-07,  3.2809439e-07],
       ...,
       [ 2.1299475e-06,  2.1338824e-06,  1.7860609e-06, ...,
         1.0248037e-06,  5.6441888e-07, -1.1827825e-06],
       [ 8.1844485e-07,  7.0805027e-07,  5.9680133e-07, ...,
         3.1796873e-07,  4.7439704e-07, -5.9551905e-07],
       [-2.7215717e-06, -1.3365681e-06, -3.4279282e-07, ...,
        -3.4111881e-07, -7.4719776e-07, -1.7610223e-07]], dtype=float32)>, <tf.Tensor: shape=(30, 45), dtype=float32, numpy=
array([[-2.9738450e-07, -3.5781250e-06, -2.3549069e-06, ...,
        -1.2112822e-06, -2.9095398e-07,  1.7738755e-06],
       [ 9.0921429e-07,  9.5114177e-07,  1.0873517e-07, ...,
         6.9409936e-08,  6.9871291e-08,  3.4252736e-07],
       [-8.3061548e-07,  3.2315668e-07,  2.3821366e-07, ...,
         3.5288727e-07, -5.4129885e-07, -2.7578511e-07],
       ...,
       [-1.2700333e-06, -1.5629340e-06,  6.9918508e-07, ...,
        -1.9608406e-06,  8.7053530e-07, -2.7656512e-07],
       [ 8.1157958e-07,  1.1424996e-07,  9.9529110e-08, ...,
        -2.7474152e-07,  3.5592271e-07, -2.9611635e-07],
       [-4.7391616e-08,  1.5943017e-06,  1.2896853e-06, ...,
         1.9370611e-07, -1.3151649e-08, -1.1979207e-06]], dtype=float32)>, <tf.Tensor: shape=(90,), dtype=float32, numpy=
array([-1.4882932e-04,  5.0391007e-04, -6.6305322e-05,  1.4467843e-04,
        5.2430271e-04,  3.3707613e-05,  1.4466938e-04,  9.9413679e-05,
       -3.5838794e-04, -2.3696852e-04, -1.6828436e-04, -2.9013731e-04,
        4.1641644e-04,  1.6061048e-04,  3.3081748e-04, -3.4942877e-04,
       -6.1081351e-05,  8.2781306e-05, -5.9302725e-05, -2.3154564e-06,
        7.3619856e-05,  4.5655979e-04, -1.4835550e-04,  8.4967629e-05,
       -1.0129603e-04,  4.3135613e-05, -9.7488875e-05, -5.3914433e-04,
        3.3059873e-04,  9.1708411e-05,  2.4618709e-04,  2.2791859e-04,
       -5.2104797e-04, -7.4377909e-05, -9.9947152e-05, -1.7836914e-04,
       -5.6985624e-05,  5.7654084e-05, -4.6119363e-05,  2.6155569e-04,
       -4.4442699e-04, -7.8339667e-07,  6.8187003e-04, -3.0675909e-04,
       -1.2444243e-04, -4.2140394e-04, -1.7227727e-03, -8.9554628e-04,
       -4.9142318e-04, -4.2830757e-04,  2.8789096e-04,  2.8876628e-04,
        1.3298693e-03,  8.7222748e-04,  1.3948507e-03, -1.5238180e-03,
       -5.9208664e-04, -1.6139616e-03,  2.7539104e-04,  5.9658336e-04,
        3.2835401e-04,  1.2362691e-03,  6.4240716e-04, -1.3447425e-04,
       -5.8231910e-04,  3.3436727e-04,  5.7577179e-04,  8.9027931e-04,
        1.9607428e-03,  8.3929359e-04, -7.9810229e-04,  8.1515062e-04,
        1.0025506e-03,  2.1472636e-04, -3.4018169e-04,  8.8515918e-04,
        5.7999115e-04,  7.8089797e-04,  2.1957229e-03,  8.5821637e-04,
        2.7113332e-05, -2.4999294e-04, -4.4136119e-04,  1.7349689e-03,
        1.1229212e-03,  1.2964056e-03,  3.1941006e-04, -4.3088090e-04,
       -2.4995676e-04,  8.2247471e-04], dtype=float32)>, <tf.Tensor: shape=(45, 24), dtype=float32, numpy=
array([[ 4.2726756e-06, -1.2528744e-06, -1.7747712e-06, ...,
         2.3486950e-06, -1.0736331e-08, -1.7422570e-06],
       [ 3.6074646e-06, -2.3479304e-06,  2.8575619e-06, ...,
         3.4042243e-06, -2.2620895e-06, -3.2854418e-06],
       [ 5.1153080e-07, -1.5454386e-06,  5.7132831e-07, ...,
         4.2268698e-06, -2.2427391e-06, -1.3378615e-06],
       ...,
       [ 5.3083824e-07,  4.1754080e-07,  1.3163930e-06, ...,
        -3.4348063e-06,  1.4847636e-06,  3.2987893e-07],
       [ 3.1766911e-06, -4.2232355e-06,  4.0044729e-06, ...,
         4.5006177e-06, -1.8570036e-06, -4.3233736e-06],
       [-3.3188842e-06,  3.7589696e-06, -3.7327377e-06, ...,
        -2.2892025e-06,  1.0980946e-06,  3.9294041e-06]], dtype=float32)>, <tf.Tensor: shape=(45, 24), dtype=float32, numpy=
array([[-5.77853598e-07, -4.78690708e-07,  1.07178926e-06, ...,
         2.49586236e-07,  1.26275563e-06,  2.47520433e-07],
       [ 4.18782565e-06, -1.09499592e-06, -1.92514926e-06, ...,
         1.20886546e-06,  6.12844190e-07, -2.95521318e-06],
       [ 5.14832493e-07,  7.74384546e-07, -2.35480798e-06, ...,
         6.48919183e-07, -4.34293213e-07, -1.10891555e-07],
       ...,
       [ 7.68919335e-07, -2.22209701e-06,  3.03443016e-06, ...,
         2.21670257e-06, -5.77869628e-07, -2.23287248e-06],
       [ 4.67606213e-07, -2.31753552e-06,  1.67964993e-06, ...,
         4.35662650e-06, -2.22646440e-06, -4.91269520e-06],
       [ 4.34147239e-07,  2.57395936e-06, -3.74037381e-06, ...,
        -4.45509886e-06,  2.08170104e-06,  4.70535224e-06]], dtype=float32)>, <tf.Tensor: shape=(48,), dtype=float32, numpy=
array([-8.5983262e-04,  3.8630649e-04, -3.8221860e-04, -5.9333298e-04,
       -7.1878848e-04, -1.5009783e-04,  1.3535118e-03,  3.7953403e-04,
       -1.8820171e-04, -1.8128559e-03, -6.8959402e-04, -2.5878532e-04,
       -2.4355239e-05,  7.3598250e-04,  1.1263821e-04, -4.2419089e-04,
       -1.7894811e-03,  3.3085566e-05,  3.7702895e-04, -6.5137882e-04,
       -2.4768556e-04, -1.4636086e-04, -1.9658246e-04,  1.1050487e-03,
       -1.1204989e-03,  7.4730697e-04, -5.2465720e-04, -4.2346900e-04,
       -1.2172592e-03,  4.5228917e-06,  1.2716807e-03, -2.2651019e-04,
       -2.8455368e-04, -1.6367340e-03, -1.2340803e-03, -7.2453194e-04,
       -4.1407725e-04,  9.7938161e-04,  3.6148005e-04, -9.0270024e-04,
       -1.8248594e-03, -3.3637662e-05,  9.1710861e-04,  2.5305411e-05,
       -3.0415531e-04, -8.6483627e-04,  6.2003039e-04,  9.3737454e-04],
      dtype=float32)>, <tf.Tensor: shape=(24, 12), dtype=float32, numpy=
array([[-1.35532455e-05,  3.56755118e-06, -6.11143878e-06,
         1.44717469e-05, -7.25876680e-06, -1.27333906e-05,
         6.12662461e-06, -5.45913736e-06,  2.99467001e-06,
         1.55384896e-05, -4.86085082e-06,  7.92251012e-06],
       [ 8.92235221e-06, -1.40714178e-06,  5.01271234e-06,
        -1.12784037e-05,  7.77525315e-07,  5.44384784e-06,
        -4.58209206e-06,  5.03030242e-06, -4.52070071e-06,
        -8.18239369e-06, -7.73633246e-07, -2.20804441e-06],
       [-1.17461905e-05,  4.02669593e-06, -2.22436574e-06,
         1.44389960e-05, -6.71176667e-06, -7.73130159e-06,
         7.74556247e-06, -7.20381831e-06, -1.37298559e-06,
         7.31972250e-06, -8.15771500e-07,  4.95324548e-06],
       [-1.00201651e-05,  4.61576747e-06, -5.61806360e-07,
         8.67500785e-06, -5.81540644e-06, -5.57526391e-06,
         4.97650854e-06, -4.16482681e-06, -1.97095005e-06,
         4.32439037e-06, -1.14726345e-06,  4.31610533e-06],
       [ 3.42234500e-07, -1.00120724e-06, -4.32108254e-06,
        -2.15022078e-06,  8.33294962e-07, -2.41655948e-06,
        -3.00811303e-06,  2.60161028e-06,  4.23293432e-06,
         6.20158835e-06, -4.03703370e-06,  1.44202170e-06],
       [-4.48130140e-06,  5.94524636e-07, -4.40484001e-07,
         7.49223773e-06, -4.98674626e-06, -4.50318475e-06,
         4.88380556e-06, -4.58594241e-06, -2.44197508e-06,
         4.52602626e-06, -2.30450539e-07,  3.40204292e-06],
       [ 2.18508230e-05, -7.01915633e-06,  7.28140412e-06,
        -2.52311129e-05,  1.10343217e-05,  1.63658788e-05,
        -1.24570752e-05,  1.05719610e-05, -2.70373698e-06,
        -1.86850666e-05,  3.18437515e-06, -1.03797811e-05],
       [ 8.35604078e-06, -5.64111269e-06, -5.06298420e-07,
        -6.19484490e-06,  5.70186285e-06,  3.78089703e-06,
        -3.92351285e-06,  2.09706673e-06,  3.38168888e-06,
        -1.38822770e-06,  1.29762361e-06, -2.98371992e-06],
       [-1.64497146e-06, -1.56573424e-06, -4.86706494e-06,
         5.30393891e-06, -2.72732746e-06, -5.37571486e-06,
         2.36664710e-06, -2.58273280e-06,  1.88133299e-06,
         9.41569124e-06, -2.25755571e-06,  2.95548944e-06],
       [-4.68903727e-06,  5.09926963e-07, -3.74268643e-06,
         5.62039577e-06, -2.52624795e-06, -7.53384302e-06,
         1.66500649e-06, -1.28746569e-06,  3.94495646e-06,
         9.77305172e-06, -3.88976923e-06,  4.59943794e-06],
       [-4.79705568e-06,  4.98536701e-06,  2.18690417e-07,
        -1.13598423e-06, -2.15930095e-06,  1.77214395e-06,
        -8.92202536e-07,  1.95783286e-06, -3.46424213e-06,
        -3.25814131e-06, -6.34431387e-07, -1.08717404e-06],
       [-7.20809453e-07, -2.99117846e-06, -2.55650593e-06,
         7.52806181e-06,  1.33363329e-07, -4.46105923e-06,
         4.28584008e-06, -4.83630856e-06,  3.84017221e-06,
         6.61503373e-06,  7.03680598e-07,  2.30245314e-06],
       [-2.39846963e-06, -1.01768205e-06, -1.29671321e-06,
         7.42870225e-06,  4.31434231e-08, -2.88906153e-06,
         4.35093853e-06, -4.22265066e-06,  2.24054884e-06,
         3.86845522e-06,  1.97998065e-06,  8.45229124e-07],
       [-8.43624093e-06,  4.30614500e-06, -3.00048868e-07,
         6.96152438e-06, -5.39570419e-06, -4.42262490e-06,
         3.95685402e-06, -2.76765513e-06, -2.05254446e-06,
         3.03221759e-06, -1.14629836e-06,  3.51387462e-06],
       [ 7.38868539e-06, -4.13252292e-06,  4.12308145e-06,
        -5.06424522e-06,  3.07301946e-07,  2.92628533e-06,
        -7.24593065e-07, -2.89429664e-07, -3.90819832e-06,
        -4.67471546e-06,  1.70289275e-06, -7.82419875e-07],
       [ 1.85837143e-05, -5.91342996e-06,  6.11220594e-06,
        -2.48200467e-05,  1.01650749e-05,  1.45043796e-05,
        -1.28564961e-05,  1.08153581e-05, -1.64567427e-06,
        -1.60789987e-05,  1.90307810e-06, -8.75692604e-06],
       [-2.75509024e-06,  3.69627810e-06,  3.30126409e-06,
        -4.02704842e-07, -4.04854700e-06,  3.13172109e-07,
         1.32456239e-06, -4.24228617e-08, -5.09944221e-06,
        -3.65453661e-06, -3.93548930e-07,  1.08588154e-06],
       [-6.87333022e-06,  1.57214788e-06, -6.70375539e-06,
         7.33306570e-06,  8.33605441e-07, -5.87658269e-06,
         1.27673593e-06, -1.46236448e-06,  7.33243951e-06,
         9.62339982e-06, -2.26812745e-06,  2.03330023e-06],
       [-1.38065695e-06, -2.22276662e-06, -1.60471430e-07,
         8.11516111e-06, -2.40421798e-07, -4.33582181e-06,
         5.17154695e-06, -5.13946134e-06,  2.35819289e-06,
         4.35050833e-06,  1.88491936e-06,  2.60790512e-06],
       [ 1.77602442e-06, -1.73847309e-06, -3.00901638e-06,
        -2.63571906e-06,  1.08571533e-06, -2.21741334e-06,
        -2.70811393e-06,  2.31463468e-06,  3.70915950e-06,
         5.05307935e-06, -3.10585574e-06,  1.61730827e-06],
       [-1.78086011e-05,  7.26927055e-06, -5.29576300e-06,
         1.94964960e-05, -8.56658880e-06, -1.26458408e-05,
         9.13136228e-06, -7.30669626e-06,  1.46998150e-06,
         1.33409312e-05, -3.27596399e-06,  7.38712515e-06],
       [-1.02635122e-05,  4.95297490e-06, -1.98955468e-06,
         1.05368945e-05, -4.53730581e-06, -5.87318436e-06,
         5.36633797e-06, -3.68727956e-06,  4.67608771e-07,
         5.10369864e-06, -1.12219448e-06,  3.38066184e-06],
       [ 5.81463883e-06, -2.59027934e-06,  1.33489470e-06,
        -4.23342453e-06,  1.31651018e-06,  2.46696641e-06,
        -1.23125960e-06,  8.30804026e-07, -1.19857009e-06,
        -2.18242644e-06,  8.18116291e-07, -1.20694676e-06],
       [ 1.11001154e-05, -4.22585254e-06,  3.22831420e-06,
        -1.01276100e-05,  6.52949666e-06,  8.16459033e-06,
        -3.97053282e-06,  3.33663297e-06, -1.42950142e-07,
        -9.06976129e-06,  3.56129863e-06, -5.39474968e-06]], dtype=float32)>, <tf.Tensor: shape=(24, 12), dtype=float32, numpy=
array([[-9.79750257e-06,  1.14314662e-06, -7.97564826e-06,
         1.15137618e-05, -5.18644993e-06, -1.26887790e-05,
         3.65181404e-06, -3.45577928e-06,  6.09146218e-06,
         1.78282226e-05, -5.73554416e-06,  7.31088039e-06],
       [ 6.68791063e-06, -5.46666615e-06, -1.66271866e-06,
        -3.18728507e-06,  4.42446390e-06,  1.82232714e-06,
        -2.42018473e-06,  7.03726130e-07,  3.59222827e-06,
         1.33472065e-06,  9.53086101e-07, -2.22358585e-06],
       [-8.05828950e-06,  3.32786499e-06, -1.66741120e-06,
         6.90286697e-06, -5.04253831e-06, -6.52764538e-06,
         3.38739937e-06, -2.57186093e-06, -5.91829973e-07,
         6.59808165e-06, -2.50391759e-06,  4.61694799e-06],
       [-7.89090154e-06,  2.33064270e-06, -1.15879573e-06,
         1.10627434e-05, -5.76094953e-06, -5.89524643e-06,
         6.66255528e-06, -6.12694839e-06, -2.26557199e-06,
         5.53909604e-06, -4.90897349e-08,  4.18206491e-06],
       [-4.99370742e-07, -1.05675461e-07, -3.70777866e-06,
        -2.58649402e-06, -5.92952119e-07, -2.32645834e-06,
        -2.98841883e-06,  2.18419927e-06,  1.76966114e-06,
         5.13535042e-06, -3.84996019e-06,  1.79443930e-06],
       [-1.22396159e-05,  3.79634116e-06, -7.77156401e-06,
         1.12890684e-05, -4.85551573e-06, -9.23843345e-06,
         3.47562764e-06, -3.43456122e-06,  3.63069944e-06,
         1.38173555e-05, -4.03209924e-06,  5.22532810e-06],
       [ 1.68280094e-05, -1.62538106e-06,  7.55365272e-06,
        -2.42277274e-05,  9.24563847e-06,  1.62585893e-05,
        -1.21413923e-05,  1.20342220e-05, -2.89033096e-06,
        -2.03375766e-05,  1.87220292e-06, -1.01579881e-05],
       [ 4.81280449e-06,  1.46615932e-07,  1.09199789e-06,
        -8.89433886e-06,  1.20773120e-06,  4.03372815e-06,
        -4.38480583e-06,  4.74570425e-06, -1.73489821e-06,
        -4.74825038e-06, -1.45989918e-06, -1.58133400e-06],
       [-4.02325441e-06,  5.14774285e-07, -6.50415404e-06,
         1.69151122e-06, -2.38646726e-06, -5.10543396e-06,
        -1.47797459e-06,  1.53706515e-06,  3.52705774e-06,
         1.03460306e-05, -4.82356563e-06,  3.17243894e-06],
       [-1.74231059e-06,  3.73430140e-07,  1.72071935e-07,
         3.20040385e-06, -3.70909765e-06, -2.67350401e-06,
         2.63874199e-06, -1.99223791e-06, -2.15527780e-06,
         2.65462040e-06, -1.07131189e-06,  2.50816584e-06],
       [ 1.30826436e-06,  5.05697471e-07, -1.01688579e-06,
        -4.78466927e-06, -8.75506146e-07, -7.77132868e-07,
        -3.19712012e-06,  2.53701069e-06,  1.50670516e-08,
         1.15678768e-06, -4.15415934e-06,  1.51605923e-06],
       [-3.42659769e-06, -3.42402586e-07, -5.73899979e-06,
         2.71092085e-06, -2.47283970e-06, -5.90673244e-06,
        -7.11618043e-07,  3.19091527e-07,  3.38401446e-06,
         1.07712194e-05, -4.49403524e-06,  3.54811459e-06],
       [-3.32010450e-06, -1.15074090e-06, -2.47689513e-07,
         7.29270869e-06, -3.75046466e-06, -3.23174572e-06,
         4.43163799e-06, -4.91860874e-06, -2.11376755e-06,
         3.91501499e-06,  7.00240264e-07,  2.25952294e-06],
       [-4.23073431e-07, -6.19126183e-07,  3.74629531e-06,
         1.06299819e-06, -2.85315673e-06,  5.31433557e-07,
         2.25159192e-06, -2.87152216e-06, -5.65699929e-06,
        -3.04149057e-06,  2.31091508e-06,  8.70083738e-07],
       [ 6.45887712e-06, -5.68774112e-06, -2.14052966e-06,
        -1.69150576e-06,  4.60286628e-06,  1.12943076e-06,
        -1.59348986e-06,  2.97721556e-07,  4.43559929e-06,
         2.39728024e-06,  8.63574314e-07, -1.72382204e-06],
       [ 1.62659126e-05, -2.75662660e-06,  5.60599437e-06,
        -2.45207757e-05,  7.17583498e-06,  1.31573743e-05,
        -1.24618300e-05,  1.18016560e-05, -3.39887606e-06,
        -1.54294194e-05, -4.18831235e-07, -7.50462186e-06],
       [-3.88537637e-06,  1.64735764e-06, -3.40692418e-06,
         1.61124763e-06, -2.27402961e-06, -4.40265921e-06,
        -4.48804400e-07,  3.58353105e-07,  1.59636090e-06,
         6.54046380e-06, -3.62565220e-06,  3.31082629e-06],
       [-8.10443282e-07, -1.80789073e-06, -1.74806280e-06,
         3.15316788e-06,  1.06622554e-06, -5.16687260e-06,
         9.17625300e-07, -1.11327608e-06,  5.17520084e-06,
         6.24513086e-06, -1.46564707e-06,  2.74633203e-06],
       [-4.65317135e-06,  1.55996383e-06, -3.26511031e-06,
         6.38054007e-06,  6.92439414e-07, -2.55908299e-06,
         2.38459211e-06, -1.91752360e-06,  4.01936222e-06,
         3.86446072e-06,  2.18093447e-07,  6.46342869e-07],
       [-7.30763804e-06,  2.90581170e-06, -3.73556895e-06,
         8.37860898e-06, -1.54228758e-06, -3.31077354e-06,
         3.37457527e-06, -2.55153350e-06,  3.32891727e-06,
         4.84746897e-06,  3.28155409e-07,  1.45530885e-06],
       [-9.15346027e-06, -6.74798457e-07, -4.19486969e-06,
         1.73201297e-05, -3.05353660e-06, -1.10431220e-05,
         9.12813175e-06, -9.23230800e-06,  5.00560782e-06,
         1.28422598e-05,  5.32102149e-07,  6.43054364e-06],
       [-1.09389657e-05,  5.11549024e-06, -2.13513931e-06,
         9.24555297e-06, -5.16257933e-06, -7.35815047e-06,
         4.27813529e-06, -3.22205437e-06,  1.02697086e-07,
         6.75154070e-06, -2.29216926e-06,  4.77675621e-06],
       [ 5.78906020e-06, -2.12434838e-06,  2.24802125e-06,
        -3.96986843e-06,  4.75722027e-06,  5.95657684e-06,
        -1.13571855e-06,  6.29641590e-07,  4.13255208e-09,
        -7.02372199e-06,  3.95931602e-06, -4.35626816e-06],
       [ 8.28576594e-06, -1.95898383e-06,  5.40886901e-07,
        -1.19272772e-05,  5.38541917e-06,  6.22948073e-06,
        -7.26913640e-06,  6.61561035e-06,  1.77314234e-06,
        -5.62854348e-06, -1.41168357e-06, -4.26385850e-06]], dtype=float32)>, <tf.Tensor: shape=(24,), dtype=float32, numpy=
array([-0.00142598,  0.00015655, -0.0009249 ,  0.00204275, -0.00055956,
       -0.00145019,  0.00087759, -0.00084406,  0.00071621,  0.00199959,
       -0.00030858,  0.00081018,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ], dtype=float32)>, <tf.Tensor: shape=(30, 30), dtype=float32, numpy=
array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],
      dtype=float32)>, <tf.Tensor: shape=(30, 30), dtype=float32, numpy=
array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],
      dtype=float32)>, <tf.Tensor: shape=(60,), dtype=float32, numpy=
array([-1.2231125e-07, -2.6939929e-06,  9.0424743e-07, -1.8168786e-06,
       -8.0810992e-07,  6.7156071e-07, -5.6243846e-07,  2.1330261e-06,
       -2.6986224e-06, -1.0225049e-06,  1.4130941e-07,  3.2601625e-07,
        2.2017628e-06,  1.0561886e-06, -3.0043670e-06, -1.3070785e-06,
       -3.2099112e-07, -7.3923991e-07, -1.5605291e-06, -1.3767226e-06,
       -3.0376220e-06, -1.3912827e-06,  1.2146370e-07, -7.8429269e-07,
        1.4733538e-06, -1.1099355e-06,  2.3422683e-06,  2.0414214e-07,
        1.7497193e-06,  2.6659998e-06,  3.0237898e-06, -9.0446326e-07,
       -5.9339698e-09, -1.2777359e-07,  1.0992236e-06,  2.8536638e-06,
        9.7733027e-07, -2.1502206e-06,  2.4241344e-06,  8.3226905e-07,
        1.2776251e-06, -9.1079391e-07,  4.7933315e-07,  8.9565503e-07,
        1.0025192e-06,  1.3649457e-06,  6.2998822e-07,  8.9811050e-07,
       -4.3760599e-07,  1.0325963e-06, -3.0446245e-07,  1.0048595e-06,
        8.0634896e-07,  6.2394278e-08,  4.7518319e-07, -7.9529144e-08,
       -3.0038558e-07, -1.3062415e-06, -1.0179874e-06, -2.9373331e-07],
      dtype=float32)>, <tf.Tensor: shape=(30, 30), dtype=float32, numpy=
array([[ 2.65343988e-06,  1.30448825e-08, -1.18744856e-06,
        -2.72397415e-07, -5.97859980e-07,  7.35545655e-07,
         7.85449799e-07, -2.03407899e-06,  2.58028490e-06,
         2.64881351e-07,  3.14020497e-07, -2.44367328e-07,
         2.03961918e-07,  1.23585608e-06,  1.04884703e-06,
         1.51991890e-06,  7.96785287e-07,  7.56736483e-07,
         7.00608553e-07,  2.13443855e-07,  9.46036437e-07,
         2.24430948e-07,  2.88100637e-07, -4.21288178e-07,
        -3.27913511e-07,  2.07183461e-06, -2.00505397e-07,
        -4.71243766e-07, -1.31486843e-06,  2.94737333e-07],
       [ 4.88000751e-10,  9.97960683e-07,  1.23335985e-06,
         3.92235705e-07,  8.81496817e-07, -7.47774493e-07,
         1.17849572e-06, -8.33813260e-07,  4.76514970e-07,
         3.54569295e-07, -1.19981451e-06,  1.12240241e-06,
        -1.23244195e-06,  1.01810031e-07,  2.05302536e-06,
        -7.93307436e-07, -8.22199809e-07,  5.22454911e-07,
         3.18098841e-07,  3.26699478e-07,  9.91825573e-07,
         1.61359617e-06,  5.12434269e-07,  8.76519039e-07,
        -5.01422221e-07,  2.01673015e-06, -1.26828854e-06,
         1.93661521e-07,  1.69150113e-07, -2.77100435e-06],
       [-1.81123141e-07,  9.72486873e-07,  1.01442549e-06,
        -3.94374410e-07,  7.57867610e-07,  4.40695658e-09,
         6.54833968e-07, -1.06118227e-06,  1.24885048e-06,
        -6.03115097e-07, -1.53059477e-06,  7.91256241e-07,
        -9.59195063e-07, -5.39529537e-07,  1.07201674e-06,
        -2.32891352e-07, -9.25533413e-07,  6.39086011e-08,
         3.48544575e-07,  1.48363398e-07,  8.20525997e-07,
         1.97961572e-06,  3.57136742e-07,  1.76680328e-06,
        -8.58173735e-07,  1.79804658e-06, -7.90830654e-07,
        -1.12553053e-07,  3.77044273e-07, -1.81067276e-06],
       [-1.49031507e-07, -6.06981700e-07,  2.05448464e-06,
         2.35591799e-07,  1.83066891e-06,  1.70911608e-06,
         2.94036340e-07, -2.07476387e-06,  1.64168512e-06,
        -6.90056581e-08,  8.55723670e-07, -9.57353521e-08,
        -1.08732934e-08,  1.57804152e-06, -5.82643793e-07,
         9.47955755e-07,  1.05124673e-06,  1.22550853e-06,
         9.95949449e-07,  2.17394427e-06,  5.26824465e-07,
         5.21020070e-07,  8.51085247e-09,  2.45675608e-07,
        -4.54240080e-07, -1.96939709e-08, -8.77174443e-07,
         1.24000593e-07, -1.73940532e-06,  1.29784780e-06],
       [-8.06226524e-07, -1.44893920e-06,  2.22898780e-06,
         5.40265660e-07,  2.24574978e-06,  2.04010257e-06,
        -2.59905050e-07, -1.26593909e-06,  1.12687928e-06,
         2.52124494e-07,  1.73203387e-06, -4.46430164e-07,
         4.85917610e-07,  1.52286566e-06, -1.21891242e-06,
         1.19974004e-06,  8.86416217e-07,  1.52040968e-06,
         7.79505001e-07,  2.34864865e-06, -3.59598403e-07,
         2.81161647e-07, -1.33841098e-07, -4.39186124e-07,
        -8.94964103e-08, -1.36100812e-06, -5.18645777e-07,
         3.29053535e-07, -2.09506129e-06,  2.30144065e-06],
       [-5.00320311e-07,  5.19709829e-07, -2.22718154e-06,
        -9.73576334e-07, -2.11494989e-06, -2.07349694e-06,
        -1.07798598e-07,  7.40028611e-07, -1.13932401e-06,
        -4.25737255e-07, -1.11475219e-06,  7.46123703e-07,
        -3.06934766e-07, -1.44436615e-06,  1.11603435e-06,
         1.19773631e-08, -7.00507826e-07, -6.16276964e-07,
        -9.28845850e-07, -1.56869987e-06,  8.53561133e-08,
        -1.10089979e-06,  1.83513350e-07, -4.06969832e-07,
         5.98927500e-07,  1.81732844e-07,  3.24729086e-07,
        -1.42079188e-07,  1.20581990e-06, -1.53644282e-06],
       [-7.87040790e-07,  1.95720034e-07, -2.41260523e-09,
        -1.86639411e-07, -3.58537648e-07, -4.23952798e-07,
        -2.45897070e-07,  2.14703437e-06, -1.48968900e-06,
        -8.69447376e-08, -3.83968370e-07,  6.34658335e-08,
         6.59367345e-07, -1.10602264e-06, -1.47532944e-07,
        -8.58672308e-07, -2.04908520e-06, -5.33481568e-07,
        -3.97649501e-07, -1.59008107e-06, -1.37101551e-06,
         1.02348906e-06, -1.94728685e-07,  5.23184781e-07,
         1.50340441e-07, -4.72694012e-07,  1.11027043e-06,
         2.77863478e-07,  1.88314607e-06, -3.45646527e-07],
       [ 8.48373986e-07, -1.60826687e-06, -2.83727445e-07,
         2.30043156e-07,  2.00725083e-07,  1.74502088e-06,
        -3.44179568e-07, -5.81469010e-07,  6.67864299e-07,
         6.28525868e-07,  2.15475734e-06, -1.32745015e-06,
         1.33011736e-06,  1.51813128e-06, -1.09514963e-06,
         1.31442448e-06,  1.23017605e-06,  9.87868134e-07,
         2.52779188e-07,  6.89920398e-07, -8.70654276e-07,
        -9.89516366e-07, -2.32002776e-07, -1.55431621e-06,
         4.97914812e-07, -1.40180214e-06,  6.09029883e-07,
        -6.05606658e-08, -1.55567250e-06,  2.69958582e-06],
       [ 1.41341388e-07, -8.25001962e-07,  2.15175578e-06,
         5.01983891e-07,  2.07308926e-06,  2.06068921e-06,
         4.28512749e-07, -3.03790171e-06,  1.94265726e-06,
         3.70519160e-07,  1.65248605e-06, -8.37291054e-07,
         6.65184814e-08,  1.93500910e-06, -7.22144705e-07,
         1.31902107e-06,  2.11004294e-06,  1.68691247e-06,
         9.37394532e-07,  2.85752867e-06,  1.24092435e-06,
        -6.84187853e-08, -1.57014995e-07, -5.72971430e-08,
        -5.63180038e-07,  5.81547340e-07, -1.06158234e-06,
         2.59658918e-07, -3.08581139e-06,  1.28889280e-06],
       [ 1.69928342e-07,  9.91272486e-08, -5.08994617e-07,
        -3.40169635e-07,  3.62715468e-07,  1.03382581e-06,
         3.53760527e-07, -2.85112878e-06,  2.45298497e-06,
         1.44455711e-07,  2.07587348e-07, -3.87874394e-07,
        -8.28779605e-08,  4.69192940e-07,  1.57040881e-06,
         2.08943493e-06,  9.31040461e-07,  1.12137900e-06,
         7.58815361e-07,  1.51549125e-06,  1.18942103e-06,
         4.28743192e-08,  1.15700118e-07, -2.16717154e-07,
        -3.24786669e-07,  4.97645033e-07, -1.29292960e-06,
        -1.22208448e-08, -2.27697046e-06, -4.62067959e-07],
       [-7.74086402e-07,  5.12528345e-07,  6.25539201e-07,
         1.61175706e-07,  1.03032448e-06, -5.24696816e-07,
         3.69898146e-07, -1.79405583e-06,  1.14778891e-06,
        -2.75681998e-07, -2.11076966e-07,  9.68049449e-07,
        -9.60446641e-07,  1.77309460e-07,  6.08844061e-07,
         4.00534702e-07,  9.13353006e-07,  3.45088665e-07,
         3.92899381e-07,  1.43810894e-06,  1.93800474e-06,
        -1.08202897e-07,  2.44786548e-07,  3.88516668e-07,
        -4.83657118e-07,  1.18712717e-06, -1.25117685e-06,
        -2.42173769e-07, -1.10012661e-06, -8.18133117e-07],
       [ 8.27255178e-07, -1.86132155e-07,  4.42899932e-07,
         6.48610865e-07,  7.49059723e-07,  6.81296797e-07,
        -2.06316287e-07, -8.18128399e-07,  6.43709882e-07,
         8.24843767e-08,  5.28062969e-07, -2.97416989e-07,
        -1.06645018e-07,  9.84286771e-07, -9.12532187e-07,
        -7.92374522e-08,  1.43361785e-06, -3.59875969e-08,
         5.70643067e-07,  9.44882231e-07,  5.21806783e-07,
        -3.94530161e-07, -7.85189442e-08, -2.18532307e-07,
        -3.31626609e-07, -1.54776757e-07, -2.84217919e-07,
        -1.36881170e-07, -9.95228334e-07,  9.29044063e-07],
       [-6.63517881e-07, -1.91753043e-06,  2.77864956e-06,
         1.21885364e-07,  1.66925020e-06,  1.18229514e-06,
         1.34212428e-08,  3.96005049e-07, -1.55329781e-06,
        -3.46071715e-07,  1.39067072e-06, -8.69775363e-07,
         8.93753338e-07,  1.48779691e-06, -3.42708336e-06,
        -5.82729513e-07,  8.64320441e-07, -1.11956837e-07,
        -5.06999186e-07,  6.86390763e-07, -9.61888190e-07,
        -2.42997032e-07,  9.85773170e-08, -1.18906598e-07,
         3.79167489e-07, -1.48317656e-06,  1.18455227e-06,
         1.84406275e-07, -5.02309490e-07,  3.23247241e-06],
       [-6.06987783e-07, -3.92027573e-07,  2.43046134e-06,
        -1.24339991e-07,  1.09678695e-06, -9.59438012e-07,
         1.28884767e-06,  3.25418512e-07, -1.28221416e-06,
        -1.10877920e-08, -6.97968858e-07,  9.46137277e-07,
        -5.93455809e-07,  4.99607893e-07,  1.62290888e-07,
        -1.25048996e-06, -1.05021923e-06,  2.93775400e-07,
        -1.15840692e-06, -2.08189036e-07, -1.06558666e-07,
         1.17289733e-06,  8.03985927e-07,  6.58709951e-07,
         1.61929734e-07,  1.68770021e-06,  2.33709827e-07,
         2.32595966e-07,  6.32250874e-07, -7.79836796e-07],
       [-9.00848676e-08,  8.06207652e-07, -1.25714905e-06,
        -7.27614236e-07, -7.59521754e-07, -1.46357991e-06,
         3.95312156e-08,  4.39731195e-07,  8.18764079e-08,
        -7.01422209e-07, -1.23194854e-06,  9.34883417e-07,
        -3.51014734e-07, -1.22690426e-06,  1.26177952e-06,
        -2.37214408e-08, -1.23844620e-06, -5.23489518e-07,
        -3.68793081e-07, -1.14795523e-06,  4.41792167e-07,
         4.32970012e-07,  2.90469728e-07,  4.48981439e-07,
         1.06035770e-07,  1.07319067e-06,  6.60161206e-08,
        -3.40121517e-07,  1.07113681e-06, -1.36652932e-06],
       [ 3.16874207e-08, -1.87668411e-06,  4.20738360e-07,
        -1.21015898e-06, -9.06526566e-07, -9.76190222e-08,
         3.28551749e-07,  2.39559586e-06, -2.03710511e-06,
        -9.79192691e-08,  1.76102958e-07, -7.23226435e-07,
         1.21551170e-06,  2.92753498e-07, -1.07941958e-06,
         2.41641033e-07, -1.55951795e-06, -3.33934679e-07,
        -1.44310297e-06, -1.28519525e-06, -2.40605004e-06,
        -2.28146433e-07,  2.59960018e-07, -8.07473498e-07,
         1.36023675e-06, -1.42536976e-06,  1.85713475e-06,
         1.18398987e-07,  1.29031810e-06,  2.31659260e-06],
       [ 3.33779553e-07, -9.70436531e-07,  1.60604645e-06,
         2.76464561e-07,  8.38009100e-07,  1.41337023e-06,
         1.43099953e-07,  1.17106470e-06,  3.38169173e-08,
         4.73144013e-07,  5.43165470e-07, -5.90404227e-07,
         6.01750799e-07,  2.90601946e-07, -7.51939922e-07,
        -1.76687934e-07, -1.15448393e-06,  1.86439877e-07,
         1.82576059e-07, -4.12932309e-07, -1.48110507e-06,
         1.74760885e-06,  4.22171240e-08,  4.15934807e-07,
        -6.53659242e-08, -5.71192686e-07,  8.70627332e-07,
         3.71438205e-07,  2.68834668e-07,  9.00487748e-07],
       [ 6.91838238e-07,  4.92495474e-07, -1.00785996e-06,
         5.04703280e-07, -1.20532127e-06, -1.49535572e-06,
        -2.41769669e-07,  2.04141293e-06, -1.44577325e-06,
         1.21360358e-07, -4.75029481e-07,  4.15928582e-07,
        -2.73103751e-07, -7.45441753e-07, -4.16663894e-08,
        -1.64168432e-06, -5.23197627e-07, -1.27777980e-06,
        -7.44084275e-07, -1.81701625e-06, -3.51145559e-07,
        -5.31904561e-07,  4.18862101e-08, -2.74233571e-07,
         1.62114560e-07,  5.14903661e-07,  7.48261073e-07,
        -3.33994564e-07,  1.77617676e-06, -7.51374046e-07],
       [-9.57612997e-07, -1.17023933e-06, -4.24467430e-08,
        -3.73878720e-07, -6.94250446e-07, -3.30144815e-07,
        -7.94891889e-07,  2.76603441e-06, -2.94437382e-06,
        -1.67564281e-07,  5.28408918e-07, -5.89699425e-07,
         1.12519001e-06, -4.06475209e-07, -2.03465197e-06,
        -8.82539041e-07, -6.78332867e-07, -9.05005891e-07,
        -1.16541264e-06, -1.33655737e-06, -2.10126973e-06,
        -9.39889844e-07, -3.09177324e-07, -6.62383513e-07,
         1.05356605e-06, -2.48564857e-06,  1.74103013e-06,
         2.39044255e-07,  1.46949560e-06,  1.97302415e-06],
       [ 1.91710888e-06, -4.85434839e-07, -1.65122458e-08,
        -9.36377432e-07, -9.73764031e-07, -5.61832849e-07,
         1.59160754e-06,  2.40312147e-07,  8.57897930e-08,
        -4.45014123e-08, -7.79371931e-07, -1.02794409e-07,
         6.86425494e-08,  4.66868386e-07,  7.31928935e-07,
         3.89152660e-07, -9.90949388e-07,  1.00778607e-07,
        -1.11530449e-06, -1.04356127e-06, -2.77520741e-07,
         7.88307261e-07,  9.65995127e-07, -1.87417754e-07,
         3.47025292e-07,  2.47620801e-06,  8.82512609e-07,
        -3.05215622e-07,  5.13958128e-07, -8.40774987e-08],
       [ 2.66298912e-07, -1.78232952e-07, -1.08318841e-06,
         9.47954959e-09, -5.22458549e-07,  1.68317683e-07,
        -5.11874418e-08, -1.53493730e-07,  6.92740741e-07,
         5.28386465e-07,  4.78373636e-07, -9.64703233e-08,
         4.83664394e-07, -5.56540698e-08,  1.09836810e-06,
         7.64142044e-07, -3.72019997e-07,  7.36514096e-07,
         2.32020881e-07, -3.52857057e-07, -2.74668025e-07,
         1.11797988e-07,  4.22245812e-08, -5.30112629e-07,
         1.50156239e-07,  4.43385773e-08,  4.35653860e-08,
         1.46055177e-08, -2.38665137e-07, -3.60912679e-07],
       [-5.33449281e-07, -5.84096654e-07,  1.28862553e-07,
         9.36567062e-07,  3.42123514e-07,  6.41339170e-07,
        -7.72091596e-07,  1.44932073e-07, -4.96143514e-07,
         6.46384365e-07,  1.44325281e-06, -8.43327086e-07,
         1.71154198e-07,  5.89868876e-07, -1.21125629e-06,
        -1.22639676e-07,  1.55958492e-06, -6.38330562e-08,
         2.59423643e-07,  9.09570872e-07, -2.86635782e-08,
        -1.67751568e-06, -5.36786388e-07, -9.25524319e-07,
         1.55481587e-07, -1.95856683e-06, -2.67850226e-07,
         1.01625048e-07, -9.91343768e-07,  1.33094795e-06],
       [ 3.07320818e-07,  3.13347925e-07, -1.00116949e-06,
        -4.28346823e-07, -1.30426588e-06, -3.75101422e-07,
        -5.95516667e-08,  1.57042632e-06, -8.29757141e-07,
         9.54391624e-08, -7.17644639e-07, -1.06496351e-07,
         3.57226895e-07, -7.83411622e-07,  4.18236141e-07,
        -4.10922553e-07, -1.31211959e-06, -5.99541977e-07,
        -3.20064117e-07, -1.63984168e-06, -1.21408266e-06,
         3.95852481e-07, -6.52856613e-08,  2.17620311e-07,
         2.19338517e-07, -4.57162855e-07,  8.28140799e-07,
         1.08982661e-07,  1.68309271e-06, -4.62064293e-07],
       [-5.46586023e-07, -3.51845415e-08, -2.50341509e-06,
        -1.21848961e-06, -2.57448846e-06, -1.51519316e-06,
        -3.73100079e-07,  3.08757808e-06, -2.02568799e-06,
        -4.50720790e-07, -1.27998737e-06,  3.70871845e-07,
         5.12550230e-07, -1.88172044e-06,  5.02668854e-07,
        -1.91474896e-07, -2.20284323e-06, -1.53661449e-06,
        -1.22784729e-06, -2.76115679e-06, -1.84408032e-06,
        -4.74175408e-07,  1.70983355e-07, -3.88522267e-07,
         1.11539032e-06, -1.69074451e-06,  1.48703566e-06,
        -1.34024134e-07,  2.91554807e-06, -2.15639261e-07],
       [ 5.17510955e-07, -3.38885968e-08, -1.57152726e-07,
         1.16564161e-06,  3.81499774e-07,  1.37838686e-06,
        -6.54859036e-07, -9.58057626e-07,  8.21418894e-07,
         8.39915970e-07,  1.20598065e-06, -9.84366579e-07,
         6.51407106e-09,  4.57132757e-07, -1.39499150e-08,
         1.96168003e-07,  1.70871135e-06,  5.29671865e-07,
         8.11515690e-07,  9.84609869e-07,  3.85247660e-07,
        -5.47605111e-07, -5.09249503e-07, -6.33938726e-07,
        -3.81786464e-07, -7.25405073e-07, -7.75973149e-07,
         8.89968419e-08, -1.59758179e-06, -1.49581638e-07],
       [ 7.43887540e-07,  4.72011493e-07, -2.08756182e-06,
        -5.22298137e-07, -2.26225347e-06, -1.65107792e-06,
        -3.79462278e-07,  1.95808616e-06, -7.87313525e-07,
        -6.37664073e-07, -1.16825390e-06,  6.13630277e-07,
        -3.93255107e-07, -1.36492974e-06,  4.88349031e-07,
        -1.67290409e-07, -8.81685423e-07, -1.31954562e-06,
        -1.04898641e-06, -2.01497915e-06, -2.63254094e-07,
        -8.30746671e-07,  2.68667264e-07, -4.43731182e-07,
         5.04908257e-07,  3.34773972e-07,  6.92679350e-07,
        -5.33562684e-07,  1.80955942e-06, -8.32338742e-07],
       [ 9.46624255e-08,  7.29403496e-07, -2.30002911e-06,
        -4.27971202e-07, -1.66065456e-06, -1.33500134e-06,
        -2.79809427e-07, -3.69741571e-08, -2.67794150e-07,
        -4.08006912e-07, -9.43420986e-08,  2.76455012e-07,
         2.52530782e-07, -7.78751485e-07,  5.80207143e-07,
         3.89637250e-07,  1.35934926e-07, -4.45981300e-07,
        -2.52677410e-07, -1.01119485e-06,  7.77504738e-07,
        -1.21277594e-06, -1.36204747e-07, -4.57581507e-07,
         1.81136144e-07,  8.74250532e-07,  4.29864031e-07,
        -4.02056884e-07,  5.11728331e-07, -5.99297096e-07],
       [ 1.76787239e-07,  6.62062178e-07, -1.17438969e-06,
        -1.50513017e-07,  2.37235156e-08,  2.61222993e-07,
         2.81033408e-07, -7.60072908e-07,  2.50972698e-06,
         7.23708098e-08, -1.55507564e-07,  6.19348612e-07,
        -3.97581573e-07, -7.39527877e-07,  2.32921138e-06,
         1.06933408e-06, -1.00157899e-06,  3.96334741e-07,
         7.83787755e-07, -3.15496180e-07,  8.55646249e-07,
         1.24662563e-06,  1.74123755e-07,  4.54642020e-07,
        -3.75595761e-07,  1.03718412e-06, -9.29170881e-07,
        -5.20032302e-07,  3.27716521e-09, -1.76897231e-06],
       [-1.92619086e-06, -2.05006387e-07, -3.01703864e-07,
         3.80918834e-07,  1.69989249e-07, -8.16156103e-08,
        -1.10351584e-06,  9.05856439e-07, -6.47991556e-07,
         1.02162758e-07,  8.21055096e-07,  2.18994188e-07,
         6.28406895e-07, -5.06165350e-07, -4.34923038e-07,
         4.07463688e-07, -2.19632881e-07,  2.51501490e-07,
         3.43435346e-07,  9.36889535e-08, -5.77770152e-07,
        -6.66548658e-07, -4.55266502e-07, -3.70609371e-07,
         2.10675637e-07, -1.90353251e-06,  2.40920258e-07,
         4.05946651e-07,  8.56720703e-08,  5.03895876e-07],
       [ 7.16127943e-07, -7.19853119e-07,  7.25282121e-07,
         9.77692252e-07,  1.14578870e-06,  1.24951032e-06,
        -3.76694686e-07, -8.26936116e-07,  9.24758808e-07,
         3.98465431e-07,  1.46087211e-06, -6.23402457e-07,
         9.80153629e-08,  1.50432527e-06, -1.41294856e-06,
         1.41974510e-07,  1.85223371e-06,  2.41124354e-07,
         8.09770995e-07,  1.47335811e-06,  5.52215056e-07,
        -6.91191417e-07, -1.54069497e-07, -6.33624154e-07,
        -2.36743418e-07, -7.60786975e-07, -2.97676735e-07,
        -1.23765119e-07, -1.46154764e-06,  1.83535803e-06]], dtype=float32)>, <tf.Tensor: shape=(30, 30), dtype=float32, numpy=
array([[-2.85462875e-07, -1.91237928e-06,  3.83672557e-07,
        -1.58857301e-06, -6.27114787e-07,  7.39567838e-07,
        -2.61351630e-07,  1.14228010e-06, -1.80034613e-06,
        -7.54210873e-07,  5.56522025e-07, -1.31359616e-06,
         1.31875629e-06, -1.46658010e-08, -2.24224664e-06,
         9.20015566e-08, -6.71151781e-07, -6.29832186e-07,
        -1.44655382e-06, -1.06377149e-06, -2.04311596e-06,
        -3.35461891e-07, -2.25691238e-07, -2.47690764e-07,
         1.08148174e-06, -1.78483583e-06,  1.76162462e-06,
        -2.43517320e-07,  8.21406047e-07,  3.01974342e-06],
       [ 2.87981516e-06, -7.82984728e-07, -1.77781476e-06,
        -7.76972456e-07, -2.36082246e-06,  1.81387932e-07,
         6.83886924e-07, -5.38288475e-07,  1.93084952e-06,
         2.69072586e-07, -6.99265001e-08, -3.84477516e-07,
         5.05261596e-07,  1.25000997e-06,  1.31139529e-06,
         2.82062524e-06,  3.98189854e-08,  8.63140144e-07,
        -1.04886446e-07, -7.33727802e-07, -3.02518458e-07,
        -6.71027919e-07,  7.33070181e-07, -1.55385737e-06,
         4.05312107e-07,  1.38394103e-06,  6.76261664e-07,
        -1.46227009e-07, -6.67446443e-07,  7.60203989e-07],
       [ 1.10768633e-06, -1.55007990e-06, -6.54952260e-07,
        -1.46052150e-06, -1.40073462e-06,  8.10128938e-07,
         3.62308185e-07,  1.07642177e-06, -4.20862534e-07,
         5.66712401e-08,  8.03857034e-08, -9.71324766e-07,
         1.15330067e-06,  1.07430040e-07, -3.15992565e-08,
         9.34209993e-07, -1.53360656e-06,  3.23166120e-07,
        -1.16541594e-06, -1.71349620e-06, -2.25516669e-06,
         7.46441856e-07,  3.01608793e-07, -6.60495061e-07,
         1.12440125e-06, -6.58044200e-07,  1.47611217e-06,
        -8.15293362e-08,  8.87097087e-07,  1.34501784e-06],
       [ 1.29023829e-06, -6.08946152e-07,  2.84869293e-06,
        -1.23148823e-07,  9.14343161e-07, -9.62468789e-08,
         1.62621859e-06,  1.91618074e-06, -1.83798079e-06,
        -6.90764068e-09, -1.01282478e-06,  1.16137088e-07,
        -3.37325446e-07,  7.42495104e-07, -1.34228185e-06,
        -2.91373499e-06, -1.61468211e-06, -8.34512662e-07,
        -1.16471676e-06, -1.31582647e-06, -1.64381493e-06,
         2.18556875e-06,  6.10829147e-07,  1.15772116e-06,
         5.54053656e-08,  1.27592170e-06,  1.05524884e-06,
        -7.21233988e-08,  2.16763965e-06,  4.56098405e-07],
       [ 1.83720522e-06, -1.05323863e-07,  3.48041567e-06,
         1.13555507e-06,  1.78409221e-06, -2.95615052e-07,
         1.97418012e-06,  1.94856102e-06, -1.45129070e-06,
         4.54955540e-07, -7.59771638e-07,  4.10749237e-07,
        -9.50144567e-07,  8.00440318e-07, -1.25516567e-06,
        -3.97355598e-06, -9.44399460e-07, -8.53432255e-07,
        -8.87214867e-07, -7.74040927e-07, -5.25823054e-07,
         2.29667148e-06,  7.07899972e-07,  1.42053307e-06,
        -6.37405037e-07,  2.95413906e-06,  6.15466377e-07,
        -8.78735662e-08,  1.62658671e-06, -5.31850901e-07],
       [-1.11147165e-06, -1.14182143e-07, -2.08957840e-06,
        -3.95750646e-07, -6.66069809e-07,  6.63040908e-07,
        -1.08200550e-06, -1.94011272e-06,  2.27904684e-06,
        -1.13959857e-07,  9.68869131e-07, -1.78039897e-07,
         7.65188133e-07,  9.59134354e-08,  1.23956238e-06,
         4.17627734e-06,  1.17134914e-06,  1.27290627e-06,
         1.26443581e-06,  1.81440032e-06,  9.92482001e-07,
        -1.96940073e-06, -3.13922811e-07, -1.57561112e-06,
         2.60161073e-07, -1.76500544e-06, -9.33874901e-07,
         2.05719644e-07, -2.44393186e-06,  8.71601230e-07],
       [ 1.16424326e-06,  3.36425444e-07, -1.53905091e-06,
         7.04241984e-07, -1.15339640e-06, -1.12006234e-07,
        -8.97883581e-08, -6.41165798e-07,  9.56370286e-07,
         9.57130510e-07,  4.69364778e-07, -2.58909097e-07,
        -1.90174092e-07,  5.37143194e-07,  1.14155523e-06,
         5.75306217e-07,  9.33853130e-07,  8.35221272e-07,
         5.15315548e-07,  3.80308563e-07,  4.10795963e-07,
        -1.34215384e-06, -1.54811261e-07, -1.11718475e-06,
        -1.59205342e-08,  2.80394914e-07, -8.31274917e-07,
         1.31495881e-08, -7.06698586e-07, -7.20195771e-07],
       [-6.97450560e-08, -1.70260108e-07,  2.37053473e-06,
         8.90659066e-07,  1.65136123e-06, -4.59200294e-07,
         6.78462300e-07,  8.03636453e-07, -1.70960470e-06,
         2.22746451e-07,  3.30565541e-07,  1.09011609e-08,
        -6.40082988e-07,  5.39696714e-07, -1.80533175e-06,
        -2.74365902e-06,  4.07042876e-07, -8.83592577e-07,
        -5.86266651e-07,  2.74391681e-07,  3.48950493e-07,
         1.24228151e-07, -2.25684005e-08,  7.39486836e-07,
        -2.48486913e-07,  7.60470641e-07,  1.28576360e-07,
        -1.54142768e-07,  6.01639272e-07,  5.25699534e-07],
       [ 1.78462187e-06, -3.55347169e-07,  3.79334915e-06,
        -2.77793589e-07,  7.30004899e-07, -3.40196493e-07,
         1.82052713e-06,  1.64226162e-06, -1.67676546e-06,
        -5.58433840e-07, -1.62319930e-06,  4.45739772e-07,
        -7.77294190e-07,  1.41099440e-06, -1.65330425e-06,
        -2.81071448e-06, -1.31256400e-06, -7.70199961e-07,
        -9.94714128e-07, -1.19489414e-06, -1.44057833e-06,
         2.11401539e-06,  8.68587620e-07,  1.45297088e-06,
        -1.89506352e-07,  2.03670379e-06,  9.58430178e-07,
        -8.76400463e-09,  2.45375168e-06,  5.92448771e-07],
       [ 1.20778805e-06, -1.68506108e-06,  1.43860268e-06,
        -6.73291368e-07,  1.26548571e-07,  1.41002488e-06,
         4.34231367e-07,  3.93845028e-07, -2.27539715e-07,
        -8.04574398e-08,  5.93550624e-07, -1.12326745e-06,
         9.11175277e-07,  1.32537855e-06, -1.56165254e-06,
         6.67919380e-07, -3.41677861e-07,  2.84838734e-07,
        -4.60032425e-07, -3.83932559e-07, -1.45648551e-06,
         7.62952368e-07,  3.11368893e-07, -2.93554990e-07,
         4.60920660e-07, -3.82411713e-07,  1.33254991e-06,
         1.01810031e-07, -5.14390877e-08,  2.43477689e-06],
       [ 1.88943773e-07, -3.09068156e-07,  7.57377734e-07,
        -9.74593263e-07,  3.46552980e-07,  4.69026361e-07,
         7.46637284e-07,  3.21655193e-07,  2.81396865e-07,
        -4.57217311e-07, -5.91701792e-07, -1.83026430e-08,
         4.07884272e-07, -4.73754653e-08,  5.98161591e-08,
         3.67561313e-07, -1.58616365e-06, -2.22705808e-07,
        -1.38984149e-07, -4.33847106e-07, -5.90288266e-07,
         1.62548599e-06,  3.07480576e-07,  7.15808483e-07,
         7.22697280e-08,  4.35672746e-07,  5.74611192e-07,
         2.45884451e-08,  5.83963129e-07,  5.07036248e-07],
       [-1.25195515e-06,  1.05054653e-07,  1.39918279e-06,
        -2.31401060e-07,  1.10222118e-06, -5.56526572e-07,
         8.56619380e-08,  8.99458257e-07, -1.51937581e-06,
        -7.53399718e-07, -6.79520497e-07,  5.72355589e-07,
        -3.40862982e-07, -9.49765422e-07, -1.00712305e-06,
        -1.81742428e-06, -9.37750542e-07, -9.73190254e-07,
        -8.02028353e-07, -6.11754274e-07, -2.40824960e-07,
         1.13700560e-06,  1.47972798e-08,  1.33811386e-06,
        -9.89325031e-08,  3.29606934e-07,  4.67794081e-07,
        -1.06502817e-07,  1.26097621e-06, -4.17615240e-07],
       [-9.98917017e-07,  1.48244760e-06,  2.38640314e-06,
         8.15464034e-07,  1.30909723e-06, -2.34712138e-06,
         6.69792030e-07,  2.75763659e-06, -2.52693280e-06,
        -3.29295744e-07, -1.86550039e-06,  1.88427475e-06,
        -1.31918887e-06, -1.11846134e-06, -7.46823275e-07,
        -4.19921525e-06, -2.07711059e-06, -1.64649168e-06,
        -3.56792214e-07, -1.22566712e-06,  4.16967438e-08,
         1.72260138e-06,  1.30966498e-07,  2.34218169e-06,
        -6.21039931e-07,  1.31262868e-06,  2.92377365e-07,
         2.13943693e-07,  3.29900740e-06, -1.86179830e-06],
       [ 2.07237690e-06,  2.45983500e-07, -1.42892588e-06,
        -2.78786956e-07, -1.96443557e-06, -1.00398393e-06,
         8.74617967e-07,  7.05879472e-07,  1.07193318e-06,
         3.91536304e-07, -7.83761948e-07,  4.90492766e-07,
        -1.32330911e-08,  5.39525672e-07,  1.50058781e-06,
         1.34153879e-06, -7.89403941e-07,  7.79836284e-08,
         2.88536739e-07, -9.03015632e-07,  6.26058352e-08,
        -5.66618553e-07,  5.52921392e-07, -7.51186121e-07,
         1.88775104e-07,  1.40253042e-06,  3.83542186e-07,
         2.23378009e-08,  5.83344786e-07, -1.74109729e-07],
       [-7.03135015e-07, -1.13266356e-06, -1.60365926e-06,
        -9.75172952e-07, -7.60318187e-07,  1.11368024e-06,
        -5.05594187e-07, -3.13646837e-07,  8.48253137e-07,
         1.73010221e-07,  9.35487435e-07, -8.15833573e-07,
         9.30109422e-07, -2.74091178e-07,  7.77997002e-07,
         2.41888210e-06, -2.53462815e-07,  6.90181650e-07,
         1.77976162e-07,  3.71403473e-07, -8.75507169e-07,
        -6.09069502e-07, -2.51500865e-07, -9.49028390e-07,
         7.19449645e-07, -1.94065933e-06,  4.78199240e-08,
        -1.41565408e-08, -7.44548117e-07,  9.67723167e-07],
       [-6.13661086e-07,  1.02455101e-06, -1.00501506e-06,
         1.39185067e-06, -3.08971352e-07, -5.06627771e-07,
        -9.17733132e-07,  1.03339835e-06, -1.56290298e-07,
         6.48320849e-07,  2.01790186e-07,  2.97985224e-07,
        -4.29838536e-07, -5.08945050e-07,  3.23790800e-07,
        -4.71659973e-07,  5.92720653e-07, -6.25538064e-07,
         1.09123562e-06,  3.40829786e-07,  4.20759676e-07,
        -1.27846749e-06, -5.64482605e-07, -3.07753226e-07,
        -3.77788751e-07, -1.34196353e-06, -6.11579878e-07,
         3.09688289e-07,  2.24181576e-07, -9.24249264e-07],
       [ 1.70001340e-06,  1.75954057e-07, -5.65485152e-08,
         9.23480570e-07, -9.79991000e-07, -6.53669076e-07,
         1.18141656e-07,  1.42046019e-06, -1.34733614e-06,
         4.65024584e-07, -7.59921974e-08,  2.16154206e-10,
        -1.25166764e-07,  2.55168402e-07, -7.90370450e-07,
        -2.15599698e-06,  1.39449043e-07, -4.16840408e-07,
        -4.51313099e-07, -1.41284818e-06, -4.86129522e-07,
        -3.12655573e-07,  1.13173542e-08, -2.56545690e-07,
        -8.98443346e-08,  1.23256996e-06,  7.53671770e-07,
        -9.38960341e-08,  1.37239931e-06, -5.26278768e-07],
       [-1.42865599e-06,  1.01597050e-06, -2.28480371e-06,
        -7.97040798e-08, -4.76946411e-07, -6.06313790e-07,
        -8.36441814e-07, -1.19581034e-06,  7.41474139e-07,
        -2.96494932e-07,  2.27437482e-07,  3.94525841e-07,
        -1.25797868e-07, -1.33410822e-06,  1.13856049e-06,
         1.16650176e-06,  8.75338003e-07, -6.36750315e-08,
         6.02293426e-07,  6.66146548e-07,  1.65020515e-06,
        -1.37220388e-06, -4.47792615e-07, -2.54995712e-07,
        -1.53802404e-07, -4.20906787e-07, -1.00420402e-06,
        -1.59153004e-07, -9.79068773e-07, -1.19307265e-06],
       [-1.61322419e-06,  1.75878392e-06, -3.25720407e-07,
         1.67332678e-06,  9.30195881e-07, -1.23962593e-06,
        -7.27467182e-07, -1.51812401e-07,  1.03438765e-07,
         4.39902664e-07, -6.92495234e-08,  1.20156676e-06,
        -1.04852825e-06, -8.33804222e-07,  9.09591222e-07,
        -1.10646351e-06,  8.10658946e-07, -2.68734993e-07,
         1.13604767e-06,  1.09057999e-06,  1.75216849e-06,
        -8.11445773e-07, -4.55053907e-07,  4.11158368e-07,
        -8.51519701e-07, -6.45480895e-08, -1.50053245e-06,
         2.63011458e-07, -3.06032746e-07, -2.20241736e-06],
       [ 1.69251678e-07, -4.33233765e-07, -2.14475290e-06,
        -9.16253271e-07, -2.12571967e-06,  2.08080053e-07,
        -9.98308337e-07,  9.70468705e-07, -3.36354390e-07,
        -1.99315238e-07,  1.53331229e-07, -8.79834374e-07,
         1.16420063e-06, -3.25287829e-07, -4.41339438e-07,
         1.61785658e-06, -7.26091542e-09, -4.60577638e-07,
        -8.56972804e-08, -7.93934646e-07, -1.07372102e-06,
        -1.52870552e-06, -3.85853838e-07, -8.72288297e-07,
         7.13554584e-07, -1.98918519e-06,  1.13751344e-06,
         9.37061131e-08,  3.64704647e-07,  1.58373803e-06],
       [ 1.45296826e-06, -7.38459448e-07, -2.04174569e-07,
         1.41146700e-07, -3.86846068e-07,  4.14409897e-07,
         5.09755807e-07, -9.66287189e-07,  8.30717624e-07,
         6.82798998e-07,  1.04136996e-06, -6.87101419e-07,
         1.52048884e-07,  1.36418384e-06, -8.64129106e-08,
         9.02571628e-07,  1.13323983e-06,  6.67820018e-07,
         1.82335469e-07,  7.57804514e-07,  3.14667034e-07,
        -1.15221906e-06,  1.99347738e-09, -1.10359622e-06,
         3.34786819e-07,  1.58147998e-07, -2.59555776e-07,
        -2.32552452e-07, -1.09911809e-06,  1.26369321e-06],
       [-8.04505817e-07,  1.56344572e-06,  1.77875449e-06,
         1.24354005e-06,  1.99373562e-06, -8.89833018e-07,
         2.31032956e-07, -6.26901510e-07, -1.99282681e-07,
        -3.62400613e-07, -9.10475308e-07,  1.39949816e-06,
        -1.42416366e-06, -3.51601273e-07, -2.36587539e-07,
        -2.07597350e-06,  4.85226849e-07, -5.02347689e-07,
         7.67604490e-07,  9.88213742e-07,  1.82015663e-06,
         1.00364934e-06,  9.65311528e-08,  1.52607913e-06,
        -1.19967115e-06,  1.43766556e-06, -1.14375030e-06,
         9.32181052e-08,  1.24348588e-07, -1.96691758e-06],
       [ 1.76428330e-08,  6.65566233e-08, -2.05398601e-06,
         3.72240549e-07, -9.21995422e-07,  2.09509324e-07,
        -1.02374861e-06, -7.64113338e-07,  7.04575768e-07,
         4.96953419e-07,  9.77478066e-07, -5.14796682e-07,
         3.80279118e-07, -8.06043090e-08,  3.97290449e-07,
         1.21850030e-06,  1.21148491e-06,  5.01982697e-07,
         5.52234837e-07,  3.38432528e-07,  4.15287843e-07,
        -1.63677191e-06, -4.96443135e-07, -1.23505140e-06,
         1.59162354e-07, -1.12838165e-06, -4.97604162e-07,
        -1.40944039e-08, -1.09342659e-06,  2.43632030e-08],
       [-1.80456402e-06,  6.39199698e-07, -3.24745974e-06,
         8.03377588e-07, -3.17148306e-07,  5.70552515e-07,
        -1.98159114e-06, -1.42824854e-06,  1.32242587e-06,
         6.81796337e-07,  1.81535813e-06, -5.02167268e-07,
         8.54620623e-07, -1.11054226e-06,  8.95445851e-07,
         2.44516013e-06,  1.74880108e-06,  8.38581968e-07,
         1.44723765e-06,  1.60628656e-06,  1.22409358e-06,
        -2.16397348e-06, -1.07921574e-06, -1.33651383e-06,
        -8.05322600e-08, -2.67890300e-06, -9.72909561e-07,
         3.86092466e-07, -2.44883404e-06, -9.79406991e-08],
       [ 2.48079914e-07,  2.00644024e-07,  1.35180699e-06,
         4.06078129e-07,  9.02448789e-07, -4.70669306e-07,
         5.66794995e-07, -9.12948337e-07, -4.84928819e-07,
        -4.37340702e-07,  1.95475138e-08,  4.46913134e-07,
        -4.61850277e-07,  8.10452320e-07, -1.03494108e-06,
        -1.18388482e-06,  9.51274501e-07, -7.68624915e-08,
        -2.76880655e-07,  3.28315195e-07,  9.82064080e-07,
         1.72281119e-07,  4.00893811e-07,  3.72406134e-07,
        -3.94140017e-07,  1.69465534e-06,  6.36270840e-08,
        -1.97709610e-07, -1.87639216e-07, -1.12582597e-07],
       [-2.71944828e-06,  4.21699383e-07, -2.57850502e-06,
        -3.24505606e-07, -5.38852873e-09,  5.39965754e-07,
        -1.62880769e-06, -1.07104120e-06,  4.58734405e-07,
        -4.52910058e-07,  8.52518610e-07, -1.27008889e-07,
         8.17429111e-07, -1.58137277e-06,  3.62794452e-07,
         1.80919096e-06,  8.90167030e-07,  2.15536772e-07,
         7.60116450e-07,  1.09621055e-06,  5.49279207e-07,
        -1.17323373e-06, -6.91036917e-07, -4.34674405e-07,
         1.30137181e-07, -2.64407299e-06, -4.51884887e-07,
         1.70157762e-07, -1.32945263e-06,  2.41553266e-08],
       [-1.52852306e-06, -2.82405495e-07, -7.21566380e-07,
        -9.36774200e-07, -2.75970251e-07,  5.34209221e-07,
        -6.38950951e-07, -1.35555501e-06,  8.45764419e-07,
        -4.60422484e-07,  3.47455000e-07, -3.92907850e-07,
         4.58306999e-07, -1.40035539e-07,  2.56641016e-07,
         2.05964307e-06,  4.06171296e-07,  6.48557489e-07,
         4.08410159e-07,  1.30699618e-06,  2.14451518e-07,
        -1.10713268e-06, -3.79457390e-07, -2.93023732e-07,
         2.81773623e-07, -1.70184126e-06, -6.51086168e-07,
         1.22706325e-07, -1.10362976e-06,  1.11408326e-06],
       [ 1.44383785e-06, -2.15725413e-06, -4.45790874e-07,
        -1.02150511e-06, -3.88379931e-07,  1.66408120e-06,
         9.66500806e-07, -1.44242847e-07, -1.92947937e-07,
         5.88191597e-07,  1.74022375e-06, -1.76234778e-06,
         1.46612967e-06,  1.02028719e-06, -7.03487558e-07,
         9.12377175e-07, -2.91155658e-07,  6.84943757e-07,
        -8.23084065e-07, -3.23493282e-07, -1.61834441e-06,
         2.64631751e-07,  3.24726415e-08, -1.13660349e-06,
         1.08820973e-06, -8.33642275e-07,  1.08266534e-06,
        -3.21283750e-07, -4.63725598e-07,  2.59217268e-06],
       [ 1.00045577e-06,  8.07301603e-07,  9.21669482e-07,
         1.40882889e-06,  1.23186055e-06, -1.76688957e-08,
         8.40700523e-07, -9.45608519e-07,  1.31189813e-06,
         9.05360025e-07,  1.93224793e-07,  4.85308760e-07,
        -1.03089417e-06,  7.91132152e-07,  8.40489633e-07,
        -1.00588238e-06,  7.72484100e-07,  5.93290054e-07,
         1.03249829e-06,  1.22815618e-06,  1.34394236e-06,
         3.88270394e-07,  4.16321413e-08,  2.77134518e-07,
        -9.14239422e-07,  1.69711666e-06, -1.45020886e-06,
        -3.31141194e-08, -7.63179287e-07, -1.29497914e-06],
       [-1.08752795e-06,  4.26703679e-07,  2.29623424e-06,
         2.21301065e-07,  1.79303265e-06, -8.59089596e-07,
         5.37618007e-07,  1.08203676e-06, -1.94874951e-06,
        -6.39298605e-07, -9.80275217e-07,  1.00873353e-06,
        -9.05137085e-07, -7.65568814e-07, -1.26621376e-06,
        -3.18796037e-06, -9.65073923e-07, -1.32837215e-06,
        -6.94122491e-07, -4.12622796e-07,  6.37907647e-08,
         1.65228744e-06,  2.03858406e-08,  1.95211510e-06,
        -4.31010335e-07,  9.85162956e-07,  1.82763458e-07,
        -1.38033499e-07,  1.69242389e-06, -8.91400873e-07]], dtype=float32)>, <tf.Tensor: shape=(60,), dtype=float32, numpy=
array([ 2.5330007e-07, -1.8093965e-06,  8.5207932e-07, -1.1342140e-06,
       -7.0491029e-07,  5.7418850e-07,  1.4867334e-07,  2.0628763e-06,
       -2.1951619e-06, -3.1106669e-07,  2.2665759e-07, -1.1310030e-06,
        1.1372941e-06,  3.5479178e-07, -1.9511131e-06, -3.1074867e-07,
       -1.0536676e-06, -5.6415013e-07, -1.4823104e-06, -1.3114376e-06,
       -2.4880901e-06,  3.1922724e-08,  1.0641787e-07, -4.3827575e-07,
        1.1143759e-06, -1.5397412e-06,  1.9006530e-06,  9.0521880e-08,
        1.2324656e-06,  2.4789999e-06,  1.3378146e-06, -8.2043562e-07,
        3.0029662e-07, -1.0298352e-06,  1.2611025e-08,  1.1590364e-06,
        9.0473816e-07, -1.6197956e-06,  1.5773815e-06, -1.5555240e-07,
        1.7176453e-07, -6.2706511e-07,  3.1520364e-07,  8.6816971e-07,
        3.8978507e-07,  1.3740860e-06, -1.0876360e-07,  8.0478600e-07,
       -9.7244779e-08,  2.8589227e-07,  2.4776529e-08,  9.1320084e-07,
        3.3497145e-07, -2.3258416e-08,  7.9961815e-08,  1.2007986e-06,
       -1.4733951e-08, -3.5442065e-07, -9.1862080e-07,  8.7871445e-07],
      dtype=float32)>, <tf.Tensor: shape=(60, 30), dtype=float32, numpy=
array([[ 7.9309052e-07, -8.6873098e-07, -9.1010975e-07, ...,
        -2.3275651e-07, -5.8088870e-07,  8.7815096e-08],
       [ 2.9762697e-07, -6.4651942e-07, -1.7007615e-06, ...,
        -1.7233509e-07, -6.4179858e-07, -5.8921216e-07],
       [ 2.0972074e-07, -6.5902799e-07, -6.3392349e-07, ...,
        -4.3746564e-07, -6.8294133e-07,  1.4148438e-07],
       ...,
       [-7.9053677e-08,  1.9258957e-07,  1.7098332e-06, ...,
        -3.2206304e-07,  1.1374462e-06,  1.2137476e-06],
       [-6.6935286e-08,  1.9323045e-07,  1.7111804e-06, ...,
        -3.2662109e-07,  1.1557869e-06,  1.2169439e-06],
       [ 1.6462652e-06, -3.5226114e-07, -1.1928796e-06, ...,
        -2.4904261e-07, -4.2516365e-07,  6.4198531e-09]], dtype=float32)>, <tf.Tensor: shape=(60, 30), dtype=float32, numpy=
array([[-1.7988478e-07, -2.8150373e-06,  1.9373398e-07, ...,
        -4.2149463e-08,  1.3981082e-06, -1.7448406e-07],
       [-4.7824443e-08, -1.3199268e-06,  2.4303480e-07, ...,
        -5.8306767e-07,  5.4273647e-07,  1.1003458e-06],
       [-2.0069756e-08, -1.2141448e-06,  8.9433098e-07, ...,
        -6.6161954e-07, -6.1352569e-07,  9.5273634e-07],
       ...,
       [-8.0220431e-08, -1.3864565e-06,  4.9933135e-07, ...,
         3.9809407e-07, -1.5421047e-06, -2.8634585e-07],
       [-7.8091716e-08, -1.3980347e-06,  5.0008407e-07, ...,
         3.9785573e-07, -1.5153133e-06, -3.1351806e-07],
       [-1.2174225e-07, -1.3585581e-06, -1.7799432e-07, ...,
        -3.1683857e-07,  2.7478789e-06, -9.8074099e-07]], dtype=float32)>, <tf.Tensor: shape=(60,), dtype=float32, numpy=
array([ 4.6613401e-08, -1.9428440e-07, -1.7128546e-06,  1.1222677e-06,
        1.8037727e-07,  1.2104797e-06,  2.1494950e-06,  8.0929885e-07,
       -1.1872803e-06,  7.5780775e-07, -4.3736441e-07, -2.2002224e-07,
       -9.6301028e-07,  2.3739692e-06, -1.1086178e-06,  1.5551898e-07,
        5.5797432e-07,  2.5225683e-07,  2.1670404e-07,  5.8044139e-07,
       -4.3640546e-07, -9.2759436e-09,  1.5023420e-06,  1.2799413e-06,
        1.7016362e-07, -1.7112618e-06,  1.7509154e-06,  3.3418954e-07,
       -1.1878825e-06, -1.2215711e-06, -7.4100207e-08, -1.4162263e-06,
        5.0096037e-07,  1.6836698e-06, -5.5392888e-07,  1.2256634e-06,
       -2.1070241e-07, -9.6693748e-07, -1.0126162e-06,  5.4586877e-07,
        1.7242304e-06,  1.4150831e-07,  8.2416886e-07,  6.6887571e-07,
       -7.3330187e-07,  1.1216366e-06,  1.3996163e-06, -4.5850402e-07,
        1.4958657e-06, -1.1136971e-07, -1.4352579e-06, -8.5753123e-07,
       -1.8686701e-06,  3.3918374e-07,  2.3711979e-07, -8.1509376e-07,
       -5.3321281e-07,  3.9732686e-07, -1.4677438e-06, -3.6121452e-07],
      dtype=float32)>]
[Actor] Episode 7 Step 0 Loss: 417.6799011230469
Y Pred
tf.Tensor(
[[[-0.7966099   0.3865498   0.8278721  ... -0.986277    0.6737412
    0.5676516 ]
  [ 0.03910189 -0.02190648  0.00522285 ... -0.01947617  0.00446315
   -0.03124514]
  [ 0.03910189 -0.02190648  0.00522285 ... -0.01947617  0.00446315
   -0.03124514]
  ...
  [ 0.03910189 -0.02190648  0.00522285 ... -0.01947617  0.00446315
   -0.03124514]
  [ 0.03910189 -0.02190648  0.00522285 ... -0.01947617  0.00446315
   -0.03124514]
  [ 0.03910189 -0.02190648  0.00522285 ... -0.01947617  0.00446315
   -0.03124514]]

 [[ 0.9523789  -0.5134933  -0.2816209  ...  0.8946166  -0.5008163
   -0.923781  ]
  [ 0.03910189 -0.02190648  0.00522285 ... -0.01947617  0.00446315
   -0.03124514]
  [ 0.03910189 -0.02190648  0.00522285 ... -0.01947617  0.00446315
   -0.03124514]
  ...
  [ 0.03910189 -0.02190648  0.00522285 ... -0.01947617  0.00446315
   -0.03124514]
  [ 0.03910189 -0.02190648  0.00522285 ... -0.01947617  0.00446315
   -0.03124514]
  [ 0.03910189 -0.02190648  0.00522285 ... -0.01947617  0.00446315
   -0.03124514]]

 [[-0.993438    0.90186113 -0.06896222 ...  0.7890235  -0.3926139
    0.9191146 ]
  [ 0.03910189 -0.02190648  0.00522285 ... -0.01947617  0.00446315
   -0.03124514]
  [ 0.03910189 -0.02190648  0.00522285 ... -0.01947617  0.00446315
   -0.03124514]
  ...
  [ 0.03910189 -0.02190648  0.00522285 ... -0.01947617  0.00446315
   -0.03124514]
  [ 0.03910189 -0.02190648  0.00522285 ... -0.01947617  0.00446315
   -0.03124514]
  [ 0.03910189 -0.02190648  0.00522285 ... -0.01947617  0.00446315
   -0.03124514]]

 ...

 [[ 0.9608447  -0.4722719  -0.3625586  ...  0.91419035 -0.49748534
   -0.9184643 ]
  [ 0.03910189 -0.02190648  0.00522285 ... -0.01947617  0.00446315
   -0.03124514]
  [ 0.03910189 -0.02190648  0.00522285 ... -0.01947617  0.00446315
   -0.03124514]
  ...
  [ 0.03910189 -0.02190648  0.00522285 ... -0.01947617  0.00446315
   -0.03124514]
  [ 0.03910189 -0.02190648  0.00522285 ... -0.01947617  0.00446315
   -0.03124514]
  [ 0.03910189 -0.02190648  0.00522285 ... -0.01947617  0.00446315
   -0.03124514]]

 [[ 0.9433934  -0.58340496  0.5564845  ... -0.8322709   0.40517554
   -0.85110927]
  [ 0.03910189 -0.02190648  0.00522285 ... -0.01947617  0.00446315
   -0.03124514]
  [ 0.03910189 -0.02190648  0.00522285 ... -0.01947617  0.00446315
   -0.03124514]
  ...
  [ 0.03910189 -0.02190648  0.00522285 ... -0.01947617  0.00446315
   -0.03124514]
  [ 0.03910189 -0.02190648  0.00522285 ... -0.01947617  0.00446315
   -0.03124514]
  [ 0.03910189 -0.02190648  0.00522285 ... -0.01947617  0.00446315
   -0.03124514]]

 [[ 0.9414259  -0.5039149   0.867535   ... -0.98152536  0.8193897
   -0.42388445]
  [ 0.03910189 -0.02190648  0.00522285 ... -0.01947617  0.00446315
   -0.03124514]
  [ 0.03910189 -0.02190648  0.00522285 ... -0.01947617  0.00446315
   -0.03124514]
  ...
  [ 0.03910189 -0.02190648  0.00522285 ... -0.01947617  0.00446315
   -0.03124514]
  [ 0.03910189 -0.02190648  0.00522285 ... -0.01947617  0.00446315
   -0.03124514]
  [ 0.03910189 -0.02190648  0.00522285 ... -0.01947617  0.00446315
   -0.03124514]]], shape=(1125, 350, 12), dtype=float32)
omega
tf.Tensor(
[[15.555287]
 [21.139132]
 [36.096607]
 ...
 [36.096607]
 [55.30133 ]
 [15.555287]], shape=(1125, 1), dtype=float32)
mu
tf.Tensor(
[[0.48557794 0.1121356  0.53483677 ... 0.48204356 0.08986519 0.52579546]
 [0.4636634  0.13450481 0.49088734 ... 0.52586824 0.07077007 0.5159844 ]
 [0.44673172 0.13515869 0.45006287 ... 0.5329904  0.04271232 0.4779787 ]
 ...
 [0.44673172 0.13515869 0.45006287 ... 0.5329904  0.04271232 0.4779787 ]
 [0.4747922  0.10053624 0.4851761  ... 0.50007254 0.07502052 0.48110873]
 [0.48557794 0.1121356  0.53483677 ... 0.48204356 0.08986519 0.52579546]], shape=(1125, 12), dtype=float32)
mean
tf.Tensor(
[[-0.06609413  0.16847123 -0.07562591 ... -0.00103572  0.13157713
   0.05541323]
 [-0.08191307  0.15888828 -0.02082171 ... -0.01072382  0.13158932
   0.06104499]
 [-0.06296262  0.16320492  0.00297839 ... -0.00681913  0.115684
   0.04606649]
 ...
 [-0.06296262  0.16320492  0.00297839 ... -0.00681913  0.115684
   0.04606649]
 [-0.05268367  0.15463303 -0.06325252 ...  0.00886651  0.11427076
   0.05159779]
 [-0.06609413  0.16847123 -0.07562591 ... -0.00103572  0.13157713
   0.05541323]], shape=(1125, 12), dtype=float32)
grads
[<tf.Tensor: shape=(6, 30), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(30,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan], dtype=float32)>, <tf.Tensor: shape=(30, 60), dtype=float32, numpy=
array([[nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       ...,
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(60,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan], dtype=float32)>, <tf.Tensor: shape=(60, 30), dtype=float32, numpy=
array([[nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       ...,
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(30,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan], dtype=float32)>, <tf.Tensor: shape=(30, 25), dtype=float32, numpy=
array([[-2.23110081e-04,  1.92137260e-03,  3.18890484e-03,
         7.28212879e-04, -2.53191014e-04,  1.15615090e-04,
         6.24022505e-05, -1.21951103e-04, -1.02299993e-04,
         1.86296005e-04,  6.75393676e-04,  7.97763460e-06,
         2.23582610e-03,  3.52790921e-05, -2.29335797e-04,
        -2.40779409e-04, -9.33574571e-04, -7.65746576e-04,
        -1.38416630e-03,  6.67403874e-05, -1.77503476e-04,
         1.44694513e-03, -4.13695525e-04, -2.59050139e-04,
        -1.33295648e-03],
       [ 4.19770629e-04, -6.10443484e-03, -6.35061832e-03,
        -1.08904648e-03, -4.62853350e-04, -1.79210980e-03,
         2.20491365e-03, -9.81472840e-05, -2.28472520e-03,
        -3.16282967e-03, -4.01140423e-03,  3.55671946e-05,
        -4.81640734e-03,  1.11680070e-04,  2.42042661e-04,
         4.52109473e-03,  2.17598211e-03,  1.07580877e-03,
         5.40593034e-03, -1.24510581e-04, -3.29390471e-03,
        -2.20362563e-03,  3.11736483e-03,  9.43005580e-05,
         4.40050615e-03],
       [-3.80616228e-04,  5.48033696e-03,  5.61955664e-03,
         8.94535682e-04,  4.44865727e-04,  1.64256862e-03,
        -2.05676910e-03,  1.20075551e-04,  2.13497109e-03,
         2.95297429e-03,  3.59832821e-03, -4.31670924e-05,
         4.25461493e-03, -1.01104124e-04, -1.85657220e-04,
        -4.19010408e-03, -1.80702587e-03, -8.99162260e-04,
        -4.88056848e-03,  1.07254149e-04,  3.10542155e-03,
         1.85464555e-03, -2.80052843e-03, -4.05537103e-05,
        -4.01829882e-03],
       [-4.20308555e-04,  6.11270685e-03,  6.36029011e-03,
         1.09143544e-03,  4.63113771e-04,  1.79410190e-03,
        -2.20696861e-03,  9.79296965e-05,  2.28679855e-03,
         3.16583435e-03,  4.01679147e-03, -3.55056691e-05,
         4.82376898e-03, -1.11811663e-04, -2.42725480e-04,
        -4.52568848e-03, -2.18047597e-03, -1.07801333e-03,
        -5.41289896e-03,  1.24731290e-04,  3.29667772e-03,
         2.20799982e-03, -3.12142214e-03, -9.49256500e-05,
        -4.40576999e-03],
       [ 3.96755786e-04, -5.73290931e-03, -5.91572933e-03,
        -9.72483365e-04, -4.52033593e-04, -1.70307537e-03,
         2.11699679e-03, -1.11484063e-04, -2.19596247e-03,
        -3.03842453e-03, -3.76442308e-03,  4.01757898e-05,
        -4.48189117e-03,  1.05281884e-04,  2.08126730e-04,
         4.32440452e-03,  1.95425702e-03,  9.70260764e-04,
         5.09339618e-03, -1.14197246e-04, -3.18210525e-03,
        -1.99505244e-03,  2.92830192e-03,  6.18415579e-05,
         4.17405926e-03],
       [-4.67138998e-05, -1.31245222e-04,  7.42975040e-04,
         4.00750170e-04, -3.15482292e-04, -3.69137735e-04,
         6.61933387e-04, -1.68353246e-04, -7.03911413e-04,
        -8.27916199e-04, -4.51759755e-04,  4.60403971e-05,
         4.84192773e-04,  4.71550520e-05, -1.54097623e-04,
         1.11281162e-03, -4.28792788e-04, -3.93872324e-04,
         3.40192229e-04,  2.32821330e-05, -1.16582331e-03,
         6.92721456e-04,  3.62080493e-04, -2.27944998e-04,
         2.76325212e-04],
       [ 2.21337163e-04, -3.30043887e-03, -3.04296752e-03,
        -2.91597331e-04, -3.88966146e-04, -1.12145487e-03,
         1.51261734e-03, -1.68152474e-04, -1.57811260e-03,
        -2.15797964e-03, -2.24529253e-03,  5.84920708e-05,
        -2.30562361e-03,  7.25581485e-05,  2.00135528e-05,
         2.98933475e-03,  7.14560621e-04,  3.25212342e-04,
         3.03259119e-03, -5.07152326e-05, -2.38932529e-03,
        -7.23794918e-04,  1.74584903e-03, -1.05294399e-04,
         2.58338451e-03],
       [-3.75629752e-04,  5.40210120e-03,  5.52778039e-03,
         8.70156568e-04,  4.42678633e-04,  1.62383763e-03,
        -2.03822460e-03,  1.22827958e-04,  2.11617956e-03,
         2.92679807e-03,  3.54679651e-03, -4.41468437e-05,
         4.18410311e-03, -9.98019677e-05, -1.78622387e-04,
        -4.14877431e-03, -1.76094659e-03, -8.76962906e-04,
        -4.81463317e-03,  1.05094048e-04,  3.08203651e-03,
         1.81083358e-03, -2.76075955e-03, -3.38582904e-05,
        -3.97028308e-03],
       [-2.59430497e-04,  2.36557703e-03,  3.71275330e-03,
         7.86725665e-04, -2.35081461e-04,  2.20674163e-04,
        -7.49350729e-05, -1.07097156e-04,  3.21390835e-05,
         4.26625425e-04,  9.27176210e-04, -3.99183000e-06,
         2.60534091e-03,  3.24031498e-05, -2.42134149e-04,
        -5.55960461e-04, -1.02685252e-03, -8.34420382e-04,
        -1.75382802e-03,  7.57997550e-05,  6.74086696e-05,
         1.59256742e-03, -5.67154610e-04, -2.61618523e-04,
        -1.68947352e-03],
       [ 4.12259251e-04, -5.98105229e-03, -6.20631082e-03,
        -1.04990485e-03, -4.59227798e-04, -1.76254241e-03,
         2.17588432e-03, -1.02736929e-04, -2.25545885e-03,
        -3.12186521e-03, -3.92888859e-03,  3.71562637e-05,
        -4.70523024e-03,  1.09508030e-04,  2.30596488e-04,
         4.45604324e-03,  2.10125186e-03,  1.04051281e-03,
         5.30222803e-03, -1.21064077e-04, -3.25711095e-03,
        -2.13386770e-03,  3.05430498e-03,  8.32778751e-05,
         4.32586763e-03],
       [ 2.91971723e-04, -2.78525590e-03, -4.20035608e-03,
        -8.36206600e-04,  2.13108899e-04, -3.21327709e-04,
         2.11501974e-04,  9.06846108e-05, -1.64019060e-04,
        -6.67047221e-04, -1.17624039e-03,  1.71311585e-05,
        -2.94660265e-03, -2.89475793e-05,  2.52552098e-04,
         8.70567281e-04,  1.11238554e-03,  8.92502139e-04,
         2.10242206e-03, -8.42246372e-05, -3.20878054e-04,
        -1.72117411e-03,  7.06635066e-04,  2.61980633e-04,
         2.02839542e-03],
       [-4.28805826e-04,  6.25554472e-03,  6.52715517e-03,
         1.13735162e-03,  4.67372185e-04,  1.82833802e-03,
        -2.24035163e-03,  9.23933258e-05,  2.32038787e-03,
         3.21279652e-03,  4.11306042e-03, -3.35889563e-05,
         4.95257508e-03, -1.14397204e-04, -2.56237749e-04,
        -4.60066041e-03, -2.26855371e-03, -1.11920410e-03,
        -5.53282024e-03,  1.28751781e-04,  3.33885895e-03,
         2.28942931e-03, -3.19480198e-03, -1.08032611e-04,
        -4.49137297e-03],
       [ 2.84877460e-04, -2.69292737e-03, -4.09327261e-03,
        -8.25504190e-04,  2.18090310e-04, -2.99176085e-04,
         1.81268886e-04,  9.43681589e-05, -1.34913615e-04,
        -6.13715267e-04, -1.12105568e-03,  1.41702321e-05,
        -2.87175481e-03, -2.97299048e-05,  2.50308512e-04,
         8.00821930e-04,  1.09361659e-03,  8.79945233e-04,
         2.02579773e-03, -8.23732771e-05, -2.64375412e-04,
        -1.69314630e-03,  6.76227210e-04,  2.61951587e-04,
         1.95377320e-03],
       [-3.59202735e-04,  5.16009796e-03,  5.24310349e-03,
         7.98898051e-04,  4.36047208e-04,  1.56588573e-03,
        -1.97924674e-03,  1.29778811e-04,  2.05617864e-03,
         2.84203514e-03,  3.39143863e-03, -4.65020930e-05,
         3.96703370e-03, -9.61567275e-05, -1.58490599e-04,
        -4.01790719e-03, -1.62850716e-03, -8.10818630e-04,
        -4.61014407e-03,  9.86077503e-05,  3.00574722e-03,
         1.68028625e-03, -2.64064828e-03, -1.53280198e-05,
        -3.81664000e-03],
       [-3.18807608e-04,  3.13917641e-03,  4.61112801e-03,
         8.75402999e-04, -1.93552172e-04,  4.05535946e-04,
        -3.27896647e-04, -7.58924944e-05,  2.75012280e-04,
         8.74725869e-04,  1.38927216e-03, -2.92532677e-05,
         3.23262042e-03,  2.59549670e-05, -2.60715897e-04,
        -1.14095584e-03, -1.18231692e-03, -9.38853365e-04,
        -2.39481498e-03,  9.13090043e-05,  5.42508205e-04,
         1.82639016e-03, -8.19099718e-04, -2.61726003e-04,
        -2.31570704e-03],
       [-3.92600370e-04,  5.66111039e-03,  5.83192054e-03,
         9.48657689e-04,  4.49909130e-04,  1.68586627e-03,
        -2.10050074e-03,  1.14551774e-04,  2.17936886e-03,
         3.01559828e-03,  3.71547556e-03, -4.12881600e-05,
         4.41690627e-03, -1.03930266e-04, -2.01074246e-04,
        -4.28736024e-03, -1.90827786e-03, -9.49059962e-04,
        -5.03313076e-03,  1.12144087e-04,  3.16178519e-03,
         1.95317017e-03, -2.89081852e-03, -5.49014439e-05,
        -4.13187593e-03],
       [ 2.91845092e-04, -2.77920044e-03, -4.19531204e-03,
        -8.36220162e-04,  2.14565836e-04, -3.19292449e-04,
         2.07947887e-04,  9.12537726e-05, -1.60682510e-04,
        -6.61314290e-04, -1.17002986e-03,  1.67542821e-05,
        -2.94337468e-03, -2.92060467e-05,  2.52641097e-04,
         8.62734858e-04,  1.11073663e-03,  8.92719429e-04,
         2.09707394e-03, -8.41244037e-05, -3.13241035e-04,
        -1.72065361e-03,  7.04614446e-04,  2.62325571e-04,
         2.02353089e-03],
       [ 4.03377431e-04, -5.83391404e-03, -6.03427459e-03,
        -1.00279029e-03, -4.54905792e-04, -1.72727904e-03,
         2.14142818e-03, -1.08376480e-04, -2.22072820e-03,
        -3.07345670e-03, -3.83013254e-03,  3.91357862e-05,
        -4.57251817e-03,  1.06884734e-04,  2.16790475e-04,
         4.37882449e-03,  2.01111729e-03,  9.98136587e-04,
         5.17857354e-03, -1.16934811e-04, -3.21372319e-03,
        -2.05012970e-03,  2.97875702e-03,  6.99228840e-05,
         4.23736311e-03],
       [ 4.25131439e-04, -6.19061291e-03, -6.45146240e-03,
        -1.11577392e-03, -4.65400517e-04, -1.81277457e-03,
         2.22544791e-03, -9.51741094e-05, -2.30544503e-03,
        -3.19206296e-03, -4.06856276e-03,  3.45674925e-05,
        -4.89386031e-03,  1.13151669e-04,  2.49809644e-04,
         4.56706109e-03,  2.22676969e-03,  1.10006309e-03,
         5.47841564e-03, -1.26890212e-04, -3.32030398e-03,
        -2.25158618e-03,  3.16096679e-03,  1.01693622e-04,
         4.45334893e-03],
       [ 4.36854840e-04, -6.39336556e-03, -6.68798760e-03,
        -1.18199422e-03, -4.71548177e-04, -1.86138274e-03,
         2.27245013e-03, -8.69284268e-05, -2.35262397e-03,
        -3.25791631e-03, -4.20648325e-03,  3.17102604e-05,
        -5.07687125e-03,  1.16943411e-04,  2.69439013e-04,
         4.67288727e-03,  2.35449499e-03,  1.15910545e-03,
         5.64841740e-03, -1.32649686e-04, -3.37944529e-03,
        -2.36834143e-03,  3.26579204e-03,  1.20894241e-04,
         4.57347091e-03],
       [ 4.31588909e-04, -6.30387897e-03, -6.58353232e-03,
        -1.15318713e-03, -4.68838960e-04, -1.83992542e-03,
         2.25153705e-03, -9.04090848e-05, -2.33161286e-03,
        -3.22844973e-03, -4.14598640e-03,  3.28997921e-05,
        -4.99621592e-03,  1.15305578e-04,  2.60936446e-04,
         4.62585129e-03,  2.29912065e-03,  1.13330944e-03,
         5.57333883e-03, -1.30127097e-04, -3.35290981e-03,
        -2.31732195e-03,  3.21982661e-03,  1.12635666e-04,
         4.51994920e-03],
       [-2.41223403e-04,  2.14476418e-03,  3.45114339e-03,
         7.57435977e-04, -2.43479502e-04,  1.68880739e-04,
        -7.52983306e-06, -1.14369526e-04, -3.37791353e-05,
         3.08077462e-04,  8.03340983e-04,  1.94488666e-06,
         2.42078234e-03,  3.37113197e-05, -2.35683910e-04,
        -4.00824443e-04, -9.80958459e-04, -7.99888163e-04,
        -1.57042686e-03,  7.12850015e-05, -5.26472577e-05,
         1.51966990e-03, -4.91389423e-04, -2.60179950e-04,
        -1.51208579e-03],
       [-4.24904836e-04,  6.18723501e-03,  6.44751545e-03,
         1.11485273e-03,  4.65289515e-04,  1.81195908e-03,
        -2.22458388e-03,  9.52421688e-05,  2.30457354e-03,
         3.19077494e-03,  4.06639371e-03, -3.45819462e-05,
         4.89087775e-03, -1.13100927e-04, -2.49549979e-04,
        -4.56513045e-03, -2.22505257e-03, -1.09920080e-03,
        -5.47556952e-03,  1.26802450e-04,  3.31909885e-03,
         2.24987045e-03, -3.15935095e-03, -1.01464189e-04,
        -4.45114169e-03],
       [ 3.87346256e-04, -5.58422133e-03, -5.74143557e-03,
        -9.26204724e-04, -4.47807193e-04, -1.66745973e-03,
         2.08169990e-03, -1.16689182e-04, -2.16023973e-03,
        -2.98848469e-03, -3.66627960e-03,  4.20003125e-05,
        -4.34798747e-03,  1.02787752e-04,  1.94745415e-04,
         4.24564024e-03,  1.86664378e-03,  9.28164169e-04,
         4.96815657e-03, -1.10091467e-04, -3.13734962e-03,
        -1.91192469e-03,  2.85281241e-03,  4.91040482e-05,
         4.08283900e-03],
       [ 4.05050581e-04, -5.86487679e-03, -6.07031165e-03,
        -1.01349317e-03, -4.55847243e-04, -1.73470064e-03,
         2.14837771e-03, -1.06893349e-04, -2.22768332e-03,
        -3.08293384e-03, -3.85170151e-03,  3.85925086e-05,
        -4.60063480e-03,  1.07511471e-04,  2.20007103e-04,
         4.39451542e-03,  2.03201454e-03,  1.00752886e-03,
         5.20449039e-03, -1.17840973e-04, -3.22212279e-03,
        -2.06868956e-03,  2.99520791e-03,  7.31494220e-05,
         4.25500795e-03],
       [-2.56785715e-04,  3.75283137e-03,  3.58016836e-03,
         4.07329440e-04,  3.99899320e-04,  1.22941763e-03,
        -1.62871717e-03,  1.61703734e-04,  1.69748673e-03,
         2.33111018e-03,  2.51589250e-03, -5.68600735e-05,
         2.70823762e-03, -7.75413646e-05, -5.07465047e-05,
        -3.24446941e-03, -9.17680794e-04, -4.38805961e-04,
        -3.41717293e-03,  6.20065985e-05,  2.54591228e-03,
         9.47302964e-04, -1.95785239e-03,  7.99241971e-05,
        -2.89305602e-03],
       [-4.03795508e-04,  5.84415719e-03,  6.04606420e-03,
         1.00682490e-03,  4.55251313e-04,  1.72974041e-03,
        -2.14354787e-03,  1.07701075e-04,  2.22281087e-03,
         3.07617197e-03,  3.83780850e-03, -3.88804765e-05,
         4.58191987e-03, -1.07143227e-04, -2.18054076e-04,
        -4.38369997e-03, -2.01927079e-03, -1.00153091e-03,
        -5.18707372e-03,  1.17258329e-04,  3.21609038e-03,
         2.05684640e-03, -2.98454240e-03, -7.12564797e-05,
        -4.24256921e-03],
       [ 4.23576799e-04, -6.16667140e-03, -6.42339559e-03,
        -1.10862311e-03, -4.64700279e-04, -1.80703227e-03,
         2.21963529e-03, -9.58967794e-05, -2.29956442e-03,
        -3.18368827e-03, -4.05294448e-03,  3.48004396e-05,
        -4.87241335e-03,  1.12767812e-04,  2.47759570e-04,
         4.55408636e-03,  2.21332395e-03,  1.09349261e-03,
         5.45824412e-03, -1.26241997e-04, -3.31270392e-03,
        -2.23859097e-03,  3.14904610e-03,  9.97836105e-05,
         4.43833182e-03],
       [ 4.31127497e-04, -6.29426911e-03, -6.57239929e-03,
        -1.14968664e-03, -4.68531623e-04, -1.83762214e-03,
         2.24945019e-03, -9.09355222e-05, -2.32954370e-03,
        -3.22564831e-03, -4.13907971e-03,  3.30909170e-05,
        -4.98745497e-03,  1.15090603e-04,  2.59860361e-04,
         4.62108850e-03,  2.29217694e-03,  1.13029545e-03,
         5.56533644e-03, -1.29836786e-04, -3.35043273e-03,
        -2.31136195e-03,  3.21461167e-03,  1.11531248e-04,
         4.51470818e-03],
       [ 3.36714351e-04, -3.39159742e-03, -4.89872089e-03,
        -9.00478917e-04,  1.76013098e-04, -4.66681988e-04,
         4.15637332e-04,  6.41053193e-05, -3.57679499e-04,
        -1.03114988e-03, -1.55016955e-03,  3.88935987e-05,
        -3.43135907e-03, -2.32013863e-05,  2.65696784e-04,
         1.34497974e-03,  1.23306911e-03,  9.68000095e-04,
         2.60339351e-03, -9.63187995e-05, -7.15091184e-04,
        -1.89680106e-03,  8.96596466e-04,  2.60491506e-04,
         2.52102711e-03]], dtype=float32)>, <tf.Tensor: shape=(25,), dtype=float32, numpy=
array([-4.4357838e-04,  6.5140324e-03,  6.8285028e-03,  1.2221392e-03,
        4.7528732e-04,  1.8903175e-03, -2.3001477e-03,  8.1750026e-05,
        2.3803369e-03,  3.2965629e-03,  4.2894958e-03, -2.9921332e-05,
        5.1859021e-03, -1.1928959e-04, -2.8144638e-04, -4.7354721e-03,
       -2.4324311e-03, -1.1946385e-03, -5.7494119e-03,  1.3611509e-04,
        3.4141643e-03,  2.4386353e-03, -3.3286239e-03, -1.3275101e-04,
       -4.6439674e-03], dtype=float32)>, <tf.Tensor: shape=(25, 12), dtype=float32, numpy=
array([[ 5.00208419e-03, -3.89122427e-03,  1.46507821e-03,
         1.47270691e-03,  6.24650111e-03, -3.70732299e-03,
        -6.45059673e-03,  4.80029546e-03,  4.43131058e-03,
        -2.15951772e-03,  4.51840833e-03, -7.42064731e-04],
       [ 1.32915028e-03, -1.10177661e-03, -1.37832933e-04,
         4.58272552e-05,  1.37549103e-03, -9.39215359e-04,
        -1.40471966e-03,  1.50816946e-03,  1.20826473e-03,
        -3.00531654e-04,  1.14456611e-03, -6.55898417e-04],
       [-6.14240242e-04,  6.27177360e-04,  4.30449640e-04,
         4.42736346e-04,  6.33190721e-05,  2.42795053e-04,
         3.74688098e-04, -9.47625143e-04, -6.90707064e-04,
         1.77043748e-05, -4.81562514e-04,  7.49965373e-04],
       [-4.79915272e-03,  3.74715705e-03, -1.34305679e-03,
        -1.39126030e-03, -6.04296196e-03,  3.61852464e-03,
         6.18030177e-03, -4.64318600e-03, -4.23493423e-03,
         2.04845099e-03, -4.32440778e-03,  7.63919612e-04],
       [-4.86523006e-03,  3.78853222e-03, -1.21066649e-03,
        -1.28146703e-03, -5.84705034e-03,  3.46028479e-03,
         6.09474815e-03, -4.72603552e-03, -4.34910227e-03,
         1.96113694e-03, -4.38108202e-03,  8.96088895e-04],
       [-3.70427594e-03,  2.90216808e-03, -9.28421039e-04,
        -9.67555039e-04, -4.47576074e-03,  2.69070501e-03,
         4.66844020e-03, -3.62250698e-03, -3.30519467e-03,
         1.51774380e-03, -3.33055155e-03,  6.93621580e-04],
       [ 4.14474122e-03, -3.20011657e-03,  1.30794384e-03,
         1.19050802e-03,  4.88715572e-03, -2.83445651e-03,
        -5.29980706e-03,  3.90763488e-03,  3.74070555e-03,
        -1.82160037e-03,  3.76338745e-03, -5.59316715e-04],
       [ 3.96468118e-03, -3.14360601e-03,  8.05596181e-04,
         9.67486296e-04,  4.91512008e-03, -3.03913932e-03,
        -4.95921075e-03,  3.98534397e-03,  3.49652115e-03,
        -1.54592539e-03,  3.53589165e-03, -8.95164383e-04],
       [-1.19439489e-03,  9.74469236e-04, -1.10208064e-04,
        -2.14649554e-04, -1.47829158e-03,  9.72116482e-04,
         1.44590391e-03, -1.27441203e-03, -1.04228640e-03,
         4.12507565e-04, -1.04544614e-03,  3.89569206e-04],
       [ 2.28515686e-03, -1.85085973e-03,  1.70806394e-04,
         3.16103804e-04,  2.51693418e-03, -1.62755756e-03,
        -2.64622970e-03,  2.42381031e-03,  2.06778920e-03,
        -7.37699505e-04,  2.00986583e-03, -7.89953279e-04],
       [ 5.45635587e-04, -4.72479907e-04, -1.32338764e-05,
         7.48429302e-05,  7.67315971e-04, -5.63682290e-04,
        -6.76778436e-04,  6.39664766e-04,  4.47460741e-04,
        -1.81955169e-04,  4.61184245e-04, -2.41006899e-04],
       [-4.91590751e-03,  3.82311293e-03, -1.56088709e-03,
        -1.46285258e-03, -6.04429189e-03,  3.58408433e-03,
         6.38519786e-03, -4.67787776e-03, -4.37865034e-03,
         2.20300723e-03, -4.45093354e-03,  6.58582139e-04],
       [ 5.45590592e-04, -4.42643824e-04,  4.67622231e-05,
         4.05548235e-05,  4.80333401e-04, -3.13445809e-04,
        -6.03505759e-04,  5.76522783e-04,  5.24166215e-04,
        -1.75965892e-04,  4.84049524e-04, -1.97201647e-04],
       [-5.03549119e-03,  3.91862867e-03, -1.35088968e-03,
        -1.40245561e-03, -6.17772667e-03,  3.66012380e-03,
         6.39498699e-03, -4.86453716e-03, -4.47798474e-03,
         2.09353701e-03, -4.54005925e-03,  8.45085597e-04],
       [-5.01208752e-03,  3.88704427e-03, -1.66537252e-03,
        -1.52319821e-03, -6.14306005e-03,  3.61913233e-03,
         6.53704582e-03, -4.73249704e-03, -4.47239960e-03,
         2.28293519e-03, -4.54720482e-03,  6.12572127e-04],
       [-2.06359988e-03,  1.62760890e-03, -4.10740380e-04,
        -3.67979344e-04, -2.09372700e-03,  1.26339169e-03,
         2.44603795e-03, -2.05065683e-03, -1.92353688e-03,
         7.82538555e-04, -1.85163901e-03,  5.14396175e-04],
       [-1.30420341e-03,  1.04174227e-03, -4.32358618e-04,
        -2.41964284e-04, -1.19338348e-03,  7.52501306e-04,
         1.62444392e-03, -1.26485305e-03, -1.25455623e-03,
         6.19512051e-04, -1.18301180e-03,  2.36640568e-04],
       [ 4.39985469e-03, -3.40541406e-03,  1.42214389e-03,
         1.34791678e-03,  5.45084337e-03, -3.19798756e-03,
        -5.72186941e-03,  4.15988360e-03,  3.91586451e-03,
        -1.96675188e-03,  3.99103435e-03, -5.49235789e-04],
       [ 8.25649942e-04, -6.66732376e-04,  1.98276204e-04,
         1.01524405e-04,  6.99582393e-04, -4.53990622e-04,
        -9.78230499e-04,  8.29290831e-04,  8.00983049e-04,
        -3.51223280e-04,  7.41618685e-04, -2.14857864e-04],
       [ 5.31352218e-03, -4.14246367e-03,  1.47681078e-03,
         1.52916927e-03,  6.64360356e-03, -3.96118080e-03,
        -6.81743398e-03,  5.13424724e-03,  4.70105652e-03,
        -2.25159083e-03,  4.79081366e-03, -8.50213692e-04],
       [-1.88196334e-03,  1.52366189e-03, -3.28601978e-04,
        -3.48984206e-04, -2.14046892e-03,  1.38959894e-03,
         2.30519823e-03, -1.94526324e-03, -1.69664226e-03,
         7.32489163e-04, -1.66816299e-03,  5.16557368e-04],
       [ 4.30803280e-03, -3.32438573e-03,  1.47083669e-03,
         1.35451928e-03,  5.32515068e-03, -3.10348882e-03,
        -5.63452113e-03,  4.03689221e-03,  3.84136499e-03,
        -1.96595653e-03,  3.91703891e-03, -4.75842389e-04],
       [-2.75099277e-03,  2.14143749e-03, -7.12994777e-04,
        -6.29878952e-04, -2.95880181e-03,  1.73242239e-03,
         3.37173976e-03, -2.65203556e-03, -2.53614178e-03,
         1.12351927e-03, -2.48598540e-03,  5.26148826e-04],
       [ 4.36223997e-03, -3.40046268e-03,  1.23329146e-03,
         1.22731517e-03,  5.33606112e-03, -3.17607494e-03,
        -5.57785621e-03,  4.20519244e-03,  3.88662005e-03,
        -1.86033803e-03,  3.93715734e-03, -6.96809730e-04],
       [ 6.83707302e-04, -4.63997363e-04,  6.57844706e-04,
         4.25694307e-04,  8.21610796e-04, -3.46381974e-04,
        -1.06806739e-03,  4.33072652e-04,  6.41867984e-04,
        -5.17952256e-04,  6.74753799e-04,  2.75477476e-04]], dtype=float32)>, <tf.Tensor: shape=(12,), dtype=float32, numpy=
array([-0.00537656,  0.00419328, -0.00146794, -0.00153926, -0.00673258,
        0.0040177 ,  0.00688714, -0.00520546, -0.00475372,  0.00226236,
       -0.0048451 ,  0.00087849], dtype=float32)>, <tf.Tensor: shape=(30, 25), dtype=float32, numpy=
array([[ 1.2003895e-03,  2.1781442e-03, -3.8928504e-03,  3.6820573e-05,
        -6.8987696e-04, -3.3176581e-03, -3.2692035e-03, -3.5279109e-03,
         1.3140142e-03, -3.2005246e-05, -2.5186557e-03,  3.9667578e-04,
         1.2738138e-03,  2.0417790e-03, -1.8623486e-03,  1.4766651e-04,
         4.3568416e-03,  2.0749823e-04, -1.7470865e-04, -2.7244337e-04,
         3.9505712e-03,  1.7626053e-03, -2.8154273e-05,  2.7095089e-03,
        -2.0630988e-03],
       [-2.5793370e-03, -4.7517940e-03,  7.3350528e-03, -2.3347696e-03,
         1.3374654e-03,  6.7573790e-03,  7.1243057e-03,  8.5678166e-03,
        -5.4100081e-03, -5.3837510e-05,  5.9173601e-03, -2.0649405e-03,
        -1.8223168e-03, -3.6599827e-03,  4.8340308e-03, -6.8089046e-04,
        -6.6445735e-03,  1.0767868e-03, -5.2156392e-04,  9.5227227e-04,
        -8.4068216e-03, -2.0083617e-03, -4.1851931e-04, -5.2114609e-03,
         5.1413630e-03],
       [ 2.3023591e-03,  4.1003493e-03, -6.3131424e-03,  2.1363697e-03,
        -1.1064559e-03, -5.9897765e-03, -6.0481909e-03, -7.3246690e-03,
         4.8403898e-03,  5.1923078e-05, -5.2079824e-03,  1.8120070e-03,
         1.4732227e-03,  3.1850692e-03, -4.3000276e-03,  5.7781150e-04,
         5.4425015e-03, -9.8248746e-04,  5.1119615e-04, -8.1850257e-04,
         7.2477413e-03,  1.5446375e-03,  3.8372923e-04,  4.4534132e-03,
        -4.5388741e-03],
       [ 2.5830688e-03,  4.7600479e-03, -7.3480099e-03,  2.3374103e-03,
        -1.3403625e-03, -6.7673936e-03, -7.1378415e-03, -8.5835019e-03,
         5.4174466e-03,  5.3871587e-05, -5.9264605e-03,  2.0681564e-03,
         1.8266046e-03,  3.6660652e-03, -4.8409919e-03,  6.8216183e-04,
         6.6594686e-03, -1.0780293e-03,  5.2178255e-04, -9.5394650e-04,
         8.4214974e-03,  2.0139706e-03,  4.1898453e-04,  5.2210055e-03,
        -5.1491107e-03],
       [-2.4145152e-03, -4.3623457e-03,  6.7250887e-03, -2.2161673e-03,
         1.1981023e-03,  6.3023604e-03,  6.4783115e-03,  7.8214640e-03,
        -5.0700372e-03, -5.2659918e-05,  5.4953918e-03, -1.9130345e-03,
        -1.6126416e-03, -3.3775934e-03,  4.5165485e-03, -6.1871123e-04,
        -5.9223296e-03,  1.0196549e-03, -5.1551231e-04,  8.7187893e-04,
        -7.7141318e-03, -1.7293261e-03, -3.9768117e-04, -4.7582649e-03,
         4.7829822e-03],
       [ 1.7583527e-04,  6.8574550e-04, -1.4675984e-03, -5.7064462e-04,
        -2.7079968e-04, -9.0387592e-04, -1.1078520e-03, -9.6968468e-04,
        -3.0991284e-04, -4.2454871e-05, -6.1911694e-04, -1.3568733e-04,
         7.2386785e-04,  7.6083583e-04, -2.5567203e-04, -5.8166279e-07,
         2.2433996e-03,  4.3180041e-04, -3.2114409e-04, -2.9904368e-05,
         1.3115726e-03,  1.1724182e-03, -1.2623900e-04,  1.0632201e-03,
        -4.3259995e-04],
       [-1.3190177e-03, -1.9885125e-03,  2.9298333e-03, -1.4783827e-03,
         4.4437937e-04,  3.1978209e-03,  2.7531667e-03,  3.5178531e-03,
        -2.9277292e-03, -4.7400950e-05,  2.7515227e-03, -1.0406952e-03,
        -4.2450006e-04, -1.5305767e-03,  2.4227886e-03, -2.8737425e-04,
        -1.8257222e-03,  7.2236668e-04, -4.5720200e-04,  4.1756692e-04,
        -3.4751636e-03, -1.9656794e-04, -2.7184654e-04, -2.0026853e-03,
         2.4490221e-03],
       [ 2.2677095e-03,  4.0187230e-03, -6.1847689e-03,  2.1116864e-03,
        -1.0779239e-03, -5.8926228e-03, -5.9141722e-03, -7.1699508e-03,
         4.7691297e-03,  5.1707997e-05, -5.1184944e-03,  1.7805761e-03,
         1.4296379e-03,  3.1251113e-03, -4.2327698e-03,  5.6504388e-04,
         5.2927127e-03, -9.7101304e-04,  5.0996884e-04, -8.0186111e-04,
         7.1024047e-03,  1.4868013e-03,  3.7941674e-04,  4.3583503e-03,
        -4.4628941e-03],
       [ 1.4277565e-03,  2.4815011e-03, -4.3682577e-03,  1.7644996e-04,
        -7.8872190e-04, -3.7880377e-03, -3.7372075e-03, -4.0879259e-03,
         1.6630448e-03, -2.7575627e-05, -2.8885629e-03,  5.1815098e-04,
         1.3808277e-03,  2.2854162e-03, -2.1875855e-03,  1.8229072e-04,
         4.7895354e-03,  1.4598607e-04, -1.3457550e-04, -3.2578604e-04,
         4.4803163e-03,  1.8722396e-03, -3.8721746e-06,  3.0330238e-03,
        -2.3803529e-03],
       [-2.5246637e-03, -4.6215542e-03,  7.1314611e-03, -2.2951867e-03,
         1.2903280e-03,  6.6069625e-03,  6.9071250e-03,  8.3168913e-03,
        -5.2966764e-03, -5.3436248e-05,  5.7771723e-03, -2.0138640e-03,
        -1.7517154e-03, -3.5662060e-03,  4.7286917e-03, -6.5984955e-04,
        -6.4013628e-03,  1.0574020e-03, -5.1965320e-04,  9.2520280e-04,
        -8.1752613e-03, -1.9141455e-03, -4.1154920e-04, -5.0598574e-03,
         5.0223134e-03],
       [-1.6436374e-03, -2.7626348e-03,  4.7939885e-03, -3.1632007e-04,
         8.9035538e-04,  4.2000273e-03,  4.1959053e-03,  4.6392088e-03,
        -1.9962324e-03,  2.1271331e-05,  3.2173889e-03, -6.4235460e-04,
        -1.4804222e-03, -2.4962383e-03,  2.4822121e-03, -2.1911383e-04,
        -5.1961457e-03, -7.7805307e-05,  9.1155700e-05,  3.7973575e-04,
        -4.9684751e-03, -1.9722478e-03, -2.2772536e-05, -3.3268132e-03,
         2.6623956e-03],
       [ 2.6462846e-03,  4.9120956e-03, -7.5850831e-03,  2.3835476e-03,
        -1.3962048e-03, -6.9403886e-03, -7.3931268e-03, -8.8784806e-03,
         5.5492795e-03,  5.4355645e-05, -6.0887281e-03,  2.1282206e-03,
         1.9097247e-03,  3.7745484e-03, -4.9627507e-03,  7.0709595e-04,
         6.9458764e-03, -1.1011115e-03,  5.2387145e-04, -9.8582741e-04,
         8.6916974e-03,  2.1252586e-03,  4.2713445e-04,  5.3980462e-03,
        -5.2868994e-03],
       [-1.5960735e-03, -2.7009177e-03,  4.7011017e-03, -2.8530692e-04,
         8.6761639e-04,  4.1106166e-03,  4.0941555e-03,  4.5168744e-03,
        -1.9228453e-03,  2.2731456e-05,  3.1458060e-03, -6.1466394e-04,
        -1.4584766e-03, -2.4505598e-03,  2.4178827e-03, -2.1084090e-04,
        -5.1064533e-03, -9.3154813e-05,  1.0088776e-04,  3.6772114e-04,
        -4.8614191e-03, -1.9502151e-03, -1.6786460e-05, -3.2625303e-03,
         2.6010252e-03],
       [ 2.1595559e-03,  3.7749407e-03, -5.7985419e-03,  2.0366453e-03,
        -9.9638291e-04, -5.5886810e-03, -5.5222334e-03, -6.7171911e-03,
         4.5524142e-03,  5.1072009e-05, -4.8443079e-03,  1.6885727e-03,
         1.3035811e-03,  2.9410913e-03, -4.0249405e-03,  5.2884984e-04,
         4.8585702e-03, -9.3829806e-04,  5.0498161e-04, -7.5358048e-04,
         6.6678282e-03,  1.3218364e-03,  3.6642689e-04,  4.0750806e-03,
        -4.2298650e-03],
       [ 1.8275678e-03,  2.9969702e-03, -5.1424298e-03,  4.3600777e-04,
        -9.8058023e-04, -4.5323037e-03, -4.5908554e-03, -5.1144157e-03,
         2.2766283e-03, -1.5263486e-05, -3.4841301e-03,  7.5031759e-04,
         1.5641148e-03,  2.6651702e-03, -2.7241658e-03,  2.5176886e-04,
         5.5412380e-03,  1.6776190e-05, -5.2571831e-05, -4.2636786e-04,
         5.3736353e-03,  2.0563402e-03,  4.6244499e-05,  3.5688835e-03,
        -2.8905475e-03],
       [ 2.3830060e-03,  4.2843525e-03, -6.6037234e-03,  2.1928763e-03,
        -1.1691974e-03, -6.2152240e-03, -6.3466243e-03, -7.6694437e-03,
         5.0031990e-03,  5.2427466e-05, -5.4128356e-03,  1.8821070e-03,
         1.5694245e-03,  3.3224437e-03, -4.4550262e-03,  6.0569932e-04,
         5.7738852e-03, -1.0078388e-03,  5.1473262e-04, -8.5537089e-04,
         7.5755371e-03,  1.6711219e-03,  3.9355151e-04,  4.6672840e-03,
        -4.7129467e-03],
       [-1.6408474e-03, -2.7587404e-03,  4.7906060e-03, -3.1251676e-04,
         8.8760932e-04,  4.1985810e-03,  4.1856351e-03,  4.6265712e-03,
        -1.9900389e-03,  2.1808526e-05,  3.2149237e-03, -6.3849648e-04,
        -1.4787789e-03, -2.4958102e-03,  2.4794873e-03, -2.1773396e-04,
        -5.1902877e-03, -8.0609665e-05,  9.2729912e-05,  3.7808728e-04,
        -4.9618306e-03, -1.9710134e-03, -2.1524664e-05, -3.3234772e-03,
         2.6601024e-03],
       [-2.4596136e-03, -4.4653183e-03,  6.8874126e-03, -2.2479033e-03,
         1.2335826e-03,  6.4275758e-03,  6.6460734e-03,  8.0153476e-03,
        -5.1611732e-03, -5.2961543e-05,  5.6094700e-03, -1.9524938e-03,
        -1.6666624e-03, -3.4540524e-03,  4.6029156e-03, -6.3446164e-04,
        -6.1086128e-03,  1.0341236e-03, -5.1752501e-04,  8.9262566e-04,
        -7.8974925e-03, -1.8004354e-03, -4.0321244e-04, -4.8779016e-03,
         4.8799338e-03],
       [-2.6176898e-03, -4.8414990e-03,  7.4755363e-03, -2.3623114e-03,
         1.3695769e-03,  6.8625496e-03,  7.2730463e-03,  8.7397583e-03,
        -5.4886895e-03, -5.4125863e-05,  6.0146591e-03, -2.0999734e-03,
        -1.8704135e-03, -3.7250873e-03,  4.9074413e-03, -6.9517002e-04,
        -6.8105161e-03,  1.0900660e-03, -5.2310736e-04,  9.7076694e-04,
        -8.5663460e-03, -2.0722374e-03, -4.2336079e-04, -5.3157476e-03,
         5.2240347e-03],
       [-2.7072746e-03, -5.0595282e-03,  7.8144707e-03, -2.4283321e-03,
         1.4510120e-03,  7.1062120e-03,  7.6420368e-03,  9.1661587e-03,
        -5.6769089e-03, -5.4845641e-05,  6.2450022e-03, -2.1868292e-03,
        -1.9907886e-03, -3.8789655e-03,  5.0799698e-03, -7.3155016e-04,
        -7.2253812e-03,  1.1239129e-03, -5.2585400e-04,  1.0169538e-03,
        -8.9535769e-03, -2.2340249e-03, -4.3506696e-04, -5.5697085e-03,
         5.4196054e-03],
       [-2.6676294e-03, -4.9641747e-03,  7.6660104e-03, -2.3993014e-03,
         1.4156967e-03,  6.9984384e-03,  7.4813548e-03,  8.9804335e-03,
        -5.5941935e-03, -5.4527416e-05,  6.1436715e-03, -2.1489863e-03,
        -1.9385240e-03, -3.8112490e-03,  5.0038835e-03, -7.1580708e-04,
        -7.0451195e-03,  1.1092122e-03, -5.2451179e-04,  9.9687511e-04,
        -8.7841880e-03, -2.1639913e-03, -4.2992880e-04, -5.4587154e-03,
         5.3335465e-03],
       [ 1.3143390e-03,  2.3308701e-03, -4.1312585e-03,  1.0797409e-04,
        -7.3992932e-04, -3.5529144e-03, -3.5060155e-03, -3.8114171e-03,
         1.4904537e-03, -2.9572522e-05, -2.7043200e-03,  4.5872692e-04,
         1.3277846e-03,  2.1635550e-03, -2.0256059e-03,  1.6544765e-04,
         4.5742090e-03,  1.7578714e-04, -1.5413242e-04, -2.9969428e-04,
         4.2173704e-03,  1.8176378e-03, -1.5510217e-05,  2.8722030e-03,
        -2.2225038e-03],
       [ 2.6161496e-03,  4.8382385e-03, -7.4704103e-03,  2.3612345e-03,
        -1.3684501e-03, -6.8584858e-03, -7.2677466e-03, -8.7336041e-03,
         5.4856897e-03,  5.4110060e-05, -6.0110199e-03,  2.0987056e-03,
         1.8687643e-03,  3.7226556e-03, -4.9046250e-03,  6.9468166e-04,
         6.8047494e-03, -1.0895680e-03,  5.2299653e-04, -9.7011460e-04,
         8.5605513e-03,  2.0701075e-03,  4.2317089e-04,  5.3119934e-03,
        -5.2209310e-03],
       [-2.3485762e-03, -4.2072963e-03,  6.4815464e-03, -2.1690778e-03,
         1.1435281e-03,  6.1186375e-03,  6.2230062e-03,  7.5266119e-03,
        -4.9345046e-03, -5.2225427e-05,  5.3259390e-03, -1.8530837e-03,
        -1.5297482e-03, -3.2641059e-03,  4.3890551e-03, -5.9433107e-04,
        -5.6371288e-03,  9.9752436e-04, -5.1308708e-04,  8.4016292e-04,
        -7.4381670e-03, -1.6192978e-03, -3.8943897e-04, -4.5777867e-03,
         4.6390798e-03],
       [-2.4731194e-03, -4.4998522e-03,  6.9408137e-03, -2.2581168e-03,
         1.2468264e-03,  6.4646117e-03,  6.7053586e-03,  8.0837784e-03,
        -5.1904051e-03, -5.3069034e-05,  5.6452244e-03, -1.9664189e-03,
        -1.6862354e-03, -3.4779003e-03,  4.6294043e-03, -6.4044085e-04,
        -6.1758147e-03,  1.0395743e-03, -5.1775249e-04,  9.0009661e-04,
        -7.9587884e-03, -1.8270280e-03, -4.0503757e-04, -4.9182381e-03,
         4.9102469e-03],
       [ 1.5256624e-03,  2.4065736e-03, -3.6076177e-03,  1.6113734e-03,
        -5.6614768e-04, -3.7862239e-03, -3.3838355e-03, -4.2470628e-03,
         3.3157091e-03,  4.8144691e-05, -3.2555368e-03,  1.1880430e-03,
         6.2190875e-04,  1.8713041e-03, -2.8122100e-03,  3.3970652e-04,
         2.5092494e-03, -7.6950516e-04,  4.7090297e-04, -4.9309677e-04,
         4.2234822e-03,  4.4426497e-04,  2.9401120e-04,  2.4866662e-03,
        -2.8784387e-03],
       [ 2.4639738e-03,  4.4777896e-03, -6.9063171e-03,  2.2514821e-03,
        -1.2388459e-03, -6.4392341e-03, -6.6685556e-03, -8.0412878e-03,
         5.1713213e-03,  5.3006755e-05, -5.6215040e-03,  1.9577821e-03,
         1.6742153e-03,  3.4620287e-03, -4.6116426e-03,  6.3686498e-04,
         6.1344858e-03, -1.0363295e-03,  5.1747524e-04, -8.9550554e-04,
         7.9195509e-03,  1.8109475e-03,  4.0387054e-04,  4.8925243e-03,
        -4.8901150e-03],
       [-2.6069705e-03, -4.8171505e-03,  7.4372292e-03, -2.3547439e-03,
         1.3610955e-03,  6.8331156e-03,  7.2332053e-03,  8.6936792e-03,
        -5.4670735e-03, -5.4047079e-05,  5.9878062e-03, -2.0905808e-03,
        -1.8576281e-03, -3.7071009e-03,  4.8870645e-03, -6.9141807e-04,
        -6.7663332e-03,  1.0865566e-03, -5.2259886e-04,  9.6583745e-04,
        -8.5230162e-03, -2.0554089e-03, -4.2203779e-04, -5.2874875e-03,
         5.2012037e-03],
       [-2.6634571e-03, -4.9530827e-03,  7.6490268e-03, -2.3960434e-03,
         1.4112113e-03,  6.9872718e-03,  7.4618221e-03,  8.9578796e-03,
        -5.5849371e-03, -5.4489188e-05,  6.1325822e-03, -2.1443944e-03,
        -1.9320406e-03, -3.8038681e-03,  4.9957163e-03, -7.1378431e-04,
        -7.0228297e-03,  1.1073346e-03, -5.2447908e-04,  9.9439861e-04,
        -8.7645389e-03, -2.1550837e-03, -4.2934046e-04, -5.4457444e-03,
         5.3241472e-03],
       [-1.9588028e-03, -3.1634159e-03,  5.3797965e-03, -5.2691402e-04,
         1.0511818e-03,  4.7501810e-03,  4.8890365e-03,  5.4737674e-03,
        -2.4803230e-03,  9.3630888e-06,  3.6638533e-03, -8.3501829e-04,
        -1.6253095e-03, -2.7748058e-03,  2.8896425e-03, -2.7839659e-04,
        -5.7910681e-03,  3.3286669e-05,  2.1635851e-05,  4.6285623e-04,
        -5.6605544e-03, -2.1171481e-03, -6.5962959e-05, -3.7378054e-03,
         3.0443373e-03]], dtype=float32)>, <tf.Tensor: shape=(25,), dtype=float32, numpy=
array([ 2.7605144e-03,  5.1908335e-03, -8.0178212e-03,  2.4680274e-03,
       -1.5010976e-03, -7.2496952e-03, -7.8664608e-03, -9.4255498e-03,
        5.7897111e-03,  5.5300239e-05, -6.3819606e-03,  2.2396888e-03,
        2.0641426e-03,  3.9703827e-03, -5.1823631e-03,  7.5391983e-04,
        7.4783284e-03, -1.1448881e-03,  5.2734866e-04, -1.0451174e-03,
        9.1866059e-03,  2.3330548e-03,  4.4213684e-04,  5.7227090e-03,
       -5.5358848e-03], dtype=float32)>, <tf.Tensor: shape=(25, 12), dtype=float32, numpy=
array([[ 3.63217550e-04,  2.92317185e-04,  4.66069294e-04,
        -5.30755846e-04,  1.25111616e-03, -1.11772353e-03,
        -1.02870923e-03,  1.12472684e-03,  5.58007800e-04,
        -8.91738746e-04,  1.26065174e-03, -9.58674471e-04],
       [-5.06343902e-04, -2.69449141e-04, -8.57122184e-04,
         4.60516691e-04, -1.26072904e-03,  9.19278478e-04,
         8.65543785e-04, -8.78191087e-04, -2.77240761e-04,
         7.93968444e-04, -1.05630560e-03,  8.60400090e-04],
       [-7.35848618e-04, -3.90484609e-04, -8.83847300e-04,
         7.17760879e-04, -1.65893487e-03,  1.23704830e-03,
         1.18768914e-03, -1.27440889e-03, -4.75617067e-04,
         1.05611049e-03, -1.49806449e-03,  1.20366365e-03],
       [-3.74453003e-03, -1.37107959e-03, -3.64130829e-03,
         2.91780522e-03, -5.75174671e-03,  3.53302667e-03,
         3.21101677e-03, -3.29001714e-03, -1.03615201e-03,
         2.86858017e-03, -4.17326251e-03,  3.74935707e-03],
       [ 1.02970994e-03,  4.44283069e-04,  1.08530163e-03,
        -9.51796537e-04,  1.95507961e-03, -1.41570880e-03,
        -1.21913338e-03,  1.25127018e-03,  5.67608164e-04,
        -1.06394652e-03,  1.50656002e-03, -1.28948363e-03],
       [ 1.17382733e-03,  5.00769704e-04,  1.24174100e-03,
        -1.10141549e-03,  2.23716348e-03, -1.64392684e-03,
        -1.36591168e-03,  1.37982517e-03,  6.77103933e-04,
        -1.18705351e-03,  1.66726427e-03, -1.44374813e-03],
       [-1.16778538e-03, -4.80880961e-04, -1.41345419e-03,
         9.96592571e-04, -2.18680431e-03,  1.50025485e-03,
         1.31333806e-03, -1.31251512e-03, -4.96767054e-04,
         1.17365201e-03, -1.61744212e-03,  1.40385563e-03],
       [-1.82407408e-03, -7.23074016e-04, -1.89171068e-03,
         1.48148031e-03, -3.06421425e-03,  1.97836477e-03,
         1.83335040e-03, -1.90107687e-03, -6.20495062e-04,
         1.63835892e-03, -2.35637440e-03,  2.04856298e-03],
       [ 2.21039099e-03,  7.69078440e-04,  2.47982517e-03,
        -1.58155605e-03,  3.35233961e-03, -1.92344526e-03,
        -1.76558923e-03,  1.73285091e-03,  3.67834175e-04,
        -1.62800611e-03,  2.28235638e-03, -2.10389122e-03],
       [-5.12317987e-03, -1.99298467e-03, -5.19788405e-03,
         4.14242037e-03, -8.42528604e-03,  5.40496502e-03,
         4.94603859e-03, -5.10526123e-03, -1.69725623e-03,
         4.41548461e-03, -6.36563031e-03,  5.58748608e-03],
       [ 1.66968326e-03,  6.84569997e-04,  1.10089290e-03,
        -1.57758896e-03,  2.71754130e-03, -1.95931154e-03,
        -1.69203954e-03,  1.83983427e-03,  9.50489019e-04,
        -1.41704665e-03,  2.18370021e-03, -1.87759881e-03],
       [-1.43376773e-03, -5.03353891e-04, -1.56741892e-03,
         1.00897159e-03, -2.13655597e-03,  1.19404052e-03,
         1.14592921e-03, -1.14998652e-03, -2.03587988e-04,
         1.06095581e-03, -1.50961708e-03,  1.37689698e-03],
       [ 3.22145363e-03,  1.22499326e-03,  3.09246685e-03,
        -2.64682318e-03,  5.18317521e-03, -3.36330524e-03,
        -2.98108649e-03,  3.06384172e-03,  1.14631630e-03,
        -2.62948778e-03,  3.81773803e-03, -3.38846748e-03],
       [ 2.49434495e-03,  1.01959263e-03,  2.93981470e-03,
        -2.00139172e-03,  4.44644550e-03, -2.88502686e-03,
        -2.69535952e-03,  2.75406544e-03,  8.17837426e-04,
        -2.44360557e-03,  3.42376600e-03, -2.95560714e-03],
       [ 8.45131814e-04,  3.23593267e-04,  9.79570206e-04,
        -6.40392769e-04,  1.40557857e-03, -8.62605288e-04,
        -8.11107515e-04,  8.19750014e-04,  2.06933852e-04,
        -7.41868163e-04,  1.04098290e-03, -9.18554026e-04],
       [-1.73334323e-03, -5.67106414e-04, -1.90497446e-03,
         1.17099006e-03, -2.44734623e-03,  1.29537354e-03,
         1.21707818e-03, -1.18371833e-03, -1.57726492e-04,
         1.13650854e-03, -1.60464225e-03,  1.51611422e-03],
       [-9.96848103e-04, -5.11766179e-04, -1.17623387e-03,
         9.73766844e-04, -2.20155600e-03,  1.64303789e-03,
         1.54494133e-03, -1.64596038e-03, -6.44311192e-04,
         1.36754138e-03, -1.93695340e-03,  1.56833476e-03],
       [-1.89493131e-03, -7.79027643e-04, -1.45760609e-03,
         1.75840745e-03, -3.18159582e-03,  2.27523665e-03,
         1.96251366e-03, -2.09313771e-03, -1.03058701e-03,
         1.66754075e-03, -2.50550243e-03,  2.16156361e-03],
       [ 2.59670592e-03,  8.70111049e-04,  2.55413307e-03,
        -1.86955836e-03,  3.65675474e-03, -2.04579346e-03,
        -1.86878594e-03,  1.86762866e-03,  4.37924522e-04,
        -1.69725821e-03,  2.46711774e-03, -2.30683526e-03],
       [-2.92961806e-04, -1.56768641e-04, -1.21146804e-04,
         3.26598412e-04, -5.66763105e-04,  4.51422879e-04,
         4.42836434e-04, -5.21731330e-04, -2.53035949e-04,
         3.68748559e-04, -5.86947310e-04,  4.58984578e-04],
       [ 1.02193945e-03,  3.18766834e-04,  8.71333992e-04,
        -7.32970308e-04,  1.30285148e-03, -6.99415687e-04,
        -6.10981137e-04,  6.08949515e-04,  1.63385877e-04,
        -5.44475275e-04,  8.22842587e-04, -8.03516479e-04],
       [ 9.41992388e-04,  5.02410287e-04,  6.75714924e-04,
        -1.10553973e-03,  2.06726207e-03, -1.72909535e-03,
        -1.49666995e-03,  1.64188852e-03,  9.35981865e-04,
        -1.24940206e-03,  1.86063361e-03, -1.49604992e-03],
       [-4.71089128e-03, -1.76412798e-03, -4.67353780e-03,
         3.63814225e-03, -7.32352678e-03,  4.46483400e-03,
         4.17004991e-03, -4.30766121e-03, -1.23055710e-03,
         3.75789683e-03, -5.46260551e-03,  4.86034434e-03],
       [-1.11979025e-03, -3.95259645e-04, -1.57015875e-03,
         7.45122961e-04, -1.84096210e-03,  1.03674445e-03,
         9.67352069e-04, -9.04488435e-04, -9.73860442e-05,
         9.25451983e-04, -1.21883722e-03,  1.12336210e-03],
       [ 1.08008645e-03,  3.69601767e-04,  1.10079814e-03,
        -8.33942788e-04,  1.62747863e-03, -1.00352184e-03,
        -8.26289062e-04,  7.93900050e-04,  2.91051518e-04,
        -7.37773953e-04,  1.04101049e-03, -9.85087128e-04]], dtype=float32)>, <tf.Tensor: shape=(12,), dtype=float32, numpy=
array([-0.00527307, -0.00208012, -0.00538859,  0.0043472 , -0.00886385,
        0.00580002,  0.00526131, -0.00542825, -0.00191231,  0.00467563,
       -0.00672178,  0.00587031], dtype=float32)>, <tf.Tensor: shape=(30, 1), dtype=float32, numpy=
array([[nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan]], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([nan], dtype=float32)>, <tf.Tensor: shape=(34, 40), dtype=float32, numpy=
array([[-3.3860451e-07,  3.9795466e-08,  8.2438007e-08, ...,
        -1.3473243e-07,  7.4884170e-08, -6.0867251e-08],
       [ 4.0484804e-07, -6.6413321e-08,  2.0378624e-07, ...,
         2.7979223e-07, -1.9052726e-07,  1.5071002e-07],
       [ 3.0193038e-07, -1.7757714e-07, -7.6228650e-07, ...,
         4.7106269e-08,  3.2320514e-07, -3.3166347e-07],
       ...,
       [ 1.0511639e-05, -2.1306187e-06, -4.6489163e-06, ...,
         6.8005734e-06,  2.5999066e-06, -1.8870714e-05],
       [-1.2363024e-05,  2.6739731e-06,  1.3619083e-06, ...,
         4.0031096e-06, -1.4352887e-06, -8.1519402e-06],
       [ 4.5396373e-05, -6.7171286e-06, -2.8934221e-05, ...,
         9.7542197e-06,  5.9354215e-07, -2.0892947e-05]], dtype=float32)>, <tf.Tensor: shape=(40,), dtype=float32, numpy=
array([ 2.46563195e-06, -1.23533198e-07, -6.86148383e-07, -2.42896874e-07,
        3.06919924e-06,  6.49363528e-07,  4.91079277e-07,  2.26344667e-07,
       -1.28873410e-06,  2.45956755e-07,  4.14002500e-07, -2.99580847e-07,
        2.15277851e-07, -3.09679933e-07,  4.74564359e-07,  6.76608988e-06,
        3.45106059e-07,  2.39940960e-07,  1.31428590e-06,  6.30180921e-06,
       -1.08519856e-07,  4.45442220e-06,  3.41714667e-06,  1.46682373e-06,
        9.74768909e-07,  3.27217504e-06, -3.31153871e-07,  1.26932332e-06,
        1.25256588e-06,  3.11929398e-08,  8.52427547e-07, -2.46241007e-06,
       -1.36124740e-07,  9.25408870e-07, -1.19652145e-06, -5.50184313e-06,
        2.58593423e-06,  5.85259500e-07, -5.91249318e-07, -2.34772955e-07],
      dtype=float32)>, <tf.Tensor: shape=(40, 80), dtype=float32, numpy=
array([[-9.7548082e-07, -3.2956231e-07,  5.8657213e-07, ...,
         6.2619279e-09, -7.0429581e-07,  7.8177794e-07],
       [-2.8628406e-06, -4.3664892e-07,  4.9469804e-07, ...,
         1.6533909e-06, -2.7060339e-06, -1.3158455e-06],
       [-3.4552988e-07, -1.1593673e-07, -1.3172368e-07, ...,
        -1.6182003e-07, -1.2018429e-06,  1.7512480e-07],
       ...,
       [ 1.7260968e-06,  3.1754971e-07,  1.8775380e-07, ...,
        -1.6733003e-06,  2.5063950e-06,  1.8057760e-06],
       [-8.4105892e-07, -2.9015293e-08,  6.0367535e-07, ...,
        -2.7922397e-07, -5.4736347e-08,  1.2173593e-06],
       [-1.9592751e-06, -3.0731837e-07, -2.4219830e-07, ...,
         1.3322384e-06, -3.0665296e-06, -2.1378487e-06]], dtype=float32)>, <tf.Tensor: shape=(80,), dtype=float32, numpy=
array([-5.3413220e-07, -1.4949268e-07, -2.2995656e-07, -6.5087966e-07,
       -4.4191282e-08, -1.2751321e-06, -2.2122420e-06,  3.0467515e-07,
        2.3890925e-06,  7.2809075e-07,  1.6359089e-06, -1.0225781e-06,
       -3.3817852e-07,  2.2051447e-06, -4.5716109e-07, -3.6845486e-07,
        5.3678656e-07, -9.1562526e-07, -1.3008679e-07, -6.1601514e-07,
        4.6261556e-07,  1.3461577e-06, -1.5030627e-06, -2.2115580e-06,
        5.4870429e-07, -9.5799805e-07,  6.6703549e-08, -8.1566861e-07,
       -6.2616391e-07,  1.2149501e-06, -2.0328716e-06, -1.9981937e-06,
       -1.6197371e-06, -9.1964108e-07,  2.1431129e-06,  6.4173548e-07,
       -6.8137513e-07,  3.9227476e-07, -4.8977512e-07, -1.2900562e-06,
       -8.4342025e-07, -6.9474629e-07,  1.7904941e-06,  6.2527057e-08,
       -4.0312190e-07, -2.1857295e-07,  3.3578948e-07,  1.3675469e-07,
        2.5464813e-06, -2.0385794e-06,  1.0618386e-06, -1.6522631e-06,
        9.8786393e-07,  1.3087727e-06, -2.6710684e-06,  3.9744657e-07,
        1.0001492e-06, -6.2118829e-07,  1.2713990e-06,  2.2723420e-07,
       -2.1246053e-06,  2.3065154e-06,  2.0631862e-06,  5.3283924e-07,
       -1.6064939e-06, -1.9213945e-07, -1.6344068e-06,  2.6321754e-06,
       -1.5402225e-06, -7.8985778e-07,  1.8755990e-06, -3.0642883e-07,
        2.6125539e-07,  9.7001623e-07, -5.9865459e-07, -5.5892053e-07,
       -1.9978629e-07,  1.5308012e-06,  2.5214732e-08, -1.2408861e-06],
      dtype=float32)>, <tf.Tensor: shape=(80, 30), dtype=float32, numpy=
array([[-1.3831974e-06, -8.6101221e-07, -8.9428113e-08, ...,
         5.6216550e-07,  5.5483798e-07, -1.1218477e-06],
       [ 2.3364066e-06,  3.0880469e-06,  1.9892709e-06, ...,
        -4.8921383e-06, -1.0022776e-06,  3.9980973e-06],
       [-1.8447524e-06, -3.3438162e-06, -2.0754471e-06, ...,
         4.0042723e-06,  1.0089857e-06, -3.4636284e-06],
       ...,
       [ 9.4948774e-07,  2.9977625e-07, -4.3143905e-07, ...,
        -1.6366320e-07, -4.2458970e-07,  8.7928470e-07],
       [ 8.6852782e-07,  3.4873898e-07, -2.6437681e-07, ...,
         1.0628710e-06, -3.6246871e-07, -6.6068321e-07],
       [ 3.2067264e-06,  2.0599575e-06,  1.9397896e-07, ...,
        -2.4316309e-06, -1.5614637e-06,  2.7416459e-06]], dtype=float32)>, <tf.Tensor: shape=(30,), dtype=float32, numpy=
array([-6.5349002e-07, -3.1884845e-06, -1.7895201e-06, -2.2513796e-06,
       -2.5412514e-06,  6.7450418e-07, -1.6438728e-06,  9.6825147e-07,
       -4.5226940e-07, -7.7687605e-07,  1.8464284e-07, -6.5541587e-07,
        5.0263827e-07, -2.0451978e-06,  2.2996969e-06, -3.7860677e-06,
       -3.2407883e-08, -7.1607616e-07,  6.8724432e-08, -7.2958636e-07,
        1.4894913e-06, -4.4702105e-07, -9.1596542e-07, -9.0413096e-08,
       -4.9179374e-07,  1.1720880e-07,  1.2157197e-06, -1.1357514e-07,
        8.9248852e-07, -4.5908786e-07], dtype=float32)>, <tf.Tensor: shape=(30, 30), dtype=float32, numpy=
array([[ 1.18667685e-07, -3.25216178e-07,  1.13433305e-06,
        -9.34230570e-07, -2.64229243e-07, -9.68213726e-07,
         2.13470184e-06,  5.00576789e-07, -1.20903337e-06,
        -8.96471761e-07,  3.79479729e-07, -6.00126668e-07,
         8.71613111e-07,  4.03397991e-07,  2.37411177e-06,
         1.06875268e-06,  1.39669524e-07,  1.00063028e-06,
         4.40509439e-07,  4.37533174e-08, -2.31793160e-06,
         5.25600854e-07, -5.48158596e-07, -3.10025086e-07,
        -1.63698382e-06, -1.78324001e-06,  1.11976703e-06,
         9.87384396e-09, -1.48526817e-06, -9.42436372e-07],
       [-3.65592371e-07, -5.72369402e-07,  2.40255622e-06,
        -6.88338616e-07, -1.25212068e-06,  1.20546110e-06,
         2.40290149e-07, -5.29045451e-07, -2.01435228e-06,
         1.42707609e-06, -3.98163763e-07, -8.29121518e-07,
        -6.01928377e-07, -1.46347304e-07,  2.73839476e-08,
         6.00680153e-07, -5.82636176e-07,  1.22532811e-08,
        -2.57928150e-07, -5.00028818e-07, -5.90321633e-07,
         6.87608065e-07, -4.90727302e-07,  1.81354821e-07,
         7.13640361e-07, -8.61594287e-07,  1.56737337e-06,
         5.68286396e-07, -5.31916385e-07, -3.56218663e-07],
       [-9.26324674e-07, -2.13109189e-07,  1.07382550e-06,
        -1.22570498e-06, -3.17676268e-08,  3.38482664e-07,
         5.16183320e-07,  4.90829279e-08,  3.04844036e-07,
         2.93191448e-07, -7.43646353e-07, -9.67504093e-07,
        -1.97301844e-07,  3.45197890e-07, -1.44691057e-07,
        -4.53174096e-07, -1.54036115e-06, -1.39671158e-07,
         7.17346381e-07,  6.97741811e-08,  8.39277334e-07,
        -3.93325877e-07,  4.98910026e-07,  2.74154104e-08,
         2.18379114e-06,  3.25494653e-07,  1.02620106e-07,
         1.56101964e-06,  1.64165795e-06,  1.50133985e-08],
       [-6.19945183e-07,  1.27475607e-06,  1.90994342e-06,
        -2.52605844e-07, -8.65665811e-07,  1.18471951e-06,
         3.58216823e-07, -2.43387944e-07, -1.10287067e-06,
         1.40248108e-06,  9.64161586e-08,  8.96542360e-08,
         1.38000462e-06,  1.78617825e-07,  1.38315477e-06,
        -2.10305302e-08, -8.76739307e-07, -1.13924159e-06,
         5.79621997e-07,  6.02265459e-07, -1.25913107e-06,
         1.55286102e-06, -1.79924655e-07, -1.92660067e-07,
         1.13809119e-06, -1.24083101e-07, -8.16657916e-07,
        -3.76745390e-09,  1.58884563e-06, -6.83641701e-07],
       [-3.55960509e-07, -1.49652806e-07,  4.05604737e-07,
        -2.08656672e-07, -5.48427579e-08,  4.06448549e-07,
        -5.80094053e-08, -1.99941610e-08,  2.82318524e-09,
         6.89150568e-07, -3.67771634e-07, -5.01132149e-07,
        -5.13014911e-08,  2.18247891e-07, -5.12203485e-07,
        -5.57087105e-07, -7.57516716e-07, -4.97890994e-07,
         8.63715570e-08,  1.54278510e-07,  8.83911241e-07,
         2.37072939e-07,  4.05558922e-07, -1.02261943e-07,
         1.35954065e-06,  4.26790734e-07,  1.59201193e-07,
         7.23562607e-07,  9.04016190e-07,  3.21472186e-07],
       [-5.13740076e-07,  4.44602705e-07,  1.19985975e-06,
         1.27202145e-06, -5.53012796e-07,  7.89814067e-07,
         6.01845613e-07, -8.33082197e-07, -1.65832762e-06,
         8.44631472e-07,  3.87375877e-07,  1.47304195e-06,
         9.28925033e-07, -1.51149649e-07,  1.16737419e-06,
         7.07668960e-07,  5.91383468e-07,  1.05325057e-06,
         2.45424530e-07,  3.83350596e-07, -9.43765713e-07,
         1.15083299e-06,  9.97539473e-08, -2.69200484e-08,
        -6.73481964e-07, -1.29040097e-06,  6.88592763e-07,
        -1.77992433e-06, -1.24736164e-06,  1.01713162e-08],
       [ 3.36895567e-07,  1.48911829e-06, -2.90138587e-07,
        -1.02918136e-07, -2.68233151e-07,  4.58435807e-07,
        -5.21884544e-07,  1.17306826e-07,  4.78827701e-07,
         5.35638151e-07,  3.01446619e-07,  3.42917929e-07,
         1.09260702e-06,  4.74995119e-08,  8.14126906e-07,
        -4.89380454e-07, -6.21948402e-07, -2.03067407e-06,
         3.88440725e-07,  6.51697519e-07, -1.00001057e-06,
         5.53507562e-07, -4.40549570e-07, -2.58917538e-07,
         7.94202947e-07,  9.77549121e-07, -2.31921103e-06,
         4.08407743e-08,  2.64410664e-06, -5.93480479e-07],
       [-2.64918384e-07,  7.39763323e-07,  1.02286720e-06,
        -1.96189058e-06,  8.16286487e-08, -5.80766027e-07,
         8.90173624e-07,  1.35940957e-06,  5.01418413e-07,
         3.38567929e-08, -1.39005010e-07, -1.88995932e-06,
         8.29099690e-07,  7.09949177e-07,  8.80316747e-07,
        -7.25698897e-07, -1.08676659e-06, -1.33722494e-06,
         5.26694294e-07,  2.95160760e-07, -5.95505753e-07,
         1.05722245e-06,  8.72151986e-08, -3.01001933e-07,
         7.38676931e-07,  4.77772062e-07, -9.24561391e-07,
         1.77720960e-06,  1.51680069e-06, -7.42379882e-07],
       [ 4.18274396e-07,  8.40267603e-07,  2.60312959e-07,
        -5.03876777e-07, -2.18916171e-07, -1.05970798e-07,
        -2.56036856e-07,  4.92028676e-07, -4.37519247e-08,
         9.24763185e-07,  9.19527224e-07, -9.07972435e-08,
         9.05671129e-07, -2.96850715e-07,  9.98947371e-07,
         1.09450127e-07,  1.11029550e-07, -7.61749391e-07,
        -3.71103653e-07,  1.16645111e-08, -1.24530789e-06,
         1.53931978e-06, -4.17780512e-07, -2.14048924e-07,
        -1.54139786e-06, -2.25948753e-07, -6.58188071e-07,
        -2.33985347e-07, -6.15279561e-08, -8.88100999e-07],
       [-1.27246494e-07,  1.98158023e-06, -1.05504557e-06,
         1.58535067e-06,  8.94326604e-08,  1.16984211e-07,
         1.82334702e-07, -5.07343429e-08,  6.04518846e-07,
        -9.62292575e-07,  1.04130464e-07,  1.30861611e-06,
         1.78942537e-06,  4.47506579e-07,  9.52203550e-07,
        -2.85604187e-07,  4.51927605e-07, -1.49045104e-06,
         9.10945460e-07,  1.06157893e-06, -1.27596422e-06,
         6.32035494e-07, -2.83406507e-08, -4.60679473e-07,
         5.22220660e-07,  7.67867050e-07, -3.09599977e-06,
        -9.64469677e-07,  2.37856875e-06, -2.96322042e-07],
       [-4.40996018e-07,  2.27311898e-06, -8.40613097e-07,
         2.50569042e-06, -2.87379606e-07,  1.80123277e-06,
        -1.14712930e-06, -1.11699887e-06,  4.13801502e-07,
         6.91960793e-07,  1.67342861e-07,  2.45975366e-06,
         1.65931806e-06, -1.85719045e-07,  2.33335754e-07,
        -5.88549710e-07,  3.25068754e-08, -1.77790378e-06,
         7.10108111e-07,  1.21364951e-06, -1.87665222e-07,
         9.09861569e-07,  2.88545550e-07, -1.65892075e-07,
         1.82055635e-06,  1.22753886e-06, -3.23124164e-06,
        -1.68010013e-06,  3.20647882e-06,  3.25025638e-07],
       [-3.65841402e-09,  2.06227639e-07, -9.30287342e-07,
         5.17500780e-08,  1.31308582e-08,  4.80864060e-07,
        -1.53601468e-06, -3.31999104e-08,  1.04482604e-06,
         7.71742634e-07, -2.25329075e-07, -2.02975343e-07,
        -4.77934293e-07, -5.33399884e-07, -1.51288373e-06,
        -5.14586986e-07, -3.18756150e-07, -9.81634457e-07,
        -8.28269776e-07, -2.65167444e-07,  1.23871860e-06,
         2.56593125e-07,  2.13841190e-07,  3.37680461e-07,
         9.21760090e-07,  1.02179888e-06, -6.01495287e-07,
         4.90593948e-07,  9.90170975e-07,  3.82342421e-07],
       [-3.87095497e-07, -3.08202345e-07,  1.45238926e-06,
         1.78614073e-06, -1.34072127e-06,  1.51462291e-06,
         2.20572574e-07, -1.69359441e-06, -2.43363752e-06,
         8.54200891e-07, -1.94892039e-07,  1.60020409e-06,
        -1.97566479e-07, -5.46083015e-07,  3.22207228e-07,
         1.39916642e-06,  5.24130201e-07,  9.43882810e-07,
        -7.56390151e-08, -1.35999940e-07, -1.10332599e-06,
         6.38623078e-07, -4.30520913e-07,  3.22783535e-07,
         7.97515298e-08, -1.26291741e-06,  1.06454524e-06,
        -1.76215281e-06, -1.25299243e-06,  9.91745353e-10],
       [ 2.09410217e-07, -9.78901880e-07,  1.15116268e-06,
        -5.74883757e-07, -8.52418793e-07,  3.93460454e-07,
         1.15554110e-06, -5.64784841e-07, -1.38302084e-06,
        -2.07988819e-06, -1.01237504e-06, -3.96754302e-07,
        -1.19862761e-06,  3.55147364e-07,  7.69680526e-07,
         1.63111258e-06, -6.16106831e-07,  3.64960499e-07,
         9.40978680e-07, -5.65525681e-07, -1.98885505e-06,
        -1.99278315e-06, -1.20904087e-06,  3.31387952e-07,
         8.30635713e-07, -5.84174131e-07,  5.04816683e-07,
         4.02942277e-07,  4.80279851e-08, -9.39537301e-07],
       [ 1.04300045e-06,  4.53029287e-07, -5.13311079e-07,
        -2.10975122e-06,  6.00675833e-07, -1.74035904e-06,
         2.08825043e-07,  1.89538309e-06,  1.26410009e-06,
        -3.59415253e-08,  4.81060283e-07, -2.15866385e-06,
         5.91945138e-07,  3.97990959e-07,  4.27513214e-07,
        -8.96812423e-07, -1.02261026e-07, -1.67110625e-06,
        -5.04344541e-07,  2.08861678e-07, -4.89715262e-07,
         9.73336682e-07, -2.72180500e-07, -4.00731892e-07,
        -1.28398631e-06,  7.42612428e-07, -7.90408535e-07,
         1.60585785e-06,  1.94844645e-07, -6.07338393e-07],
       [ 2.38788175e-07,  1.29136970e-06, -2.31850936e-06,
         1.59897638e-06,  1.18430489e-06, -4.12976192e-07,
        -1.42777151e-06,  3.37819642e-07,  1.84712576e-06,
         8.25526172e-07,  1.33932258e-06,  1.38983614e-06,
         1.38716030e-06, -4.72396806e-07, -2.99878081e-07,
        -9.51929280e-07,  1.42119131e-06, -2.69531085e-07,
        -8.75959927e-07,  5.75990612e-07,  1.12438238e-06,
         1.35700441e-06,  8.57059661e-07, -2.77192441e-07,
        -2.25213921e-06,  2.47419280e-07, -1.01345995e-06,
        -1.61756748e-06, -7.82465008e-07,  4.46534301e-07],
       [ 1.32929230e-07, -1.64181779e-06,  7.27477243e-07,
         1.43055377e-06, -1.27358385e-06,  9.59380714e-07,
         3.67743496e-07, -1.71279612e-06, -2.55241684e-06,
        -1.12299824e-06, -6.59224497e-07,  1.05862694e-06,
        -1.60901129e-06, -1.00457498e-07, -3.87832813e-07,
         1.78819266e-06,  3.32352244e-07,  1.52257792e-06,
         3.36858420e-07, -8.84482404e-07, -1.04395383e-06,
        -1.80820541e-06, -1.08445352e-06,  3.34139003e-07,
         3.98584774e-07, -1.07200094e-06,  1.61761488e-06,
        -7.73583281e-07, -1.31445756e-06,  1.60740981e-07],
       [-4.95255563e-07,  2.49965001e-06, -1.92943025e-06,
         2.42783972e-06,  7.84862436e-07,  1.11017005e-06,
        -2.42789338e-06, -3.99807959e-07,  2.02375986e-06,
         2.47274738e-06,  1.09243013e-06,  2.12155305e-06,
         1.88426588e-06, -8.67441258e-07, -9.19841341e-07,
        -1.77602249e-06,  8.73074839e-07, -1.69336249e-06,
        -7.98872748e-07,  1.15540149e-06,  1.90207106e-06,
         2.40000008e-06,  1.24334952e-06, -1.60704928e-07,
        -1.42308068e-07,  1.49903190e-06, -2.79269034e-06,
        -1.96286510e-06,  1.79280835e-06,  9.09339064e-07],
       [ 5.22380560e-07, -4.86600243e-07,  2.12191082e-07,
        -6.63576827e-07, -4.53359291e-07, -8.08475988e-08,
         4.88048897e-07, -1.52256632e-07, -5.61531010e-07,
        -1.65421648e-06, -3.86637595e-07, -2.09812526e-07,
        -8.40679377e-07,  1.97383287e-07,  6.71182306e-07,
         1.00446755e-06, -3.70168550e-07,  2.76519287e-08,
         6.21273671e-07, -4.24301078e-07, -1.56681710e-06,
        -1.82372901e-06, -1.11241070e-06,  1.83301182e-07,
         2.12254179e-07, -6.45352429e-08, -2.20225729e-07,
         3.29204511e-07,  3.18208151e-07, -7.59324735e-07],
       [-5.16660407e-07,  2.36776714e-06, -5.74103979e-07,
         1.71405259e-06, -5.14597502e-08,  1.40078441e-06,
        -1.26946634e-06, -6.72472368e-07,  7.44146519e-07,
         1.73196270e-06,  4.15283267e-07,  1.73674675e-06,
         1.82479516e-06, -3.43520526e-07, -2.68145641e-07,
        -1.38903511e-06,  8.15084036e-08, -2.04836215e-06,
         2.67002548e-07,  1.17014429e-06,  4.92847960e-07,
         1.60153104e-06,  3.99494468e-07, -1.41057967e-07,
         1.41478938e-06,  1.44538797e-06, -2.98677696e-06,
        -1.07749588e-06,  3.13181613e-06,  6.23809569e-07],
       [-6.27411794e-07,  2.62161848e-07,  2.10771168e-06,
         4.04623989e-07, -1.06206824e-06,  1.14696763e-06,
         7.46936962e-07, -6.64794015e-07, -1.95798020e-06,
         8.53596532e-07, -1.91586693e-08,  7.14639953e-07,
         6.70479949e-07,  6.73415173e-08,  1.10036729e-06,
         9.93273488e-07, -2.66423200e-07,  3.03853028e-07,
         5.15573447e-07,  7.61179280e-08, -1.26657426e-06,
         1.27366036e-06, -1.46140678e-07, -6.31319779e-08,
         5.93004131e-07, -8.77059165e-07,  4.76122523e-07,
        -7.41752388e-07, -6.79940371e-08, -4.71993388e-07],
       [ 2.51358529e-07,  1.81367034e-06, -2.63143193e-06,
        -5.94026915e-07,  2.06087316e-06, -1.74855222e-06,
        -6.12717486e-07,  2.12797818e-06,  3.71743477e-06,
        -1.14762543e-07,  8.14588816e-07, -9.09344749e-07,
         1.79951951e-06,  5.62536059e-07, -4.29680426e-07,
        -2.71127215e-06,  2.93335859e-07, -1.88650756e-06,
        -1.02365078e-07,  1.06548669e-06,  1.78529422e-06,
         1.04022251e-06,  1.28804243e-06, -6.65716982e-07,
        -5.42221983e-07,  1.85752856e-06, -2.66960387e-06,
         9.59144927e-07,  1.91822505e-06,  5.00251701e-07],
       [ 4.63174416e-07,  1.04166202e-06, -1.02248657e-06,
        -5.19929529e-07,  1.96959604e-07, -1.31005265e-07,
        -9.07980166e-07,  4.73347853e-07,  1.12653754e-06,
         3.28169591e-07,  5.31460842e-07, -6.24000904e-08,
         6.16330112e-07, -1.43446812e-07,  1.20486646e-07,
        -6.82045766e-07, -3.77214008e-07, -1.60527452e-06,
        -1.42439120e-07,  2.61174193e-07, -3.64945720e-07,
         2.69702298e-07, -3.40357730e-07, -1.14379013e-07,
         7.85339438e-08,  1.06590323e-06, -1.77556808e-06,
         3.81681133e-07,  1.78050038e-06, -3.10248709e-07],
       [-2.76929939e-07, -1.65039751e-06,  2.08940719e-06,
        -1.44456192e-06, -8.94882874e-07, -1.32029925e-07,
         1.97556005e-06, -2.51731080e-08, -1.93402525e-06,
        -1.16325975e-06, -1.11573320e-06, -1.37988650e-06,
        -1.13022497e-06,  6.93062589e-07,  1.11203065e-06,
         1.39390306e-06, -1.00513739e-06,  1.12352814e-06,
         6.88387956e-07, -6.14438818e-07, -1.38892085e-06,
        -1.15606554e-06, -6.07687582e-07,  1.87003408e-07,
         8.44749650e-07, -1.33383446e-06,  2.17393631e-06,
         1.12389625e-06, -1.02730587e-06, -8.49035814e-07],
       [ 5.67838185e-07,  9.06063974e-07, -1.40441955e-06,
        -7.18478759e-07,  7.34833691e-07, -3.33192588e-07,
        -1.41845180e-06,  9.17485522e-07,  1.93062033e-06,
         7.95817527e-07,  4.24506482e-07, -9.18769160e-07,
         3.70691737e-07,  4.39204086e-08, -1.05337654e-06,
        -1.46016941e-06, -4.19048263e-08, -1.90770220e-06,
        -6.87150191e-07,  2.36530781e-07,  1.26307077e-06,
         5.21059462e-07,  3.26894622e-07, -2.52923144e-07,
         1.19940196e-07,  1.64741004e-06, -1.49328366e-06,
         9.38277367e-07,  1.40797113e-06,  2.50857511e-07],
       [-4.64660530e-07,  1.19532092e-06, -1.53386964e-07,
        -2.59362878e-07,  1.01047576e-07,  1.05359845e-07,
         5.42904445e-07,  5.73256443e-07,  7.86953422e-07,
        -1.42149418e-06, -2.44929680e-07, -4.63158585e-07,
         1.18874550e-06,  8.73404360e-07,  3.89589502e-07,
        -8.65894094e-07, -8.73859619e-07, -1.46037576e-06,
         1.50225060e-06,  5.50419372e-07, -5.63607159e-07,
        -2.63585548e-07, -3.63353365e-08, -4.70878319e-07,
         1.85878969e-06,  9.71850454e-07, -2.21719711e-06,
         1.20657774e-06,  3.16180353e-06, -2.66322246e-07],
       [ 7.71157829e-07, -1.89489384e-07, -3.54036956e-06,
         1.69700195e-06,  1.24473752e-06, -7.76053184e-07,
        -1.12057160e-06,  3.36941355e-08,  2.07343828e-06,
        -2.49798541e-06,  2.53265497e-08,  1.20805271e-06,
        -6.74404362e-07, -1.89260874e-08, -1.42688907e-06,
        -2.49395214e-07,  9.73279612e-07,  2.47189888e-07,
         3.43744944e-08, -4.12127008e-08,  9.71088411e-07,
        -2.26221391e-06,  1.06411562e-07,  9.75861099e-08,
        -4.25242973e-07,  1.10699852e-06, -1.30652029e-06,
        -8.04103706e-07,  2.36026906e-07,  7.30263366e-07],
       [ 9.41354756e-07, -1.86842590e-06,  8.86440318e-07,
        -7.01198246e-07, -8.18285628e-07, -3.44921546e-07,
         2.89799118e-09, -3.64219602e-07, -1.76861408e-06,
         1.43178340e-07,  6.30218437e-08, -3.16829130e-07,
        -1.93558230e-06, -6.84684210e-07, -1.87914793e-07,
         1.88805075e-06,  6.82438554e-07,  1.57072486e-06,
        -1.14478280e-06, -1.26685541e-06, -1.09590997e-06,
        -5.16572413e-07, -1.13266515e-06,  4.33082533e-07,
        -2.30903788e-06, -1.38276857e-06,  2.48480364e-06,
        -3.74420495e-07, -3.33684989e-06, -6.35612196e-07],
       [ 2.01599377e-08,  2.21804157e-08, -3.92483628e-07,
        -1.73860735e-06,  6.08621633e-07, -9.30921146e-07,
         2.60127308e-07,  1.19088418e-06,  1.50618780e-06,
        -8.65005973e-07, -2.88987167e-07, -1.68664189e-06,
        -1.00458898e-07,  5.69793315e-07, -1.46476992e-07,
        -7.45001728e-07, -1.00166858e-06, -9.26893904e-07,
         4.61951231e-07, -1.09306058e-08,  3.56521383e-07,
        -5.38745553e-07,  1.17182580e-07, -1.76879965e-07,
         7.53250163e-07,  9.22317156e-07, -8.08272716e-07,
         1.97520194e-06,  1.59437468e-06, -3.47872628e-07],
       [ 1.27650779e-07,  8.36577897e-07, -1.63910272e-06,
         1.55395992e-06,  3.67556822e-07,  5.61391630e-07,
        -8.01483793e-07, -2.21544525e-07,  8.19548745e-07,
        -7.96988957e-07,  1.48786938e-08,  1.36786866e-06,
         4.80327003e-07,  1.55258618e-07, -3.51025164e-07,
        -4.68533017e-07, -6.66513245e-08, -9.84415010e-07,
         5.84454938e-07,  5.36998016e-07, -2.25139587e-08,
        -7.84948611e-07, -4.71157335e-09, -1.33915435e-07,
         1.25510553e-06,  1.18872492e-06, -2.20451398e-06,
        -7.88109787e-07,  2.10811913e-06,  1.65307043e-07]], dtype=float32)>, <tf.Tensor: shape=(30, 30), dtype=float32, numpy=
array([[ 2.28973818e-08, -1.87328214e-06, -8.94027266e-07,
         1.84412625e-06,  2.84585326e-07,  1.54217844e-07,
        -1.69611951e-07, -1.01404976e-06, -8.59046736e-07,
        -1.78395283e-06, -3.13970304e-07,  2.00651198e-06,
        -1.55057432e-06, -3.07382038e-07, -9.69455527e-07,
         1.28686906e-06,  4.65017820e-07,  2.80797735e-06,
         1.85281749e-07, -7.62679008e-07,  6.03725084e-07,
        -2.29814532e-06,  1.16697322e-07,  5.30002694e-07,
        -4.58532696e-08, -7.33313527e-07,  2.14305032e-06,
        -1.56736314e-06, -2.28143790e-06,  9.78331514e-07],
       [-3.47770538e-07,  7.57872215e-07,  1.13526028e-07,
        -9.66101197e-07,  6.52569270e-07, -7.19951970e-07,
         1.62686263e-07,  1.05764911e-06,  1.15875469e-06,
         1.81313203e-06,  5.98904194e-07, -1.16805882e-06,
         1.26960242e-06,  8.54735518e-08,  1.26866553e-08,
        -1.54298164e-06,  1.14542573e-07, -4.51890941e-07,
        -7.07947947e-07,  4.98152474e-07,  1.32862294e-06,
         2.07004427e-06,  1.10980704e-06, -3.81806984e-07,
        -6.05050275e-07, -3.26640475e-08,  5.38347820e-08,
         8.27581175e-07, -1.93014387e-07,  2.29059850e-07],
       [ 8.52099106e-07, -3.74956130e-07, -6.21333641e-07,
        -2.40956524e-06,  7.34509740e-07, -2.07632092e-06,
         3.92037293e-07,  1.99434203e-06,  1.15990667e-06,
        -7.48780451e-07,  2.17294840e-07, -2.41092653e-06,
        -2.29280857e-07,  3.93085600e-07,  3.27732636e-07,
        -5.65635673e-07, -2.96246185e-07, -6.68891460e-07,
        -4.35705289e-07, -3.93050925e-07, -3.35911921e-07,
         2.70427250e-07, -3.19766940e-07, -2.55889859e-07,
        -1.54752126e-06,  4.08969981e-07,  1.61436787e-07,
         1.89629213e-06, -4.35455291e-07, -5.67282939e-07],
       [-3.06943605e-07, -1.04462333e-06,  1.19745130e-06,
        -1.22284132e-06,  4.17839772e-08, -7.87683973e-07,
         1.98872794e-06,  7.26092935e-07, -7.70879637e-07,
        -1.67784015e-06, -5.41512748e-07, -1.46307002e-06,
        -2.99211308e-07,  8.58900250e-07,  8.17799560e-07,
         5.48212427e-07, -4.32550905e-07,  1.36126050e-06,
         7.42307350e-07, -3.39547540e-07, -7.24153267e-07,
        -6.73989177e-07,  5.94990830e-08, -1.27234074e-07,
         2.32436435e-08, -1.12720465e-06,  1.46540208e-06,
         1.16493209e-06, -1.33235721e-06, -3.37695809e-07],
       [ 5.72915667e-07,  4.53879579e-07, -4.07839138e-07,
        -1.64192568e-06,  7.16770785e-07, -1.28270426e-06,
         2.73279653e-07,  1.47198989e-06,  1.25805434e-06,
         8.62440643e-08,  3.44018133e-07, -1.44811372e-06,
         5.95533322e-07,  4.06665691e-07,  4.61169009e-07,
        -8.87941155e-07, -1.39724534e-07, -8.74165949e-07,
        -1.72730921e-07,  2.05417365e-07,  2.24468820e-07,
         6.96568236e-07,  1.65299056e-07, -3.57165447e-07,
        -7.52846120e-07,  6.10569714e-07, -6.74262139e-07,
         1.29159559e-06,  4.21865394e-07, -2.21786678e-07],
       [-1.00826833e-06, -7.60803914e-07, -6.74868829e-07,
         1.61485809e-06,  3.24225084e-07,  6.50602772e-07,
         6.95280562e-08, -9.32160106e-07,  1.90520041e-07,
        -1.62061576e-06, -8.50881122e-07,  1.10461417e-06,
        -7.94797870e-07,  2.21215060e-07, -1.35925006e-06,
         2.00930401e-07, -2.81478663e-08,  1.39475719e-06,
         7.41559688e-07, -1.23566309e-07,  1.32182845e-06,
        -1.75686080e-06,  7.72268152e-07,  2.60925702e-07,
         1.87182741e-06,  6.53736265e-08,  5.64077197e-07,
        -4.40460184e-07,  8.90719178e-08,  1.06318885e-06],
       [-2.32354694e-07, -7.29474152e-07,  2.83231839e-06,
        -1.15705120e-06, -1.17239301e-06,  3.52350924e-07,
         1.81627456e-06,  1.07903020e-07, -2.67747191e-06,
        -6.27198915e-07, -5.09824531e-07, -8.49651201e-07,
        -1.39483873e-07,  5.41838858e-07,  1.90385890e-06,
         1.63026823e-06, -7.24127119e-07,  9.35275295e-07,
         7.78077606e-07, -4.70602970e-07, -2.55811756e-06,
        -6.20643164e-08, -1.01352748e-06,  2.86407200e-08,
         2.00755750e-08, -1.63327365e-06,  1.64281175e-06,
         4.02684975e-07, -9.72820658e-07, -1.28250144e-06],
       [ 9.11069435e-07, -1.17585887e-06, -1.84650503e-07,
        -8.98989242e-07,  2.20324566e-07, -1.80884740e-06,
         1.49641141e-06,  9.11801123e-07, -3.86030365e-07,
        -2.18806895e-06,  5.01540001e-08, -1.20401364e-06,
        -6.26558403e-07,  5.15990962e-07,  7.32404089e-07,
         7.72757403e-07,  7.54891005e-07,  9.90370836e-07,
        -1.06908828e-08, -6.23178209e-07, -1.28516001e-06,
        -7.52238975e-07, -6.35043079e-07, -2.15597197e-07,
        -1.98480757e-06, -9.85256293e-07,  1.10648875e-06,
         6.58206943e-07, -2.19741833e-06, -5.22781420e-07],
       [-4.79540063e-07, -3.68196737e-07,  9.07273659e-07,
         1.76047763e-06, -5.73315390e-07,  9.65497748e-07,
         7.61554361e-07, -9.24745621e-07, -1.86515035e-06,
        -2.34716708e-07, -2.95640518e-07,  1.43066052e-06,
         2.05416455e-07,  1.29388241e-07,  5.92664378e-07,
         8.60848672e-07,  1.43990576e-07,  1.20974892e-06,
         6.20737353e-07,  1.61120340e-07, -8.49119942e-07,
        -1.16083413e-07, -6.79462460e-08,  1.37679486e-08,
         4.20701724e-07, -1.13765054e-06,  8.57383384e-07,
        -1.45363128e-06, -8.34158641e-07,  4.13106029e-08],
       [-5.05175706e-08, -2.30941077e-06,  2.07021185e-06,
        -2.85839064e-07, -1.26364512e-06,  2.53349810e-07,
         2.42703163e-06, -1.07002131e-06, -3.11011445e-06,
        -3.40359156e-06, -1.32736841e-06, -2.64821466e-07,
        -1.64440416e-06,  6.83368228e-07,  1.39234419e-06,
         2.64868368e-06, -6.52446829e-07,  2.10317739e-06,
         1.51931113e-06, -9.60774059e-07, -2.64940013e-06,
        -3.12788279e-06, -1.53376448e-06,  2.77856373e-07,
         8.40344114e-07, -1.78731091e-06,  2.26274119e-06,
         3.80618786e-07, -1.31209038e-06, -6.71612497e-07],
       [-6.55834526e-07, -9.19483909e-07,  3.00624879e-06,
        -1.89452533e-06, -1.25065822e-06,  3.09573409e-07,
         2.56975886e-06, -1.07268306e-07, -2.25115082e-06,
        -1.95221241e-06, -1.22010749e-06, -1.48128743e-06,
        -3.67964873e-07,  1.04352148e-06,  1.65127028e-06,
         1.32961543e-06, -1.45088040e-06,  7.54173300e-07,
         1.76576600e-06, -4.12263489e-07, -2.40217150e-06,
        -1.51865595e-06, -9.95473670e-07, -2.49386289e-09,
         1.80018594e-06, -1.31199886e-06,  1.21829453e-06,
         1.76744174e-06,  6.54048051e-07, -1.23073653e-06],
       [ 4.08608997e-07,  1.39432916e-06,  6.21333868e-07,
        -9.97176585e-07, -2.38141013e-07, -3.32892455e-07,
         1.94454628e-07,  9.32300338e-07,  3.13000442e-07,
         1.61335811e-06,  5.80813150e-07, -9.05989111e-07,
         1.34648883e-06,  1.17974878e-07,  9.57011594e-07,
        -1.07065807e-06, -1.27990702e-07, -1.79453536e-06,
        -2.95020470e-07,  5.91876983e-07, -6.00857220e-07,
         2.09593918e-06, -6.26833412e-08, -3.64100941e-07,
        -5.61457455e-07,  4.23928498e-07, -1.42563908e-06,
         5.81666313e-07,  1.21317282e-06, -6.47524189e-07],
       [-9.76054821e-07, -2.58572300e-07,  4.93408720e-07,
         4.28174758e-07,  7.30129841e-08,  6.56954285e-07,
         2.29318303e-07, -5.88104172e-07, -1.44281330e-07,
         3.59453907e-07, -4.14575737e-07,  3.13517660e-07,
         4.76174336e-08,  1.94142345e-07, -6.94612481e-07,
        -4.48969672e-07, -4.65145490e-07,  6.21958520e-07,
         3.02684839e-07,  2.11007446e-07,  1.57666705e-06,
         7.80924267e-08,  9.44558735e-07, -1.05818614e-07,
         1.71498414e-06,  1.68278689e-08,  5.71060013e-07,
         2.39301954e-07,  4.34957997e-07,  9.93355229e-07],
       [ 2.00742107e-07, -1.23060033e-06,  1.81094947e-06,
        -1.22991128e-06, -9.41711164e-07,  3.46556028e-08,
         5.57162934e-07, -5.02618605e-08, -1.89503282e-06,
         1.16204171e-06, -1.11532387e-07, -9.93646722e-07,
        -9.85534029e-07, -2.63836853e-07,  1.61412686e-07,
         1.06406014e-06, -7.71112099e-08,  7.87998601e-07,
        -7.78694698e-07, -8.24871108e-07, -6.75155945e-07,
         8.78200922e-07, -5.95945551e-07,  1.35569053e-07,
        -6.32271451e-07, -1.10437054e-06,  2.26058114e-06,
         5.58520355e-07, -1.86997067e-06, -2.78583286e-07],
       [ 6.79173297e-07,  1.98684006e-07, -2.77108256e-08,
         5.95583060e-07, -1.86061072e-07, -2.41440034e-07,
         3.00455383e-07, -1.61813091e-07, -6.32836986e-07,
        -2.04255173e-07,  6.31640091e-07,  9.01953399e-07,
         3.61509706e-07, -2.47411833e-07,  9.96236963e-07,
         9.87235921e-07,  1.12708221e-06,  6.61811669e-07,
        -2.06261632e-07, -3.29400294e-08, -1.16998388e-06,
         5.65007554e-07, -4.33160125e-07, -5.85193192e-08,
        -1.89481932e-06, -8.26512860e-07,  1.86947489e-08,
        -1.19652475e-06, -1.54391716e-06, -3.15589574e-07],
       [-6.50313098e-07,  6.20323192e-07, -7.50456707e-07,
         3.01144473e-06, -3.87392618e-07,  1.90610695e-06,
        -3.93200310e-07, -1.80855386e-06, -5.66521749e-07,
        -1.13713463e-06, -7.10939162e-07,  2.81508233e-06,
         2.41287694e-07, -5.01609136e-08, -7.41278257e-08,
         8.11240398e-07,  1.88948832e-07,  3.41915012e-07,
         1.12220300e-06,  4.08549454e-07, -3.42007610e-07,
        -1.31240915e-06, -2.48434837e-08,  3.99070643e-07,
         2.06559298e-06,  2.49107416e-07, -1.37338839e-06,
        -2.13797966e-06,  1.68749602e-06,  3.67967687e-07],
       [-5.19303342e-07, -2.01216608e-08,  1.01812157e-06,
        -6.15614454e-07, -1.76669175e-07,  8.25111101e-07,
        -3.60284957e-07, -2.82625024e-07, -1.64781625e-07,
         2.23254369e-06, -9.42249869e-08, -6.96017310e-07,
         1.00266789e-07, -1.17000269e-07, -5.82283178e-07,
        -1.10529334e-06, -6.81087101e-07, -3.29141642e-07,
        -4.21974306e-07,  2.37035593e-07,  1.50904953e-06,
         6.96495249e-07,  6.07539334e-07, -4.14235402e-08,
         9.32734565e-07,  5.08460971e-08,  8.23506127e-07,
         5.92071558e-07,  3.43000238e-07,  5.98858321e-07],
       [-5.46564479e-07,  7.46669684e-07,  4.05757930e-07,
         6.50450261e-07, -4.21034002e-07,  6.75383490e-07,
         1.11889744e-06, -4.17515935e-07, -5.63363017e-07,
        -1.61967375e-06, -5.57191527e-07,  5.50713537e-07,
         1.03720959e-06,  8.81466462e-07,  1.27982992e-06,
         2.56883709e-07, -8.41880194e-07, -7.44168517e-07,
         1.67151597e-06,  5.91998401e-07, -1.51779273e-06,
        -8.18246463e-07, -3.55415722e-07, -2.70782493e-07,
         2.20259517e-06,  2.00499073e-07, -1.68794998e-06,
         5.63037545e-08,  2.48564402e-06, -4.42960129e-07],
       [-1.16587984e-07, -8.49447758e-07,  2.00576960e-06,
        -6.73628790e-07, -1.04935305e-06,  5.98382826e-07,
         3.57993827e-07, -3.78201577e-07, -1.95816483e-06,
         1.23593236e-06, -1.26625508e-07, -5.19458752e-07,
        -7.25940424e-07, -2.77909862e-07,  3.98319344e-07,
         1.03959394e-06, -3.14748888e-07,  5.48783987e-07,
        -3.98964460e-07, -5.78436357e-07, -9.90397325e-07,
         5.85419116e-07, -6.13762211e-07,  1.99947692e-07,
        -3.86697820e-07, -1.14144552e-06,  1.82939561e-06,
         2.71217502e-08, -1.41978319e-06, -6.55512906e-07],
       [-4.71757119e-07, -4.17120532e-07,  1.79600454e-06,
        -1.91399226e-06, -6.16409977e-07, -2.83600173e-07,
         2.27263195e-06,  3.59071549e-07, -1.15348678e-06,
        -2.11884571e-06, -6.15579381e-07, -1.68040413e-06,
         1.70053170e-07,  1.07712526e-06,  1.70826547e-06,
         7.51086816e-07, -1.38345399e-06,  1.91951983e-07,
         1.52637529e-06, -1.58052728e-07, -2.11382394e-06,
        -1.21053199e-06, -8.02016757e-07, -2.32007125e-07,
         1.30078820e-06, -8.87619592e-07,  1.73622908e-07,
         1.72507828e-06,  7.74170644e-07, -9.85290740e-07],
       [-7.13397867e-07, -9.83582026e-07,  4.76636046e-07,
        -7.83202623e-08,  1.81123198e-07, -2.66152426e-07,
         9.55814926e-07, -1.42951592e-07, -2.44657826e-07,
        -1.21441985e-06, -7.00739406e-07, -4.37676363e-07,
        -5.30435955e-07,  4.55857162e-07, -1.77945822e-07,
         1.58385603e-07, -2.18361095e-07,  1.56077249e-06,
         5.74169121e-07, -3.31416516e-07,  8.79827041e-07,
        -9.35445655e-07,  5.80858625e-07,  7.59740715e-10,
         6.95102244e-07, -6.28361363e-07,  1.48990478e-06,
         7.35090566e-07, -7.74060197e-07,  5.69845952e-07],
       [ 1.05550259e-06, -7.54087637e-08, -1.20870504e-06,
         3.83642771e-07, -8.76529498e-08, -7.02454713e-07,
         4.98014970e-07, -4.70095074e-10, -3.28267333e-08,
        -3.08666313e-06, -8.67676135e-08,  4.63915683e-07,
        -3.06035275e-07,  2.86064449e-07,  9.42284316e-07,
         1.33833487e-06,  6.12450606e-07,  3.39115900e-08,
         7.50274978e-07, -2.66396682e-07, -2.35022662e-06,
        -2.09422296e-06, -1.39352812e-06,  6.23565199e-08,
        -8.14292889e-07, -5.56160558e-08, -1.20411744e-06,
        -5.08118660e-07,  8.68369057e-08, -8.37686514e-07],
       [ 1.32114820e-07, -4.72662407e-08,  1.60822492e-06,
        -4.90231798e-07, -7.94554808e-07,  3.98503971e-07,
         8.80177993e-07,  2.16193584e-07, -1.66376128e-06,
         2.17691536e-07,  2.74109766e-08, -3.60354477e-07,
         2.52812839e-07,  2.57762792e-07,  1.40464726e-06,
         8.93132039e-07, -4.44899314e-07, -2.36657769e-08,
         3.69160261e-07, -1.21738651e-07, -2.00342856e-06,
         4.58299212e-07, -7.59446266e-07, -3.85454371e-08,
        -2.39574604e-07, -8.60709406e-07,  3.85126043e-07,
        -8.93008263e-08, -3.61989919e-07, -1.13159024e-06],
       [ 5.46075228e-07, -9.39274287e-07, -4.94015808e-07,
        -8.14612235e-07,  6.71269106e-07, -1.18386356e-06,
        -2.28593052e-07,  6.70993927e-07,  6.11422877e-07,
         4.26124444e-07,  1.60295542e-07, -8.20957382e-07,
        -8.84533108e-07, -2.89097102e-07, -8.47950332e-07,
        -2.15968498e-07,  6.81822030e-07,  7.23772018e-07,
        -1.08840754e-06, -4.64218687e-07,  1.27711621e-06,
         4.52167171e-07,  4.36600033e-07,  1.08105681e-07,
        -1.56197791e-06, -4.21474624e-08,  1.42481736e-06,
         4.83621420e-07, -1.92288780e-06,  4.77239041e-07],
       [ 1.77505427e-07,  1.37544316e-06,  8.43470161e-07,
        -7.22263962e-07, -4.07136667e-07,  3.40659483e-08,
         3.74636613e-07,  5.55960355e-07,  5.12336555e-08,
         9.90776243e-07,  5.06901131e-07, -4.26988663e-07,
         1.30063927e-06,  1.20881509e-07,  1.61565356e-06,
        -2.80455197e-08, -1.66458847e-10, -1.12426426e-06,
         4.93278378e-08,  4.96236794e-07, -1.27206113e-06,
         1.51290715e-06, -2.64695927e-07, -2.62943104e-07,
        -7.75181206e-07, -2.15715687e-07, -1.29530440e-06,
         6.75960408e-08,  7.38185008e-07, -1.08601216e-06],
       [ 6.94835478e-07, -1.84488010e-06,  1.48618813e-06,
        -2.46988134e-06, -4.81080576e-07, -1.30803369e-06,
         2.47190860e-06,  8.09194148e-07, -1.55193754e-06,
        -2.55935174e-06, -7.54200698e-07, -2.52962218e-06,
        -1.16436161e-06,  9.20919433e-07,  1.70975056e-06,
         1.48098923e-06, -5.02153853e-07,  1.10576502e-06,
         6.77336629e-07, -1.02472745e-06, -2.55263808e-06,
        -2.24979885e-06, -1.52441498e-06, -3.09495363e-08,
        -7.94674236e-07, -1.65525728e-06,  2.05631886e-06,
         1.81310645e-06, -1.73695673e-06, -1.46188654e-06],
       [ 3.25860185e-07,  7.66637669e-08,  7.70584052e-07,
         4.99984367e-07, -1.20052925e-06,  1.46406387e-06,
        -6.73121349e-07, -1.15612579e-06, -1.29490491e-06,
         5.40502128e-07, -3.80274741e-07,  8.53459142e-07,
        -6.05650655e-07, -4.39150426e-07,  4.06274125e-08,
         8.69146618e-07, -3.55244964e-07, -9.00910436e-07,
         1.07766795e-07, -1.59385763e-07, -1.27854594e-06,
        -6.21954314e-07, -1.15665364e-06,  3.63844492e-07,
         1.00908983e-06,  3.20683654e-07, -6.54624728e-07,
        -6.61881074e-07,  1.20919469e-06, -5.66591211e-07],
       [-7.88521163e-07,  1.30446995e-06, -7.37197752e-08,
         2.36074038e-06, -4.53667610e-08,  1.77009213e-06,
        -2.03049581e-06, -1.12140594e-06,  1.22416736e-07,
         3.86663214e-06,  7.94542757e-07,  2.11016413e-06,
         1.07953497e-06, -1.14019190e-06, -1.16324532e-06,
        -1.15177409e-06,  6.98369774e-07, -4.86847910e-07,
        -1.16529009e-06,  7.15650685e-07,  2.15148793e-06,
         2.91821652e-06,  1.22487916e-06,  6.36846238e-08,
        -8.20190849e-08,  4.04885554e-07, -3.66379311e-07,
        -2.18489140e-06,  2.69287852e-07,  1.23546636e-06],
       [ 1.07167807e-06, -3.53296173e-07,  1.19058882e-07,
        -1.20213645e-06, -1.75030721e-07, -1.15705075e-06,
         4.60432034e-07,  9.04174158e-07, -3.73816533e-07,
        -4.50013260e-07,  3.33413652e-07, -1.19725223e-06,
        -2.36693623e-07, -3.58347307e-08,  8.48665309e-07,
         5.11274948e-07,  3.59080246e-07, -2.06839317e-07,
        -4.29434550e-07, -4.60614075e-07, -1.59620174e-06,
         1.93066555e-07, -1.04279059e-06, -6.62941702e-08,
        -2.03107220e-06, -5.33085370e-07,  4.09237686e-07,
         5.50343088e-07, -1.15270711e-06, -8.78834271e-07],
       [ 1.08734980e-07, -7.35662297e-07,  2.14619854e-06,
        -1.20071343e-06, -1.08849747e-06,  3.37705160e-07,
         1.37622760e-06, -2.24833855e-07, -1.87722560e-06,
        -5.61210925e-07, -8.06507387e-07, -9.21762762e-07,
        -6.49014169e-07,  5.31788146e-07,  1.20433936e-06,
         1.28861780e-06, -8.63494108e-07,  6.39175184e-08,
         8.34398236e-07, -3.08851611e-07, -1.75846947e-06,
        -8.69396331e-07, -1.03850255e-06,  4.93988814e-08,
         9.60786451e-07, -6.77244032e-07,  6.10457050e-07,
         9.16466433e-07,  3.81452452e-07, -1.05239678e-06]], dtype=float32)>, <tf.Tensor: shape=(60,), dtype=float32, numpy=
array([-3.6242380e-04, -8.0482749e-04,  5.9164583e-04, -3.0700555e-03,
        1.8453998e-03, -5.0285575e-04,  2.0388672e-03,  2.5541235e-03,
        7.1483193e-04, -1.1383435e-03, -1.2131749e-03, -2.5071204e-03,
       -3.3444996e-04,  6.0402538e-04,  1.1035779e-03, -9.3269598e-04,
       -2.2402548e-03, -1.5622373e-04,  5.7090074e-04, -1.7899487e-04,
       -6.0397183e-04, -1.9768644e-03, -5.6354195e-04, -1.3416861e-04,
       -5.6688063e-04, -5.6625751e-04,  1.2546852e-03,  2.1221570e-03,
       -1.4757554e-03, -1.4809018e-03, -2.0917151e-04,  9.6001080e-04,
       -1.3376924e-04,  1.5775139e-03, -1.8967403e-03,  7.3729485e-04,
       -1.7501183e-03, -1.8386523e-03, -8.1022113e-04,  6.2609185e-04,
        2.9948080e-04,  1.6392921e-03,  3.2274224e-04,  7.9504668e-04,
       -4.6612480e-05,  4.8590841e-04,  2.3277347e-04,  6.9365313e-05,
        4.7590028e-04,  6.1087828e-04,  1.1724868e-03,  9.1235718e-04,
        6.8358937e-04,  1.4941714e-04,  2.4273058e-03,  3.1297922e-04,
       -4.8750866e-04, -2.9911930e-04,  2.5957578e-03,  6.1033078e-04],
      dtype=float32)>, <tf.Tensor: shape=(30, 45), dtype=float32, numpy=
array([[-7.0891235e-07, -4.1339600e-08, -1.0858657e-06, ...,
         4.5827787e-07, -8.5405520e-07, -9.2903753e-07],
       [ 5.9232070e-07,  7.0330049e-07, -3.4304182e-07, ...,
         8.5552739e-07,  2.6598007e-08,  9.0193629e-07],
       [ 7.0698979e-07,  7.0959300e-07,  8.5531138e-07, ...,
        -1.1553527e-07,  9.4950423e-08,  4.9549919e-07],
       ...,
       [-2.4043058e-07, -2.7649335e-06, -9.8475277e-07, ...,
         2.4893276e-07, -4.5479424e-06,  5.0996728e-06],
       [-6.6497552e-07, -1.2498138e-07, -3.7251419e-07, ...,
         1.0977682e-06, -7.8837724e-07, -1.7752419e-07],
       [-2.8767175e-07,  7.8774440e-07,  2.4083677e-06, ...,
         1.4797918e-07,  3.0840126e-06, -1.8877514e-06]], dtype=float32)>, <tf.Tensor: shape=(30, 45), dtype=float32, numpy=
array([[-2.9295234e-07,  5.2989867e-07,  1.8503358e-06, ...,
        -5.8143314e-08,  2.0249747e-06, -3.0042127e-06],
       [-3.9122511e-07,  9.7794396e-07, -1.4268406e-06, ...,
        -2.4432765e-07,  3.7385129e-07, -1.2485079e-06],
       [ 1.5855373e-06,  4.6523397e-07, -1.4043295e-06, ...,
        -4.1457903e-07, -1.3733536e-06,  1.7429526e-06],
       ...,
       [ 2.4135625e-06,  2.8659476e-06, -1.2570478e-06, ...,
         7.4480459e-07, -2.2370921e-06,  6.4601875e-07],
       [ 3.7504523e-07,  2.2712472e-06,  5.0629103e-07, ...,
        -4.7470405e-07,  1.2715163e-06, -2.2472236e-06],
       [ 6.6537304e-08, -1.0343922e-06, -4.5136684e-07, ...,
         6.8788459e-08, -5.5126100e-07,  1.9815652e-06]], dtype=float32)>, <tf.Tensor: shape=(90,), dtype=float32, numpy=
array([ 2.67685136e-05, -1.50797627e-04,  3.77928372e-05,  1.86911129e-05,
       -5.96950646e-04, -8.57088817e-05, -5.59629043e-05, -5.97557810e-05,
        3.98979406e-04,  1.31254827e-04,  1.72925531e-04,  3.76511452e-05,
       -1.92345484e-04, -4.24045465e-05,  2.11500374e-05,  4.42682474e-04,
        7.29280800e-05,  8.72082019e-05, -8.11918071e-05, -1.91101091e-04,
       -2.95689533e-04, -3.36902449e-04,  1.21427751e-04,  1.12292888e-04,
        1.01770975e-05,  1.08252440e-04,  1.07451211e-04,  4.02472331e-04,
        1.65200894e-04, -1.50843945e-04,  1.23657999e-04, -1.00646008e-04,
        2.14946602e-04,  6.88583896e-05,  1.41034790e-04,  5.71101700e-05,
        2.41940361e-05,  1.51924833e-04,  7.78297472e-05, -1.06493622e-04,
        3.35925084e-04,  7.47852464e-05, -3.44838278e-04,  9.55015348e-05,
        2.62047601e-04, -7.99367204e-04,  6.97649957e-04,  4.53398505e-04,
        2.73780635e-04,  1.83239463e-04, -5.38495369e-04, -8.00601207e-04,
       -1.52822235e-04,  9.22112260e-04, -9.64856066e-04,  4.53956920e-04,
       -2.32308623e-04,  8.33136612e-04, -1.34255958e-03, -6.85779494e-04,
       -3.02933331e-04, -6.70169655e-04, -4.46477323e-04,  7.50143547e-04,
        1.14176283e-03, -1.24383543e-03, -2.69129989e-04, -1.92685780e-04,
       -1.21080608e-03, -3.29085946e-04,  4.03807091e-04,  1.84715522e-04,
       -8.17488763e-04,  1.24670129e-04,  5.52150363e-04, -5.33798884e-04,
       -4.68273880e-04, -8.91335309e-04, -1.18722755e-03, -9.84129030e-04,
       -1.37751136e-04, -4.12864087e-04,  1.10069395e-03, -1.54817000e-03,
        7.14622729e-04, -6.64245454e-04,  4.36518967e-05,  3.42621315e-05,
        1.48528663e-03, -1.49878371e-03], dtype=float32)>, <tf.Tensor: shape=(45, 24), dtype=float32, numpy=
array([[-2.2384631e-06,  5.5739093e-07,  9.8743396e-07, ...,
         6.2149201e-07, -5.0201169e-07,  2.3799383e-07],
       [-2.0112489e-06,  2.2493197e-07, -1.1989080e-06, ...,
        -6.5864828e-07, -6.2137985e-08,  1.2563651e-06],
       [-2.2091040e-06,  2.3579521e-06, -4.0580858e-06, ...,
        -2.5787665e-06,  1.6638955e-06,  2.0982443e-06],
       ...,
       [ 3.2313606e-06, -1.4669357e-06,  2.3732455e-06, ...,
         9.5382643e-07, -1.6318787e-06, -2.9682101e-06],
       [-2.9750464e-07,  2.1215910e-06, -2.4691037e-06, ...,
        -1.9064377e-06, -3.9712830e-07, -2.9038665e-07],
       [-1.9331567e-06, -8.6241636e-07,  5.4271675e-07, ...,
         3.8262530e-07,  4.6312917e-07,  8.4038805e-07]], dtype=float32)>, <tf.Tensor: shape=(45, 24), dtype=float32, numpy=
array([[-5.6248393e-07,  6.9905212e-08, -1.3561720e-07, ...,
         2.0101108e-06, -2.0150167e-06,  1.9184203e-07],
       [-2.4877554e-06,  6.6473621e-07,  7.2592542e-07, ...,
         7.2585476e-07, -1.2598608e-06,  7.5352028e-07],
       [-1.4542340e-07, -7.6316491e-07,  1.1607781e-06, ...,
         1.4593479e-06, -6.3586029e-08,  6.4512938e-07],
       ...,
       [-1.0928055e-06,  1.5613285e-06, -2.4746660e-06, ...,
        -2.2345871e-06,  1.3909881e-06,  2.4953385e-07],
       [-1.6235003e-07,  1.2345481e-06, -3.6032657e-06, ...,
        -1.9118979e-06,  3.7548200e-06,  1.5152810e-06],
       [-2.4966965e-07, -2.0700654e-06,  4.9068294e-06, ...,
         3.3830052e-06, -3.1549916e-06, -1.6839401e-06]], dtype=float32)>, <tf.Tensor: shape=(48,), dtype=float32, numpy=
array([ 1.6999048e-04,  1.7797764e-04, -2.8183419e-04, -5.6905043e-04,
        1.1097685e-03, -1.0271263e-03, -6.0089625e-04, -4.6106166e-04,
        8.4192056e-05,  1.1230926e-03,  1.6092528e-03, -1.5243827e-04,
       -3.8901542e-04, -9.7959104e-04,  6.7862042e-05,  8.8975491e-04,
        9.7482081e-04,  4.2383134e-04, -2.1207239e-04,  9.7592990e-04,
       -6.6259602e-04, -4.9605826e-04,  2.3815299e-04, -4.9791601e-04,
        4.0101691e-04,  1.5169587e-04, -1.3244798e-04, -7.1238674e-04,
        1.3077710e-03, -1.0431893e-03, -5.8978633e-04, -2.0751482e-04,
        2.2079797e-04,  9.1865507e-04,  1.9575334e-03,  2.1955627e-04,
       -3.7931791e-04, -1.0789644e-03,  1.6753828e-04,  1.3367099e-03,
        1.2436544e-03,  4.0589029e-04, -4.5096636e-04,  3.9915839e-04,
       -5.2965310e-04,  2.7414208e-05, -3.5917218e-04, -3.3141530e-04],
      dtype=float32)>, <tf.Tensor: shape=(24, 12), dtype=float32, numpy=
array([[-8.26684845e-06,  1.82651866e-06,  4.88041769e-06,
        -4.48724813e-06, -1.03339073e-06,  1.08589084e-05,
         5.47437685e-06,  5.26108761e-06, -1.77060400e-07,
        -2.59545432e-05,  6.73962722e-06,  3.88047511e-06],
       [-2.97294082e-06,  3.59555816e-06, -4.64715367e-06,
         3.63696176e-06, -3.76990192e-06, -5.36588595e-06,
         3.47440960e-06, -1.88118781e-06,  6.73795194e-06,
         9.35394655e-06, -3.32753962e-06,  3.99719465e-06],
       [-3.70255998e-06, -2.03407581e-06,  3.08135850e-06,
        -5.62650212e-06,  4.10419807e-06,  5.09540996e-06,
        -6.67542508e-07,  4.18338868e-06, -6.38615802e-07,
        -1.43645530e-05,  3.01976047e-06, -7.51895016e-07],
       [-5.09120764e-06, -1.44086857e-06,  4.03552394e-06,
        -5.54661619e-06,  2.95048858e-06,  7.42372276e-06,
         1.69988425e-06,  5.24246070e-06, -4.64821710e-07,
        -1.96206747e-05,  4.96899429e-06,  7.99597728e-07],
       [-2.23356233e-06,  3.40916040e-06,  2.47477715e-07,
         2.04570460e-06, -5.18538400e-06,  3.13511509e-06,
         3.91006506e-06, -1.51970733e-06,  5.62723585e-07,
        -3.58276270e-06,  2.21106598e-06,  3.45349167e-06],
       [-6.53009602e-06,  1.02446177e-06,  2.32236471e-06,
        -4.58494605e-06,  2.62810568e-06,  5.24596544e-06,
         9.75334160e-07,  4.80116933e-06,  2.38262118e-07,
        -1.33973072e-05,  2.49140862e-06,  2.09029054e-06],
       [ 9.52428218e-06,  1.88487684e-06, -1.08178810e-05,
         1.26460927e-05, -4.96077701e-06, -1.97647805e-05,
        -2.05974538e-06, -1.15425228e-05,  5.70731299e-06,
         4.58698923e-05, -1.24449998e-05, -5.97260112e-07],
       [ 3.00841475e-06,  2.12616214e-06, -2.30018395e-06,
         4.06630897e-06, -2.29882244e-06, -4.70230634e-06,
        -1.89546358e-06, -3.96009182e-06, -1.21469145e-06,
         1.46287239e-05, -4.34257208e-06, -2.72906590e-07],
       [-7.39322422e-06,  3.32931086e-06,  3.45705075e-06,
        -5.16100226e-06,  8.57246789e-07,  8.58180647e-06,
         1.09226744e-06,  5.23887047e-06, -1.08944130e-06,
        -1.64922894e-05,  4.43899626e-06,  2.89035211e-06],
       [-7.37133541e-06,  3.67101347e-06,  3.91451204e-06,
        -2.15091154e-06, -3.57426597e-06,  9.49993409e-06,
         5.87871637e-06,  3.35943218e-06, -6.13432292e-07,
        -2.13361600e-05,  5.26042686e-06,  4.50719017e-06],
       [-2.06984123e-06, -3.08687504e-06,  5.57188059e-06,
        -5.80869846e-06,  2.91210335e-06,  1.02321765e-05,
         8.32835781e-07,  4.43006047e-06, -2.67512701e-06,
        -2.19308422e-05,  8.43988619e-06, -1.50604217e-06],
       [-4.11726387e-06,  2.19368508e-06,  3.98087195e-06,
        -4.97651126e-06,  2.38281746e-06,  6.75090314e-06,
        -1.73678234e-06,  5.17108901e-06, -5.13241866e-06,
        -1.41816072e-05,  1.07279698e-06,  1.49669020e-07],
       [ 2.78919856e-06, -1.58686066e-06, -2.61609330e-07,
        -8.43923942e-07,  3.62881042e-06, -3.15480929e-06,
        -4.30932369e-06,  4.28012669e-07, -1.69717600e-06,
         6.37791436e-06, -2.66626535e-06, -2.90382332e-06],
       [-6.56328154e-07, -1.89209027e-06, -1.03571665e-06,
         1.19904016e-08,  8.13164775e-07, -1.52555049e-06,
         1.44274975e-06,  1.56155480e-07,  2.76656101e-06,
        -5.47273260e-07,  6.23438382e-07,  4.91268395e-07],
       [-1.69826865e-06,  2.74807940e-06, -6.59952593e-06,
         3.86512238e-06, -3.11904898e-07, -1.04599194e-05,
        -2.62176968e-07, -2.16957505e-06,  6.53113420e-06,
         2.03357285e-05, -7.85884640e-06,  2.50367816e-06],
       [ 4.00958425e-06,  2.24326845e-06, -4.62767730e-06,
         7.27428960e-06, -4.56475300e-06, -7.71843770e-06,
         7.08491143e-07, -6.02630416e-06,  2.27637338e-06,
         2.03101063e-05, -4.86582758e-06,  1.13727140e-06],
       [-4.95910172e-06, -8.21691572e-07,  3.22114420e-06,
        -4.42134615e-06,  1.89776665e-06,  6.34520939e-06,
         2.72396528e-06,  4.68791040e-06,  6.83758458e-07,
        -1.75103123e-05,  4.77291724e-06,  1.89743298e-06],
       [ 3.69981467e-06, -1.56162810e-06,  2.46634477e-06,
        -8.34477234e-08, -1.14476461e-06,  3.39719827e-06,
        -5.45577166e-07, -9.75078933e-07, -4.00692397e-06,
        -4.98174040e-06,  2.82701080e-06, -2.37183258e-06],
       [-2.07675825e-07,  4.86115425e-07, -1.39108280e-07,
        -1.43283913e-07,  1.43412001e-06, -1.41192231e-06,
        -1.10762244e-06,  1.17313891e-06, -1.71492638e-06,
         9.34697994e-07, -2.91408583e-06, -3.34140339e-07],
       [-3.19138326e-06,  3.86557258e-06, -2.49336551e-07,
         2.08882534e-06, -5.12914767e-06,  2.36213941e-06,
         4.17666570e-06, -8.68995642e-07,  1.13364536e-06,
        -2.91214565e-06,  1.06259733e-06,  4.02986370e-06],
       [-3.69174631e-06, -2.42785450e-06,  5.80308188e-06,
        -6.10634061e-06,  2.09958307e-06,  1.00633843e-05,
         2.32050616e-06,  5.16336058e-06, -2.25799931e-06,
        -2.57948304e-05,  7.36539960e-06, -5.17478838e-07],
       [-1.69149212e-06, -2.95208793e-06,  5.10100290e-06,
        -5.35917252e-06,  2.57859597e-06,  8.32749447e-06,
         5.60490321e-07,  4.43810222e-06, -3.71181636e-06,
        -2.08420588e-05,  5.99094392e-06, -1.42647605e-06],
       [ 4.14431724e-07,  2.22463404e-06, -4.48396986e-06,
         2.78995753e-06, -7.51676566e-07, -6.76775653e-06,
        -9.07267918e-07, -2.01002854e-06,  3.34235847e-06,
         1.54248974e-05, -4.76634932e-06,  9.88401894e-07],
       [ 6.05802916e-06, -9.06575508e-08, -4.66905294e-06,
         4.07369726e-06,  1.12716953e-06, -9.73733131e-06,
        -4.47295679e-06, -3.49413949e-06, -1.14260871e-07,
         2.29417255e-05, -6.92525509e-06, -2.38944290e-06]], dtype=float32)>, <tf.Tensor: shape=(24, 12), dtype=float32, numpy=
array([[-8.25934876e-06,  3.57852718e-06,  4.37219978e-06,
        -2.90461412e-06, -3.98043994e-06,  1.07043934e-05,
         6.00862222e-06,  3.88218314e-06, -3.99558360e-07,
        -2.27578930e-05,  6.57239843e-06,  4.81055395e-06],
       [ 3.13160558e-06,  2.07813559e-06, -2.95634595e-06,
         4.17144247e-06, -1.80591599e-06, -5.84896316e-06,
        -2.05032484e-06, -4.06437130e-06, -7.07100583e-08,
         1.69697414e-05, -4.93221341e-06, -3.91111342e-07],
       [-6.31606417e-06,  8.75904163e-07,  3.04010041e-06,
        -3.51596987e-06,  2.83642265e-07,  6.85727946e-06,
         3.81618338e-06,  4.43330418e-06,  7.98391056e-07,
        -1.79755662e-05,  4.65206540e-06,  2.85098417e-06],
       [-5.77572473e-06, -6.34270236e-07,  3.49766879e-06,
        -5.61350862e-06,  3.98872044e-06,  6.75751744e-06,
         3.84238035e-07,  5.15959528e-06, -9.80158120e-07,
        -1.69617197e-05,  4.01311945e-06,  7.13424924e-07],
       [-3.13267151e-06,  3.35599634e-06, -1.15552325e-06,
         2.25654526e-06, -4.63448850e-06,  1.37722668e-06,
         4.35906486e-06, -1.61142680e-06,  2.69180646e-06,
        -3.69993018e-07,  1.17697982e-06,  3.92158290e-06],
       [-1.66585687e-06, -3.75547586e-07,  2.20677384e-06,
        -1.85663771e-06, -1.61662868e-07,  4.78814218e-06,
         1.34530092e-06,  7.93134177e-07,  2.01304204e-07,
        -8.58137719e-06,  4.70965369e-06,  4.68137586e-07],
       [ 1.32285386e-05, -1.32875505e-06, -1.07755122e-05,
         1.23578084e-05, -5.02663534e-06, -2.04575754e-05,
        -2.40470308e-06, -1.23547425e-05,  6.16327543e-06,
         4.55425470e-05, -1.09050425e-05, -2.69297607e-06],
       [ 3.81317477e-07,  1.73371609e-06, -3.73589319e-06,
         3.58961324e-06, -3.08251811e-06, -3.95619236e-06,
         2.18247487e-06, -2.58485784e-06,  3.35126037e-06,
         9.26689336e-06, -1.66882307e-06,  2.09390032e-06],
       [-5.22331720e-06,  2.95809650e-06,  2.94848678e-06,
        -1.87045146e-06, -3.84269015e-06,  8.64742560e-06,
         4.12141026e-06,  2.11075417e-06, -1.41279628e-07,
        -1.43990246e-05,  6.25560824e-06,  3.51783638e-06],
       [-7.00961482e-06,  2.21633900e-06,  2.20935590e-06,
        -3.77943616e-06,  5.78116556e-07,  5.95074835e-06,
         2.45923366e-06,  4.48592937e-06,  6.61205149e-07,
        -1.44480182e-05,  2.86688055e-06,  3.09691654e-06],
       [-7.89733622e-06,  4.25967710e-06,  2.34951426e-06,
        -1.94931044e-06, -4.44853004e-06,  8.69511314e-06,
         6.02650471e-06,  2.49751611e-06,  1.07474284e-06,
        -1.88545546e-05,  4.39885525e-06,  5.11887083e-06],
       [-4.77753565e-06,  3.58016950e-06,  1.06552216e-06,
        -8.02032503e-08, -4.18929721e-06,  4.81875213e-06,
         4.17289175e-06,  4.82755695e-07,  1.95220946e-06,
        -6.99626935e-06,  3.26283907e-06,  3.94279186e-06],
       [-1.53337191e-06, -7.26847063e-08, -1.76272965e-06,
        -1.05816571e-06,  2.46942136e-06, -3.54185795e-06,
        -2.10437042e-06,  7.65703476e-07,  2.21762639e-06,
         5.74423757e-06, -3.24149119e-06,  3.66116950e-08],
       [-4.56615271e-06,  1.25494694e-07,  1.44631684e-08,
        -1.61897890e-06,  2.85086412e-06,  2.45968749e-07,
         7.54213545e-07,  2.58025784e-06,  1.76094522e-06,
        -3.36319226e-06, -4.72431395e-07,  1.62101799e-06],
       [ 3.13103237e-06,  2.44847820e-06, -2.64495407e-06,
         3.70773068e-06, -1.91069284e-06, -5.22753453e-06,
        -2.65320023e-06, -3.65840629e-06, -1.18506273e-06,
         1.57030863e-05, -4.82655469e-06, -5.23939150e-07],
       [ 1.82152348e-06,  2.55414716e-06, -4.94123788e-06,
         6.57139481e-06, -5.65863184e-06, -6.18889862e-06,
         2.97946758e-06, -5.41887403e-06,  4.25884946e-06,
         1.58419243e-05, -2.95606310e-06,  2.61088258e-06],
       [-6.90397383e-06,  3.09374127e-06,  4.04735556e-06,
        -2.80167251e-06, -2.47622074e-06,  9.99117765e-06,
         5.00207761e-06,  3.32005789e-06, -5.81400968e-07,
        -2.03859454e-05,  6.54567248e-06,  4.09870154e-06],
       [-5.16084310e-06,  3.26726126e-06,  3.95882989e-06,
        -1.67240296e-06, -2.55696659e-06,  7.82468396e-06,
         3.94362542e-06,  3.50905430e-06, -3.43330203e-06,
        -1.82797430e-05,  2.73020851e-06,  3.00933880e-06],
       [ 4.41519205e-06, -2.67680844e-06,  2.56347630e-06,
        -1.01770968e-06,  1.49539028e-06,  2.48148103e-06,
        -2.19819231e-06, -3.42793015e-07, -4.36097207e-06,
        -3.90490641e-06,  2.22715903e-06, -3.21655466e-06],
       [ 6.71158205e-06, -4.06603795e-06,  6.37183177e-07,
         1.24963861e-07,  1.82655799e-06, -1.73440264e-06,
        -3.61863795e-06, -1.69965790e-06, -2.53283361e-06,
         5.37787582e-06,  7.71465864e-07, -4.06687059e-06],
       [-6.57924465e-06,  1.16946751e-06,  6.81876190e-06,
        -6.85787882e-06,  3.55026577e-06,  1.16002684e-05,
         4.56579926e-07,  7.79400307e-06, -6.63274704e-06,
        -2.71716417e-05,  4.35785569e-06,  6.90015156e-07],
       [-4.03564218e-06, -1.34668142e-06,  4.25515327e-06,
        -3.63102322e-06,  8.79618312e-07,  8.02755949e-06,
         3.80652909e-06,  3.96298856e-06, -5.04188733e-07,
        -2.09867922e-05,  6.26109386e-06,  1.14790396e-06],
       [ 7.29150543e-06, -2.60037859e-06, -2.43667409e-06,
         1.66813982e-06,  3.02282342e-06, -6.82661948e-06,
        -5.79823882e-06, -2.64923324e-06, -2.38968119e-06,
         1.63497662e-05, -4.80596464e-06, -4.73429964e-06],
       [ 6.54614041e-06,  7.98274698e-07, -5.97267081e-06,
         7.54765870e-06, -5.26646772e-06, -1.00798643e-05,
        -1.91437834e-07, -7.75390799e-06,  3.49919719e-06,
         2.43585619e-05, -5.63348249e-06, -2.87263390e-07]], dtype=float32)>, <tf.Tensor: shape=(24,), dtype=float32, numpy=
array([-5.9709710e-04,  9.0901980e-05,  7.2983676e-04, -7.0680794e-04,
        1.5380797e-04,  1.3357942e-03,  1.0672732e-04,  6.5871788e-04,
       -5.5029691e-04, -2.7971123e-03,  7.2238862e-04,  7.8332669e-05,
        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,
        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,
        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00],
      dtype=float32)>, <tf.Tensor: shape=(30, 30), dtype=float32, numpy=
array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],
      dtype=float32)>, <tf.Tensor: shape=(30, 30), dtype=float32, numpy=
array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],
      dtype=float32)>, <tf.Tensor: shape=(60,), dtype=float32, numpy=
array([-5.9841585e-07,  2.6416692e-06, -1.2940317e-06,  1.7328405e-06,
       -1.1048539e-06, -2.0550576e-06, -7.8535015e-07, -7.4786880e-07,
        1.7189382e-06, -4.0252615e-07, -2.0469006e-06, -1.8959929e-07,
       -2.4618387e-06, -1.8892043e-06,  3.3422452e-06,  2.1880425e-07,
       -1.0959805e-07, -1.1362790e-06,  2.3636044e-07, -6.2477898e-07,
        1.3497623e-06, -4.5212511e-07, -8.8728811e-08, -1.7674148e-07,
       -2.0848886e-06,  5.0479088e-07, -2.4747073e-06, -4.8162275e-07,
       -1.4559705e-06, -3.4724526e-06, -2.5374773e-06,  1.7609235e-06,
        5.1180899e-07,  3.7115163e-07, -2.9169644e-07, -1.6307865e-06,
       -1.2980099e-06,  1.1958261e-06, -1.8432688e-06, -7.4939197e-07,
       -7.8927258e-07, -3.9992909e-07, -1.0825682e-06,  1.3317923e-08,
       -1.9906151e-06, -2.0518851e-06,  8.2727399e-07, -1.0384246e-06,
        1.6190169e-06, -1.3030720e-07,  5.4446900e-07, -9.0927574e-07,
       -1.6040655e-06,  1.0273973e-06, -7.4664695e-07, -1.6999606e-07,
       -3.1212699e-07,  1.4447340e-06,  1.6663125e-06,  5.1627296e-07],
      dtype=float32)>, <tf.Tensor: shape=(30, 30), dtype=float32, numpy=
array([[-1.73174737e-06, -2.48023980e-07,  2.51664278e-06,
         3.17050990e-07,  6.95543804e-07,  3.76288540e-07,
        -1.11928148e-06,  2.35054040e-06, -3.53212590e-06,
        -8.68600978e-08,  1.25437680e-06, -7.37537505e-07,
        -1.99613623e-07,  4.48799256e-08, -1.86008651e-06,
        -2.40999452e-06, -1.92620632e-07, -7.52656888e-07,
         2.56938790e-07,  3.98912164e-07, -3.87335291e-07,
         4.58696178e-07, -2.60801869e-07,  9.24372216e-07,
         6.54357393e-07, -9.58440069e-07,  6.96766847e-07,
        -1.88809352e-07,  1.90836045e-06,  5.61585864e-07],
       [ 1.00382874e-06, -5.37436335e-08, -2.09211180e-07,
        -4.55139030e-07,  2.05837495e-07,  2.16917738e-06,
        -2.41096245e-06, -1.94574659e-06,  2.07363485e-07,
        -5.67768438e-07,  2.20910010e-06, -1.71587226e-06,
         2.05969127e-06,  1.32171920e-07, -2.61749665e-06,
         1.40351790e-06,  3.82352937e-06,  6.24910399e-07,
        -3.04473105e-07,  9.63918751e-07, -6.04595527e-07,
        -1.73265573e-06, -1.71153147e-06, -9.63829848e-07,
        -2.79850894e-07, -2.03312720e-06,  2.76904274e-07,
         4.54054856e-07, -3.22301457e-06,  3.40029965e-06],
       [ 2.14453712e-06,  4.87386274e-08, -1.17542754e-06,
         1.28128488e-06, -5.62377863e-07,  1.76016329e-06,
        -1.16555270e-06, -8.92482490e-07, -4.40081976e-07,
         1.61093749e-06,  3.31693832e-06, -2.32026605e-06,
         1.53862118e-06,  2.12951090e-06, -1.87789351e-06,
        -5.59064119e-08,  3.23411973e-06,  5.41364159e-07,
         1.61647461e-06,  1.34080199e-06,  3.35421248e-07,
        -1.45085414e-06, -1.51239192e-06, -3.58984778e-07,
         3.98562833e-07, -1.08878385e-06,  6.00787644e-07,
         2.29324669e-07, -1.51638005e-06,  3.29915019e-06],
       [ 1.37843540e-06,  8.71772613e-07, -9.83851805e-07,
        -2.12816076e-09, -2.19601588e-06, -6.77675416e-07,
         7.76750881e-07,  2.23317875e-06, -1.98818475e-06,
         7.00478154e-07, -1.45167576e-06,  2.69517955e-07,
        -9.84816552e-07,  2.96240927e-07, -1.21933965e-07,
        -2.09892733e-06, -2.02158594e-06, -1.33851108e-06,
         8.25056759e-07, -8.36068693e-07, -3.88873815e-07,
         1.11479642e-06,  6.12392341e-08,  7.65641062e-07,
         8.50010622e-08,  7.55695226e-07,  7.94354776e-07,
         1.28405944e-07,  3.09804113e-06, -1.35237565e-06],
       [ 1.95213534e-06,  1.71379156e-06, -2.69507996e-06,
        -8.83002770e-07, -3.81245218e-06, -1.47794958e-06,
         7.21984406e-07,  1.92267999e-06, -1.50081814e-06,
        -3.40965926e-07, -4.50472862e-06,  1.10091469e-06,
        -1.91990330e-06, -9.09251924e-07,  9.57484872e-07,
        -2.10237590e-06, -2.55823079e-06, -2.01193757e-06,
         6.29284102e-07, -1.95775192e-06, -1.05127560e-06,
         1.07705739e-06,  3.95411519e-07,  9.07474316e-07,
        -5.05016146e-07,  1.11921713e-06,  3.28063237e-07,
         3.66115501e-07,  3.58297439e-06, -3.74341289e-06],
       [-1.95915891e-06, -1.02906483e-06,  2.85910164e-06,
         3.10068629e-07,  3.41797522e-06,  9.91639126e-07,
        -3.67427788e-07, -1.94476911e-06,  1.37887946e-06,
        -3.65921295e-07,  2.64866935e-06, -4.66463490e-07,
         1.24373548e-06,  5.22667278e-08, -8.36200513e-07,
         1.23736481e-06,  2.10830035e-06,  1.47220749e-06,
        -1.04163928e-06,  1.21410699e-06,  4.70876842e-07,
        -6.60583737e-07, -1.42613459e-07, -2.72266107e-07,
         7.64678703e-08, -9.75145099e-07, -5.81271479e-07,
        -2.94392066e-08, -2.96264284e-06,  2.18731384e-06],
       [ 2.42106739e-06,  6.41775955e-07, -1.54041663e-06,
         2.47106300e-07, -9.76772753e-07,  2.30471983e-07,
        -1.32826938e-06, -1.99650549e-06,  1.53663973e-06,
        -3.45065928e-07, -9.53829158e-07, -2.87661237e-07,
         2.61934588e-07,  5.74947592e-07, -6.36409368e-07,
         1.03184129e-06,  2.87627518e-06,  1.76202192e-07,
         3.73795729e-07,  1.31397655e-07,  9.11385314e-08,
        -1.61568846e-06, -1.18689456e-06, -6.41492647e-07,
        -1.26542000e-06,  4.47616912e-08, -5.04164632e-07,
         5.07200070e-07, -2.50332687e-06,  8.21845106e-07],
       [-1.99258102e-06,  9.31917043e-07,  2.09827249e-07,
        -1.02242416e-06, -1.58058936e-06, -1.67243206e-06,
        -6.84087468e-07,  1.93265851e-06, -2.30908245e-06,
        -1.97416807e-06, -4.24415748e-06,  1.86962120e-06,
        -1.99110241e-06, -2.28357112e-06,  3.99997361e-07,
        -1.99296665e-06, -2.49844265e-06, -1.63144568e-06,
        -6.10919756e-07, -1.49037839e-06, -8.90851197e-07,
         8.16329305e-07,  7.06974447e-07,  1.10240376e-06,
        -6.96710401e-07,  4.18277637e-07, -7.82633265e-08,
        -8.27563014e-08,  3.05953881e-06, -3.95861480e-06],
       [ 9.82685492e-07,  1.25355609e-06, -9.94222660e-07,
        -4.23419579e-07, -1.32325192e-06, -5.39949610e-07,
         9.16210297e-07,  1.46798322e-06, -1.71409340e-06,
         4.07750633e-07, -1.21122343e-06,  5.35563743e-07,
        -7.49125149e-07, -8.58154294e-08,  2.95110056e-07,
        -1.64646599e-06, -2.06876598e-06, -1.15920625e-06,
         3.59419005e-07, -6.34180026e-07, -4.80210758e-07,
         1.24764006e-06,  2.23056460e-07,  5.24952952e-07,
         1.72768637e-07,  4.74644537e-07,  7.14980047e-07,
         4.49192328e-08,  2.80990093e-06, -1.64052233e-06],
       [ 9.26227301e-07,  4.32964384e-07,  2.01693729e-06,
         3.32399225e-07,  5.59965315e-07,  8.60396128e-08,
         1.12666658e-06,  1.86394675e-06, -2.14206602e-06,
         7.90356012e-07, -2.22209849e-07, -1.53205534e-07,
        -4.97988026e-07,  9.08788195e-07, -1.29294665e-06,
        -2.53275039e-06, -9.79192350e-07, -7.34051355e-07,
         3.29086816e-07,  4.23311519e-09, -2.37719860e-07,
         1.26679640e-06,  3.29712861e-08,  1.06180642e-06,
        -2.35589823e-08,  6.51996515e-07,  7.82653615e-07,
         2.53107174e-07,  2.17025877e-06,  2.93712219e-08],
       [-3.71060423e-07, -8.65318043e-07,  1.31130901e-06,
         3.80100573e-07,  1.31156821e-06,  5.46910087e-07,
         1.66413952e-06,  8.86564862e-07, -3.71684450e-07,
         1.51699794e-06,  2.65666699e-06, -6.92610513e-07,
         6.49185893e-07,  1.05153208e-06, -3.42461135e-08,
        -2.67237397e-07, -1.20772802e-06,  7.38397887e-07,
         2.08983039e-07,  6.93498237e-07,  3.67263794e-07,
         1.05413562e-06,  6.09290680e-07,  1.63586932e-07,
         1.46103287e-06, -9.78889148e-08,  5.56115992e-07,
        -2.46654650e-07,  1.01063711e-06,  1.47202286e-06],
       [-8.67969732e-07, -1.45011256e-08, -7.66304538e-07,
        -3.65281835e-07, -1.01412547e-06, -7.65308187e-07,
         9.34033380e-07,  1.66105485e-06, -9.06214098e-07,
         3.23759366e-07, -8.54876873e-07,  3.56413352e-07,
        -7.34397531e-07, -5.51281460e-07,  1.40779321e-06,
        -5.25922587e-07, -2.51034362e-06, -6.47508614e-07,
         7.70282327e-08, -7.53281029e-07, -1.90944633e-07,
         1.03701223e-06,  8.41216888e-07,  2.35173445e-07,
         7.96779716e-07,  4.03086403e-07,  1.92782267e-07,
        -3.04559137e-07,  2.38838584e-06, -1.61899266e-06],
       [-8.33861463e-07,  6.69536746e-07, -3.77035371e-06,
         1.96441221e-07, -2.55714872e-06, -2.31563786e-06,
         1.13662782e-06,  6.09190010e-07,  1.55441671e-06,
         3.46389584e-07, -1.80225595e-06,  2.19308004e-06,
        -1.59420210e-06, -1.39821520e-06,  3.88585113e-06,
         4.31579792e-07, -3.37248184e-06, -3.69044585e-07,
         2.46623529e-07, -1.38136818e-06,  7.86342980e-07,
         9.57002158e-07,  1.31599211e-06, -2.29972613e-07,
         5.64795243e-07,  1.26086229e-06, -7.66056928e-07,
        -4.87666398e-07,  1.85014733e-06, -3.65217056e-06],
       [-1.35357004e-06, -3.30618292e-07, -1.04455603e-06,
        -2.97578055e-07,  1.00470473e-07,  8.72256578e-07,
        -2.39063615e-06, -2.28722479e-06,  1.50302833e-06,
        -9.70660381e-07,  2.08878123e-06, -4.38746213e-08,
         1.25249039e-06, -1.11295083e-06, -3.75748385e-07,
         2.37393670e-06,  2.49030018e-06,  9.18692251e-07,
        -8.74367402e-07,  5.12954159e-07,  3.08589279e-08,
        -1.56457929e-06, -7.52078677e-07, -1.23982431e-06,
         8.77614070e-08, -2.09542668e-06, -4.13917633e-07,
         1.16165637e-07, -3.37134134e-06,  1.47930018e-06],
       [ 9.34555430e-07, -4.33854723e-07,  1.33426499e-06,
         1.07544724e-06,  1.25779377e-06,  9.44680323e-07,
         1.99854213e-07,  2.22097952e-07,  2.99560554e-07,
         1.26773512e-06,  1.96369751e-06, -1.45823617e-06,
         1.39802069e-07,  1.43009925e-06,  1.37353169e-07,
        -3.74110755e-07,  9.11168627e-07,  2.92715924e-07,
         6.74246451e-07,  8.54447876e-07,  6.78340825e-07,
         5.84478755e-07, -1.31843080e-07, -6.49242793e-09,
         5.72344220e-07, -1.55400642e-07, -2.15295955e-07,
        -4.87695928e-09, -8.16755858e-07,  1.36375093e-06],
       [-8.77843718e-07,  9.58689839e-07, -1.69226496e-07,
         8.05155594e-07,  7.90080321e-08, -1.68780537e-06,
        -1.73178137e-07, -9.53620543e-07,  2.12271516e-06,
        -6.48216997e-07, -2.17047500e-06,  1.97418422e-06,
        -1.36347455e-06, -9.56567987e-07,  2.11573342e-06,
        -6.69858480e-09, -2.13715012e-07, -2.25397713e-07,
        -4.11013076e-07, -7.70371514e-07,  9.45921272e-07,
        -1.04026299e-08,  4.09874445e-07, -9.70777592e-09,
        -1.39109318e-06,  9.82254960e-07, -1.56747251e-06,
         7.31856886e-08, -1.28511840e-06, -2.75281059e-06],
       [ 2.34175263e-06,  1.47808362e-06, -3.47395235e-06,
        -3.29114300e-07, -3.64467178e-06, -6.87429804e-07,
        -1.36103517e-06, -7.49615083e-07,  1.10349788e-07,
        -6.94980486e-07, -3.02433614e-06,  4.60288959e-07,
        -4.54383581e-07, -2.18043283e-07, -4.10285736e-07,
        -1.43170809e-07,  1.45595698e-06, -6.74597402e-07,
         7.75123397e-07, -7.28399698e-07, -7.57650241e-07,
        -1.80070924e-06, -8.19576655e-07, -2.42408646e-07,
        -1.11528152e-06,  7.27619636e-08, -5.20507939e-08,
         7.06819037e-07, -1.03599433e-07, -1.13846147e-06],
       [-2.40239865e-06, -1.10743872e-06,  1.52984882e-07,
        -7.30118018e-07,  8.66452638e-07,  1.73561588e-07,
        -7.60532885e-07, -6.42882014e-07,  9.23801622e-07,
        -1.00644797e-06,  8.75982096e-07,  1.12859908e-07,
         6.64275603e-07, -1.05342008e-06,  6.73069906e-07,
         1.90496689e-06,  8.21287358e-08,  6.58574834e-07,
        -8.30287377e-07,  2.20479720e-07, -1.15672336e-07,
        -7.22770778e-07,  3.44426411e-07, -8.17463331e-07,
         4.44730375e-07, -7.83194082e-07, -2.98965432e-07,
        -4.04512321e-07, -1.15246451e-06,  5.01664260e-07],
       [-7.04996523e-07,  4.59333080e-07, -1.40145892e-06,
         5.43146825e-08, -5.30329146e-07, -1.68884117e-06,
         4.39509563e-07, -1.23299071e-06,  2.58629461e-06,
        -8.07090942e-07, -2.54375732e-06,  1.99096280e-06,
        -1.23811037e-06, -1.28287161e-06,  2.65810195e-06,
         1.07443748e-06, -1.02320553e-06,  1.13868907e-07,
        -5.07316599e-07, -8.48625973e-07,  7.51595735e-07,
         1.17988264e-07,  7.32372655e-07, -1.23476042e-07,
        -8.93527726e-07,  1.31578054e-06, -1.40270879e-06,
        -1.49545002e-07, -7.41357837e-07, -2.75316961e-06],
       [-2.65487006e-06, -2.29418276e-07,  1.25162535e-06,
         1.02185459e-06,  1.74726483e-06,  4.38103740e-07,
        -2.33773130e-06, -9.84935127e-07,  3.33997718e-07,
        -4.13916041e-07,  2.77761978e-06, -1.03355347e-07,
         7.03923035e-07, -4.38252982e-07, -1.36309382e-07,
         6.61950025e-07,  1.97372606e-06,  7.18115018e-07,
        -3.53332496e-07,  1.02715308e-06,  8.06866751e-07,
        -1.03407433e-06, -5.72533281e-07, -5.13822897e-07,
         1.70911932e-07, -1.73570982e-06, -7.28191822e-07,
        -3.12263666e-07, -2.42804526e-06,  1.18491903e-06],
       [-7.81774361e-07, -5.46663728e-08,  1.68550662e-06,
        -7.46972432e-07,  3.88162448e-07,  3.68912367e-07,
        -1.61027947e-06,  2.52425053e-07, -1.57323086e-06,
        -1.59068782e-06, -9.25640109e-07,  2.26414329e-07,
         1.95127328e-07, -7.74347313e-07, -2.61747368e-06,
        -5.89266335e-07,  1.27196427e-06, -3.76919985e-07,
        -7.03816909e-07, -1.85589698e-07, -9.86373379e-07,
        -7.01534077e-07, -6.43151509e-07,  3.80797673e-07,
        -9.86819146e-07, -6.73915395e-07,  4.55163189e-07,
         3.50562033e-07, -2.91569847e-07,  6.37682319e-07],
       [-1.12217913e-06, -1.81569249e-08, -5.76864181e-07,
        -1.25521899e-06, -6.73387149e-07, -1.66515372e-06,
         2.24510791e-06,  1.11548093e-06,  1.63744403e-07,
        -6.90617185e-07, -2.72113289e-06,  2.00530894e-06,
        -1.25301369e-06, -1.28492979e-06,  1.90523519e-06,
         1.56587703e-07, -3.74537012e-06, -4.74685805e-07,
        -8.13018346e-07, -1.31415118e-06, -3.26486798e-07,
         1.30975911e-06,  1.64716153e-06,  1.14066324e-07,
         6.38191864e-08,  1.51669246e-06,  9.80121939e-08,
        -2.85947323e-07,  2.57718193e-06, -2.85154829e-06],
       [ 6.59477450e-07, -1.53951618e-07,  2.39269866e-07,
         2.25092833e-07,  2.24805262e-07,  4.63334402e-07,
        -1.03470711e-06, -1.36627125e-06,  8.30752242e-07,
        -6.04885031e-07,  8.06428488e-08, -2.89359207e-07,
         5.75164620e-07,  3.41106471e-07, -1.30463923e-06,
         6.52876906e-07,  2.39817223e-06,  3.63122439e-07,
        -7.27585530e-08,  3.41536520e-07,  3.47921514e-09,
        -1.53747101e-06, -7.97610539e-07, -3.19425595e-07,
        -9.60973921e-07, -1.91756669e-07, -2.15553868e-07,
         3.17243575e-07, -2.16949525e-06,  1.18122921e-06],
       [-6.97707264e-07, -4.14639061e-07,  1.05831919e-06,
         1.09910900e-06,  1.59289016e-06, -6.22105802e-07,
         1.77149900e-07, -1.44598039e-06,  2.64514301e-06,
        -8.71825137e-08, -3.11500031e-07,  6.74868318e-07,
        -2.55433037e-07, -6.52890435e-08,  1.21658843e-06,
         9.42178588e-07,  7.76381512e-07,  7.85464408e-07,
        -4.24074557e-07,  1.90157479e-07,  1.25152246e-06,
        -4.38375253e-07,  2.57950859e-07, -5.94949370e-08,
        -8.67065296e-07,  8.22604932e-07, -1.39231588e-06,
        -9.31576523e-08, -2.41679209e-06, -3.41926807e-07],
       [-9.63977755e-07,  1.71897312e-07,  1.71806960e-07,
        -1.49743721e-06, -5.01443196e-07, -1.87505179e-07,
        -3.99071723e-07,  5.08229789e-07, -1.85957890e-06,
        -1.48518325e-06, -2.05435981e-06,  4.45849025e-07,
         7.27106908e-08, -1.19638673e-06, -1.46347918e-06,
        -4.27705686e-07, -6.64180448e-07, -7.34591083e-07,
        -7.12897190e-07, -6.86362966e-07, -1.29842238e-06,
        -3.77137440e-07, -1.42672548e-07,  3.27302473e-07,
        -5.66867698e-07,  1.28906663e-08,  7.13402073e-07,
         3.24462270e-08,  1.55808789e-06, -5.70098678e-07],
       [-2.01378452e-06, -1.00737782e-06,  1.54234601e-06,
         8.49531943e-07,  2.54033512e-06, -5.34723483e-08,
         6.75878368e-07,  4.18570778e-08,  1.24459052e-06,
         5.66463825e-07,  1.80034510e-06, -3.59124812e-07,
         5.61103377e-08,  2.05664605e-07,  1.94319591e-06,
         8.83183247e-07, -7.55046983e-07,  5.37704238e-07,
        -2.64287024e-07,  5.57937540e-07,  1.06621258e-06,
         5.37257733e-07,  7.90690933e-07, -3.43084082e-07,
         5.97841677e-07,  1.68072916e-07, -7.10499421e-07,
        -7.07031063e-07, -9.42653742e-07, -1.06776504e-07],
       [-1.37908228e-06, -1.21897904e-06,  2.91703145e-06,
         5.79736593e-07,  3.12000429e-06,  7.05460707e-07,
         3.85779117e-07,  2.52101358e-07, -9.30046298e-08,
         4.88758246e-07,  2.70713463e-06, -6.78644199e-07,
         5.76694333e-07,  5.63726189e-07, -2.09832208e-07,
         2.13264570e-07,  2.37486688e-08,  7.52129608e-07,
        -4.14860352e-07,  1.01315197e-06,  7.04682805e-07,
         8.13254474e-07,  2.70204595e-07,  5.76873163e-08,
         7.14246141e-07, -1.95993152e-07, -2.83804411e-08,
        -4.91333481e-07, -6.67052404e-07,  1.61591390e-06],
       [ 1.61081834e-06,  1.54908435e-07,  1.67589315e-06,
         3.57807380e-07, -1.83220095e-07,  9.34417358e-07,
        -2.31321678e-07,  1.75194509e-06, -3.01883620e-06,
         6.59038619e-07,  2.53161033e-07, -1.78391713e-06,
         1.00427485e-07,  1.53452334e-06, -2.76639821e-06,
        -2.80670861e-06,  8.40109522e-07, -7.75495096e-07,
         1.22410700e-06,  6.50346692e-07, -5.39646749e-07,
         1.68605681e-07, -7.33773163e-07,  9.43702275e-07,
        -1.40748398e-07,  1.41900330e-07,  1.08899928e-06,
         2.27347485e-07,  1.63432685e-06,  1.54010161e-06],
       [ 2.37096856e-06,  2.64355265e-07, -9.65544587e-08,
        -8.81682013e-07, -6.07294737e-07, -4.25695987e-07,
         1.60355421e-06,  2.37930408e-07,  3.38585579e-07,
        -2.31141314e-07, -2.58766340e-06,  6.42582791e-07,
        -7.86775786e-07,  6.54271162e-08, -2.31198001e-07,
        -3.50713549e-07, -9.91388561e-07, -5.27628117e-07,
        -1.84053846e-07, -9.26716552e-07, -5.29377530e-07,
         9.33731144e-07,  2.60289312e-07,  4.65017138e-07,
        -8.52207222e-07,  1.45697118e-06,  2.45941294e-07,
         4.82473922e-07,  7.96738448e-07, -9.14234874e-07],
       [-1.18012724e-06,  4.75108557e-08, -7.89849537e-07,
        -6.34609592e-07, -1.25553129e-06, -1.15257671e-06,
         1.52549546e-06,  2.21902587e-06, -1.24481664e-06,
         1.73551541e-07, -1.81673204e-06,  7.33568299e-07,
        -1.05960009e-06, -8.30883948e-07,  1.74061017e-06,
        -7.63946218e-07, -3.55902421e-06, -9.28759221e-07,
        -6.30766976e-08, -1.15833905e-06, -3.34573429e-07,
         1.46527134e-06,  1.33500771e-06,  3.58038960e-07,
         6.53777420e-07,  8.97628809e-07,  3.34776985e-07,
        -3.49262592e-07,  3.27409975e-06, -2.65798849e-06]], dtype=float32)>, <tf.Tensor: shape=(30, 30), dtype=float32, numpy=
array([[-1.68545307e-06,  1.77334493e-06, -1.96799874e-06,
         1.93730648e-06, -1.35922710e-06, -2.09659515e-06,
        -1.08843892e-06, -6.75865010e-07,  1.33313176e-06,
         5.03706588e-08, -1.55296186e-06,  1.61526077e-06,
        -1.86037937e-06, -6.75247009e-07,  3.02938224e-06,
        -1.04388414e-06, -1.79854482e-07, -4.29829186e-07,
         8.16441798e-07, -3.36356266e-07,  1.62500714e-06,
        -3.43558554e-07,  2.52238380e-07,  4.20324625e-07,
        -7.06137939e-07,  9.47578883e-07, -1.91387016e-06,
        -3.53876800e-07,  1.09427447e-07, -3.53108317e-06],
       [-2.67781888e-06, -4.86248950e-07,  4.96355824e-06,
        -1.72515570e-07,  2.80270638e-06,  7.07345009e-08,
        -7.42810983e-07,  1.99869669e-06, -2.63779066e-06,
        -1.20332277e-06,  2.39762130e-08,  6.79253560e-08,
        -3.10002576e-07, -7.43598662e-07, -1.91020195e-06,
        -2.16687818e-06, -3.93747314e-07, -1.08714016e-06,
        -1.26498264e-06, -2.27959589e-07, -6.23590722e-07,
         5.95167421e-07,  2.03058335e-08,  8.72723604e-07,
        -6.28480905e-07, -7.49666128e-07,  3.10311151e-07,
         3.61778234e-08,  4.21111849e-07, -2.51681115e-07],
       [-1.66629616e-06,  1.28334875e-06,  1.60235186e-06,
         7.65011748e-07,  1.36408858e-07, -9.33316642e-07,
        -2.62863114e-06, -4.89778472e-07, -8.72406645e-07,
        -1.85693784e-06, -2.08134134e-06,  1.03040975e-06,
        -1.20145774e-06, -1.33091123e-06, -1.29042064e-06,
        -2.09948621e-06,  1.86224202e-06, -9.28532700e-07,
        -3.16826231e-07, -1.82999088e-07,  2.72265765e-07,
        -1.00674049e-06, -1.03341415e-06,  1.15930993e-06,
        -2.01391776e-06,  4.08533083e-08, -1.19155038e-06,
         1.19980882e-07, -8.07324227e-07, -1.54420582e-06],
       [-1.60693389e-06,  5.73017019e-07, -4.50169000e-06,
         1.23692644e-07, -3.29136992e-06, -3.40139223e-07,
        -3.68607562e-06, -1.91797585e-06,  1.38173311e-06,
        -1.22717131e-06,  2.94948620e-07,  1.43448730e-07,
         2.84252394e-07, -1.66719485e-06,  1.24114479e-06,
         1.81493419e-06,  2.31317858e-06,  1.83020063e-07,
         4.12430097e-07, -9.11286691e-08,  1.47893104e-07,
        -2.68831127e-06, -8.60994703e-07, -1.28650856e-06,
         4.43379520e-08, -1.84703958e-06, -1.10850669e-06,
        -2.39649523e-07, -2.15441287e-06, -5.51698122e-07],
       [-2.51461279e-06, -3.14055228e-07, -5.65661685e-06,
        -9.17069656e-07, -3.24703979e-06,  7.95842539e-07,
        -4.77555568e-06, -2.87550438e-06,  1.45327476e-06,
        -1.71436477e-06,  2.02934871e-06, -5.23388053e-07,
         1.79035874e-06, -2.20825746e-06,  8.71576219e-07,
         4.29108468e-06,  3.05821982e-06,  1.16134606e-06,
         2.66388440e-08,  2.81346161e-07, -5.74320097e-07,
        -3.54447138e-06, -8.38578217e-07, -2.44780722e-06,
         1.08629217e-06, -3.76303456e-06, -3.36681438e-07,
        -3.04202018e-07, -3.19822720e-06,  9.16206432e-07],
       [ 2.44236935e-06,  7.68192763e-08,  4.97860765e-06,
         4.78243635e-07,  2.74553008e-06, -7.86814553e-07,
         4.61776017e-06,  3.00470174e-06, -1.43972693e-06,
         1.63532468e-06, -1.71161821e-06,  5.42348289e-07,
        -1.18620767e-06,  2.01104558e-06, -8.12842131e-07,
        -3.19764422e-06, -3.31566366e-06, -8.65642221e-07,
        -2.97791189e-07, -7.36400011e-07,  1.86479951e-08,
         3.09502593e-06,  1.13489136e-06,  1.59659737e-06,
        -6.03448314e-07,  2.79921233e-06,  8.10444192e-07,
         4.28497572e-07,  3.03839897e-06, -7.84900749e-07],
       [-2.28006979e-06, -8.03777766e-07,  3.01970726e-06,
        -1.89115474e-06,  1.67261828e-06,  7.91853381e-07,
        -1.00449142e-06,  9.23849541e-07, -2.25632925e-06,
        -2.29568559e-06, -8.50580761e-07,  7.93027652e-08,
         4.03359820e-07, -1.64066478e-06, -2.29683974e-06,
        -1.56733790e-07, -3.21627454e-07, -5.57431235e-07,
        -1.69536600e-06, -2.27485202e-07, -1.55332896e-06,
         2.64924097e-07, -5.25139399e-08,  3.36951132e-07,
        -6.26090923e-07, -7.98898441e-07,  7.91094976e-07,
        -1.02820508e-08,  3.39619419e-07,  2.55593420e-07],
       [-1.99976125e-06, -5.49995775e-07, -4.09227459e-06,
        -4.27577447e-07, -1.81537575e-06, -2.32987190e-07,
        -9.69272719e-07, -1.34557706e-06,  2.01853186e-06,
        -1.09580242e-07,  1.51249924e-06,  4.16336292e-07,
         7.83012410e-07, -1.18592493e-06,  2.75631987e-06,
         3.34848642e-06, -3.80376264e-07,  1.10078031e-06,
        -2.32022046e-08,  2.58960426e-08,  4.15136924e-07,
        -1.05402364e-06,  6.44349598e-07, -1.69471264e-06,
         1.46966363e-06, -1.24397286e-06, -3.21542871e-07,
        -5.89573403e-07, -9.47097419e-07, -2.99373141e-07],
       [-1.20034565e-06, -2.59942340e-07, -3.96379346e-06,
         2.44090700e-07, -2.69523844e-06,  8.05327289e-08,
        -2.83645704e-06, -1.18881758e-06,  1.46477612e-06,
        -3.98728076e-07,  1.82728354e-06, -3.88980197e-07,
         7.35190611e-07, -1.07117455e-06,  1.45408421e-06,
         2.29447346e-06,  1.68732640e-06,  1.55783198e-07,
         2.44496022e-07, -3.87087766e-08,  1.07222164e-07,
        -2.10370990e-06, -6.89703086e-07, -1.77648599e-06,
         6.19351397e-07, -2.14708371e-06, -6.23528422e-07,
        -3.03773788e-07, -2.33553146e-06,  3.76188609e-07],
       [-4.40669993e-07,  1.35789242e-06, -2.16893636e-06,
         5.34759010e-07, -2.17437696e-06, -1.21734877e-06,
        -1.26783470e-06,  3.04907388e-08,  1.25511974e-08,
        -4.88089768e-07, -1.44760929e-06,  9.27714098e-07,
        -9.66933612e-07, -8.08879577e-07,  1.27079170e-06,
        -7.63761705e-07,  8.12925975e-08, -7.62461298e-07,
         5.37555593e-07, -5.97329006e-07,  2.64487880e-07,
        -6.48019295e-07, -2.21391232e-07, -1.22298758e-08,
        -5.37844471e-07,  6.67033646e-08, -6.60742899e-07,
         1.51027919e-08,  3.69758567e-07, -2.28513863e-06],
       [ 2.77038112e-06,  1.25086183e-06, -8.03257535e-07,
         1.85214537e-06, -8.80776611e-07, -6.05600803e-07,
         6.47384866e-07, -7.86160967e-08,  7.62970558e-07,
         1.77053585e-06, -3.03412492e-07, -2.16242057e-07,
        -7.97630662e-07,  1.82347912e-06,  6.33261948e-07,
        -1.53874612e-06,  8.40120322e-07, -3.30083196e-07,
         1.40484326e-06,  1.39899470e-07,  1.03570824e-06,
        -6.90760942e-08, -4.50964762e-07,  3.09725522e-07,
        -5.15737156e-07,  1.05611434e-06, -6.58960971e-07,
         3.13030370e-07, -1.50876360e-07, -3.02175408e-07],
       [ 8.10335450e-07,  1.98310303e-07, -3.64529456e-06,
         8.54328732e-07, -1.68852762e-06,  3.33130060e-07,
        -1.22163726e-06, -1.87980618e-06,  2.04733760e-06,
         9.83447649e-07,  1.77360710e-06, -6.81697884e-07,
         6.48684818e-07,  4.68686864e-07,  1.61932235e-06,
         2.03067088e-06,  1.63740651e-06,  9.80825689e-07,
         1.10014059e-06,  5.67664813e-07,  8.87022793e-07,
        -1.13480837e-06, -4.37994800e-07, -1.06119603e-06,
         8.16384727e-07, -7.08859602e-07, -5.54776932e-07,
        -1.33712746e-07, -1.72957698e-06,  7.02566695e-07],
       [ 2.64705773e-06, -7.16123338e-07, -4.53481880e-06,
        -2.92139418e-07, -1.75423747e-06,  1.42015028e-06,
        -2.87449325e-07, -3.13691385e-06,  3.77288507e-06,
         1.20939023e-06,  2.69173051e-06, -1.75525940e-06,
         2.05946253e-06,  1.00105967e-06,  1.19162655e-06,
         3.91499316e-06,  2.70297232e-06,  1.73767000e-06,
         7.92654760e-07,  8.66506525e-07,  3.53739409e-07,
        -1.87370802e-06, -6.40444398e-07, -2.15519094e-06,
         1.08211316e-06, -1.18631033e-06, -1.88518854e-07,
         2.31306032e-07, -3.64060429e-06,  3.04328819e-06],
       [-1.47615594e-07, -1.24920007e-06,  4.12660029e-06,
        -2.60018652e-07,  2.96266762e-06,  2.22725504e-07,
         1.93216033e-06,  1.54185113e-06, -5.94898950e-07,
         3.06918537e-07,  8.64073399e-07, -2.30354203e-07,
         4.02457175e-07,  8.57151065e-07, -1.42293766e-06,
        -8.37489040e-07, -8.24087579e-07, -1.61045733e-07,
        -9.06257185e-07, -1.66770690e-07, -4.34481876e-07,
         8.21639901e-07,  5.75812692e-07,  4.59919534e-08,
        -9.37072571e-08,  2.04257930e-07,  6.76954357e-07,
         2.86619326e-07, -1.16664523e-07,  1.23879090e-06],
       [ 7.60388104e-07,  1.37430493e-06,  6.52811082e-07,
         8.13872475e-07, -4.44778323e-07, -6.23867663e-07,
        -3.90693913e-08,  1.73266670e-07, -8.09230073e-07,
        -1.00615523e-07, -2.59702574e-06,  6.12178155e-07,
        -1.11182248e-06,  4.07411648e-07, -3.07253799e-07,
        -1.99851456e-06,  2.72096202e-07, -7.73450779e-07,
         4.78606012e-07, -3.62498781e-07,  1.95417172e-07,
         2.20341903e-07, -1.07142981e-07,  1.05863251e-06,
        -1.45499394e-06,  1.21183518e-06, -1.83220706e-07,
         4.21968394e-07,  7.88592160e-07, -1.82529277e-06],
       [ 1.84871965e-06, -1.00850389e-06,  1.21057496e-07,
        -1.63851723e-06,  3.43530417e-07,  1.96736124e-07,
         2.91993956e-06,  4.15618558e-07,  7.46274964e-07,
         1.88332919e-07, -1.25521694e-06, -9.74790737e-09,
         5.41554982e-07,  4.74898343e-07,  3.90564310e-08,
         1.26378109e-06, -1.85424187e-06,  1.36125493e-07,
        -6.57879468e-07, -7.63950766e-07, -8.87705426e-07,
         7.90384547e-07,  9.78798880e-07, -5.79609662e-07,
         3.95516508e-09,  1.25581437e-06,  1.04460787e-06,
         2.58817039e-07,  5.96449127e-07,  3.67176767e-07],
       [-3.40080055e-06, -7.36076800e-07, -6.18425872e-07,
        -1.50998028e-06,  3.01111982e-07,  9.68943823e-07,
        -3.56400801e-06, -1.51496579e-06, -3.02681997e-07,
        -2.55073928e-06,  1.00614147e-06, -2.84729481e-07,
         1.49284654e-06, -2.31742797e-06, -9.40268990e-07,
         2.62448430e-06,  2.02942010e-06,  5.27391535e-07,
        -1.16878698e-06,  3.98671176e-07, -1.00509249e-06,
        -2.05977267e-06, -7.53196389e-07, -1.07564551e-06,
         1.70271619e-09, -2.49306277e-06, -9.28486656e-08,
        -3.08787122e-07, -2.18040850e-06,  9.71911732e-07],
       [ 1.08933989e-06, -6.72381475e-07,  1.99502460e-06,
         5.57835392e-07,  2.32340381e-06,  6.15008048e-07,
         2.56377348e-06,  6.24308427e-07,  8.81585720e-08,
         1.68063298e-06,  1.50179858e-06, -6.72684905e-07,
         2.51919346e-07,  1.58491832e-06,  1.07982544e-07,
        -2.03666914e-07, -1.12701366e-06,  6.74180171e-07,
         2.57656268e-07,  6.67108509e-07,  5.74196633e-07,
         1.65649806e-06,  5.64386482e-07,  4.72394248e-07,
         7.82273673e-07,  1.03484683e-06,  3.69540032e-07,
        -1.86812628e-07,  7.24218239e-07,  1.09499524e-06],
       [ 2.05738570e-06, -1.43839623e-06, -1.43534180e-08,
        -1.74121294e-06,  5.86215776e-07,  1.18831792e-06,
         2.38567600e-06, -3.37201413e-08,  6.75523438e-07,
         5.50848711e-07,  5.60383398e-07, -8.31402303e-07,
         1.35478797e-06,  8.06204412e-07, -7.40866369e-07,
         2.00209388e-06, -9.92031005e-07,  8.09284984e-07,
        -5.26246083e-07, -5.93267657e-08, -1.00078512e-06,
         6.33562934e-07,  6.01570264e-07, -7.68417920e-07,
         8.90159072e-07,  1.38628494e-07,  1.31893125e-06,
         3.06727429e-07,  6.73157388e-08,  2.15252112e-06],
       [ 6.77074013e-07,  7.96906079e-08,  2.62389972e-06,
         9.51504319e-07,  1.94085328e-06, -1.11661484e-06,
         2.89512468e-06,  1.09383791e-06,  6.13495445e-07,
         9.05333138e-07, -1.30963622e-06,  1.04967182e-06,
        -1.28325769e-06,  1.03061416e-06,  9.25559618e-07,
        -1.59983188e-06, -1.92586390e-06, -3.90464834e-07,
        -1.50507788e-07, -4.33635989e-07,  8.90521619e-07,
         1.80686959e-06,  9.19249089e-07,  8.12890733e-07,
        -9.23563675e-07,  2.50791709e-06, -3.78505149e-07,
         2.68186646e-08,  8.58346311e-07, -1.41369276e-06],
       [-2.68621420e-06,  1.03474605e-07,  1.74505806e-06,
        -6.79343884e-07,  9.10528684e-07, -5.70818884e-07,
         1.75759538e-07,  1.33692083e-06, -1.38251892e-06,
        -8.49567357e-07, -8.29451778e-07,  7.49436651e-07,
        -4.62458303e-07, -1.32465993e-06,  2.69892155e-07,
        -1.16718661e-06, -1.83834504e-06, -6.40412907e-07,
        -9.15382429e-07, -4.16139642e-07, -3.61344206e-07,
         8.22218340e-07,  6.70574536e-07,  4.74819217e-07,
        -4.98320176e-08,  1.35395283e-07, -6.74782896e-10,
        -2.98253752e-07,  1.50209223e-06, -1.45546016e-06],
       [ 1.75295327e-06, -9.15759983e-07, -3.03808633e-06,
        -3.55612599e-07, -1.20528034e-06,  1.20460061e-06,
         7.94797984e-07, -7.43666476e-07,  1.40713519e-06,
         1.62321066e-06,  2.86777072e-06, -1.51824668e-06,
         1.66105531e-06,  1.07387882e-06,  7.06066430e-07,
         2.44143939e-06,  3.14951535e-07,  1.14238367e-06,
         6.16379793e-07,  5.62166008e-07, -1.37473847e-07,
        -4.34044097e-07, -2.80277845e-08, -1.39363965e-06,
         1.92282278e-06, -8.96253653e-07,  6.67773350e-07,
        -6.23980299e-08, -6.11519852e-07,  2.51370830e-06],
       [-1.20254208e-06, -3.54944746e-07,  3.22585356e-06,
        -1.12165981e-06,  1.69428631e-06, -1.54294014e-07,
         7.84774500e-07,  1.37931568e-06, -1.81330802e-06,
        -1.25295935e-06, -1.62486742e-06,  5.55590589e-07,
        -4.52027109e-07, -7.77571302e-07, -1.50758683e-06,
        -1.24105475e-06, -1.40696068e-06, -5.92667675e-07,
        -1.11340819e-06, -4.40344280e-07, -8.55142616e-07,
         9.88780130e-07,  4.27879627e-07,  1.06367702e-06,
        -5.67655434e-07,  6.77025810e-07,  4.94307301e-07,
        -1.80600352e-08,  1.65993845e-06, -5.81323718e-07],
       [ 2.82347241e-06, -3.21471987e-07,  2.55354144e-06,
        -1.06898483e-06,  1.66148220e-06, -3.95428117e-08,
         3.96559335e-06,  1.34769266e-06, -7.11720759e-07,
         5.39503162e-07, -2.17414890e-06,  5.23831375e-07,
        -5.47200045e-07,  1.10384292e-06, -1.08325082e-06,
        -1.05871629e-06, -2.36455321e-06, -1.19219465e-07,
        -4.35537430e-07, -4.95784207e-07, -6.34777848e-07,
         2.23687107e-06,  1.00151681e-06,  1.19468234e-06,
        -3.59354715e-07,  2.44273383e-06,  1.20493723e-06,
         4.29229146e-07,  2.49641403e-06, -2.88877914e-07],
       [-2.24854966e-06, -7.74267960e-07, -4.80805966e-07,
        -3.45892147e-07,  3.09863708e-07,  2.30288322e-07,
        -8.55405005e-07,  3.23116296e-07, -1.09231081e-07,
        -1.42768442e-07,  2.24107498e-06, -2.49394475e-07,
         5.07518735e-07, -1.14240663e-06,  9.05043407e-07,
         1.10926749e-06, -9.44747285e-07,  3.84146574e-07,
        -4.29374779e-07,  3.92178976e-07,  1.89165249e-07,
         4.12966926e-07,  1.12787042e-07, -4.19565424e-07,
         1.50466894e-06, -1.32004857e-06, -8.80273348e-08,
        -6.34774608e-07,  1.49099890e-07,  5.46759622e-07],
       [ 3.07421783e-06,  2.39622295e-08,  5.90835498e-07,
         1.00717261e-06,  6.64388608e-07, -2.79014330e-07,
         3.24304483e-06,  3.27894440e-07,  6.77801268e-07,
         1.91382401e-06, -8.23382379e-07,  2.02706801e-07,
        -8.78251115e-07,  1.82248482e-06,  5.21423715e-07,
        -8.74711191e-07, -1.51107474e-06,  4.30865498e-07,
         8.14693067e-07,  5.64647280e-08,  7.31524608e-07,
         1.91100617e-06,  6.31450973e-07,  1.21103801e-06,
        -3.44181217e-08,  2.43239174e-06,  2.42594439e-07,
         6.08070536e-08,  1.65020901e-06, -4.23874354e-07],
       [ 1.81149574e-06,  8.13226734e-07,  1.57614181e-06,
         1.14378565e-06,  5.78201423e-07, -8.28351972e-07,
         2.66323718e-06,  1.49862331e-06, -3.74935894e-07,
         1.45128649e-06, -1.37255006e-06,  5.16017167e-07,
        -1.49254379e-06,  1.42496901e-06,  5.58410136e-07,
        -2.58345290e-06, -1.88964987e-06, -7.19966863e-07,
         6.19311209e-07, -3.45244359e-07,  6.56092027e-07,
         2.00154341e-06,  6.22667187e-07,  1.35579648e-06,
        -5.99296015e-07,  2.26399629e-06, -5.10389668e-08,
         1.53184061e-07,  2.06718278e-06, -1.49690504e-06],
       [-2.84122893e-06,  2.11191377e-06, -1.19547849e-06,
         9.76780598e-07, -1.77703396e-06, -1.74300521e-06,
        -3.12577004e-06, -5.85278144e-07, -6.17234889e-07,
        -1.91182085e-06, -2.83886948e-06,  1.78057667e-06,
        -1.89795355e-06, -2.24743644e-06,  1.10868677e-06,
        -1.69375016e-06,  6.55258077e-07, -8.86332089e-07,
         1.16622743e-07, -1.53108772e-07,  6.51444111e-07,
        -9.58412102e-07, -4.62926039e-07,  1.10344467e-06,
        -1.16483511e-06, -1.11287818e-07, -1.73893784e-06,
        -3.77817685e-07,  5.55662382e-07, -3.67210851e-06],
       [-8.02515387e-07, -3.82780513e-07, -5.12092356e-07,
        -1.77638867e-06, -1.05515869e-06,  9.20157731e-07,
        -1.04800756e-06,  5.26862834e-07, -1.77943241e-06,
        -1.10514566e-06, -7.05767889e-08, -6.71129783e-07,
         1.00053421e-06, -8.57342684e-07, -1.59774481e-06,
         4.80423068e-07,  1.34403820e-07, -2.45298395e-07,
        -4.34429069e-07, -2.06472066e-07, -1.61824710e-06,
        -5.70015914e-07, -9.22919980e-08, -5.14684984e-07,
         5.67244797e-07, -1.66303982e-06,  1.13110877e-06,
         2.05410444e-07,  6.61723902e-07,  8.70051451e-07],
       [ 1.03141645e-06, -4.56217037e-08, -4.98055306e-06,
         7.54693360e-07, -2.33805463e-06,  4.88751311e-07,
        -1.28260388e-06, -2.32246930e-06,  2.51992492e-06,
         1.35946993e-06,  2.48094693e-06, -9.44373824e-07,
         1.05928916e-06,  5.82290284e-07,  2.09754171e-06,
         2.91468541e-06,  1.68007182e-06,  1.23911252e-06,
         1.35474329e-06,  6.85965688e-07,  9.68698714e-07,
        -1.41133239e-06, -4.90748391e-07, -1.42922363e-06,
         1.37465565e-06, -1.01559181e-06, -4.90011189e-07,
        -2.70840331e-07, -1.89321668e-06,  1.34099616e-06]], dtype=float32)>, <tf.Tensor: shape=(60,), dtype=float32, numpy=
array([-1.3756942e-06,  1.4820745e-06, -1.9139850e-06,  1.0875466e-06,
       -1.1812834e-06, -1.8542153e-06, -1.2561278e-06, -1.3401664e-06,
        1.9534209e-06, -8.3260721e-07, -2.0547770e-06,  1.8357763e-06,
       -1.4405165e-06, -1.3431387e-06,  2.5742088e-06,  6.1267741e-08,
        1.7217093e-07, -3.1906580e-07,  3.4865664e-08, -5.8420528e-07,
        1.1461365e-06, -7.1816834e-07,  3.0404557e-08, -2.6318183e-08,
       -1.1891422e-06,  6.5349542e-07, -1.8671947e-06, -2.0867216e-07,
       -1.0687841e-06, -3.2141156e-06, -9.9229180e-07,  1.1329012e-06,
        5.4741361e-07,  1.3088709e-06, -4.2776421e-07, -2.0040453e-07,
       -1.7035894e-06,  9.0837648e-07, -2.0644184e-06,  2.0860629e-07,
        4.5320652e-07, -3.0295439e-07, -6.0556738e-07,  1.7687347e-07,
       -6.8087530e-07, -2.4947008e-06,  8.6075761e-07, -7.7867043e-07,
        9.0810755e-07,  4.1477458e-07,  3.7646473e-07, -2.0912171e-07,
       -7.1674424e-07,  8.5621377e-07, -1.6972857e-07, -4.2629304e-07,
       -2.4310640e-07, -1.0416397e-07,  9.5429220e-07, -4.9570860e-07],
      dtype=float32)>, <tf.Tensor: shape=(60, 30), dtype=float32, numpy=
array([[-4.9279339e-07,  1.1637659e-06,  6.5672305e-07, ...,
         1.7762579e-07,  1.4256512e-06,  1.7134319e-08],
       [-2.5372395e-07,  3.5679810e-07,  1.0805541e-06, ...,
         3.7784025e-07,  1.1846673e-06,  4.6936094e-07],
       [-5.8642695e-07,  1.6142006e-06,  2.0786874e-06, ...,
         5.7254783e-07,  1.1952004e-06, -1.4419717e-06],
       ...,
       [-2.0927321e-07,  1.6653698e-07, -1.1769171e-06, ...,
         5.9385947e-08, -9.5658197e-07, -1.1478498e-06],
       [-2.3047062e-07,  1.9937212e-07, -1.1912040e-06, ...,
         6.1016550e-08, -9.8730618e-07, -1.1923461e-06],
       [ 3.5938251e-07,  3.7993107e-07, -8.3931042e-08, ...,
        -1.1974777e-07, -1.3633892e-06, -9.4085237e-07]], dtype=float32)>, <tf.Tensor: shape=(60, 30), dtype=float32, numpy=
array([[ 1.5170147e-07,  1.4617152e-06, -4.4970136e-08, ...,
         2.7459984e-07, -2.4870067e-06,  4.3780972e-07],
       [-2.8989220e-08,  6.9915234e-07,  1.1288039e-07, ...,
         7.0865889e-07, -2.1907617e-06,  2.5141441e-07],
       [ 7.1094888e-07,  3.1603877e-06, -4.5387196e-07, ...,
         8.8025160e-07, -9.8465625e-07, -2.8273669e-06],
       ...,
       [ 4.9220813e-07,  1.5031454e-06, -3.6956317e-07, ...,
        -8.0158424e-07,  1.6678780e-06, -1.6201839e-06],
       [ 5.1173618e-07,  1.5790283e-06, -3.8559364e-07, ...,
        -8.1142730e-07,  1.7286274e-06, -1.6671072e-06],
       [ 4.0959543e-07,  1.6731833e-06, -3.0258843e-07, ...,
         2.2223041e-07,  2.4270623e-06, -1.7743026e-06]], dtype=float32)>, <tf.Tensor: shape=(60,), dtype=float32, numpy=
array([ 2.6964523e-07, -2.5877068e-07,  1.2161631e-06, -1.9621475e-06,
       -1.6903352e-06,  1.3600472e-06, -2.4463136e-06, -2.4359645e-07,
        2.4882793e-06, -1.0229317e-06,  5.1274917e-08, -1.4681776e-06,
        7.3589621e-07, -1.2877779e-06,  1.5818982e-06, -4.1624409e-07,
       -1.9833860e-06,  9.8020948e-08, -3.1221560e-07,  7.9686214e-07,
        5.2212795e-07,  1.2903122e-06, -1.2208906e-06, -1.1450236e-06,
       -1.5960059e-07,  1.4734538e-06, -3.1775784e-07, -6.4191056e-08,
        1.0400598e-06,  1.2721181e-06,  5.4659569e-07,  1.7149437e-06,
       -4.1429243e-07, -3.1576508e-06,  2.2476377e-07, -1.0049324e-06,
        4.5950407e-07,  1.3669481e-06, -6.2735808e-07, -8.3790633e-07,
       -2.5778263e-06, -1.1289210e-06, -1.6994702e-06, -5.7168978e-07,
        2.0565988e-06, -9.6872066e-07, -9.1118875e-07,  1.3216192e-06,
       -2.3783048e-06,  1.3756196e-06,  2.0601701e-06,  9.3158224e-07,
        2.9873004e-06, -4.3680441e-07, -1.2541209e-06,  1.8102144e-06,
        1.2332714e-06, -8.2871412e-07,  1.8354007e-06, -1.7490738e-06],
      dtype=float32)>]
[Actor] Episode 7 Step 1 Loss: 334.4659118652344
Y Pred
tf.Tensor(
[[[        nan         nan         nan ...         nan         nan
           nan]
  [ 0.05421898 -0.02461588  0.00534585 ... -0.01262047  0.00242397
   -0.0381892 ]
  [ 0.05421898 -0.02461588  0.00534585 ... -0.01262047  0.00242397
   -0.0381892 ]
  ...
  [ 0.05421898 -0.02461588  0.00534585 ... -0.01262047  0.00242397
   -0.0381892 ]
  [ 0.05421898 -0.02461588  0.00534585 ... -0.01262047  0.00242397
   -0.0381892 ]
  [ 0.05421898 -0.02461588  0.00534585 ... -0.01262047  0.00242397
   -0.0381892 ]]

 [[        nan         nan         nan ...         nan         nan
           nan]
  [ 0.05421898 -0.02461588  0.00534585 ... -0.01262047  0.00242397
   -0.0381892 ]
  [ 0.05421898 -0.02461588  0.00534585 ... -0.01262047  0.00242397
   -0.0381892 ]
  ...
  [ 0.05421898 -0.02461588  0.00534585 ... -0.01262047  0.00242397
   -0.0381892 ]
  [ 0.05421898 -0.02461588  0.00534585 ... -0.01262047  0.00242397
   -0.0381892 ]
  [ 0.05421898 -0.02461588  0.00534585 ... -0.01262047  0.00242397
   -0.0381892 ]]

 [[        nan         nan         nan ...         nan         nan
           nan]
  [ 0.05421898 -0.02461588  0.00534585 ... -0.01262047  0.00242397
   -0.0381892 ]
  [ 0.05421898 -0.02461588  0.00534585 ... -0.01262047  0.00242397
   -0.0381892 ]
  ...
  [ 0.05421898 -0.02461588  0.00534585 ... -0.01262047  0.00242397
   -0.0381892 ]
  [ 0.05421898 -0.02461588  0.00534585 ... -0.01262047  0.00242397
   -0.0381892 ]
  [ 0.05421898 -0.02461588  0.00534585 ... -0.01262047  0.00242397
   -0.0381892 ]]

 ...

 [[        nan         nan         nan ...         nan         nan
           nan]
  [ 0.05421898 -0.02461588  0.00534585 ... -0.01262047  0.00242397
   -0.0381892 ]
  [ 0.05421898 -0.02461588  0.00534585 ... -0.01262047  0.00242397
   -0.0381892 ]
  ...
  [ 0.05421898 -0.02461588  0.00534585 ... -0.01262047  0.00242397
   -0.0381892 ]
  [ 0.05421898 -0.02461588  0.00534585 ... -0.01262047  0.00242397
   -0.0381892 ]
  [ 0.05421898 -0.02461588  0.00534585 ... -0.01262047  0.00242397
   -0.0381892 ]]

 [[        nan         nan         nan ...         nan         nan
           nan]
  [ 0.05421898 -0.02461588  0.00534585 ... -0.01262047  0.00242397
   -0.0381892 ]
  [ 0.05421898 -0.02461588  0.00534585 ... -0.01262047  0.00242397
   -0.0381892 ]
  ...
  [ 0.05421898 -0.02461588  0.00534585 ... -0.01262047  0.00242397
   -0.0381892 ]
  [ 0.05421898 -0.02461588  0.00534585 ... -0.01262047  0.00242397
   -0.0381892 ]
  [ 0.05421898 -0.02461588  0.00534585 ... -0.01262047  0.00242397
   -0.0381892 ]]

 [[        nan         nan         nan ...         nan         nan
           nan]
  [ 0.05421898 -0.02461588  0.00534585 ... -0.01262047  0.00242397
   -0.0381892 ]
  [ 0.05421898 -0.02461588  0.00534585 ... -0.01262047  0.00242397
   -0.0381892 ]
  ...
  [ 0.05421898 -0.02461588  0.00534585 ... -0.01262047  0.00242397
   -0.0381892 ]
  [ 0.05421898 -0.02461588  0.00534585 ... -0.01262047  0.00242397
   -0.0381892 ]
  [ 0.05421898 -0.02461588  0.00534585 ... -0.01262047  0.00242397
   -0.0381892 ]]], shape=(1125, 350, 12), dtype=float32)
omega
tf.Tensor(
[[0.]
 [0.]
 [0.]
 ...
 [0.]
 [0.]
 [0.]], shape=(1125, 1), dtype=float32)
mu
tf.Tensor(
[[nan nan nan ... nan nan nan]
 [nan nan nan ... nan nan nan]
 [nan nan nan ... nan nan nan]
 ...
 [nan nan nan ... nan nan nan]
 [nan nan nan ... nan nan nan]
 [nan nan nan ... nan nan nan]], shape=(1125, 12), dtype=float32)
mean
tf.Tensor(
[[nan nan nan ... nan nan nan]
 [nan nan nan ... nan nan nan]
 [nan nan nan ... nan nan nan]
 ...
 [nan nan nan ... nan nan nan]
 [nan nan nan ... nan nan nan]
 [nan nan nan ... nan nan nan]], shape=(1125, 12), dtype=float32)
grads
[<tf.Tensor: shape=(6, 30), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(30,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan], dtype=float32)>, <tf.Tensor: shape=(30, 60), dtype=float32, numpy=
array([[nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       ...,
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(60,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan], dtype=float32)>, <tf.Tensor: shape=(60, 30), dtype=float32, numpy=
array([[nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       ...,
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(30,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan], dtype=float32)>, <tf.Tensor: shape=(30, 25), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],
      dtype=float32)>, <tf.Tensor: shape=(25,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
      dtype=float32)>, <tf.Tensor: shape=(25, 12), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],
      dtype=float32)>, <tf.Tensor: shape=(12,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
      dtype=float32)>, <tf.Tensor: shape=(30, 25), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],
      dtype=float32)>, <tf.Tensor: shape=(25,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
      dtype=float32)>, <tf.Tensor: shape=(25, 12), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],
      dtype=float32)>, <tf.Tensor: shape=(12,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
      dtype=float32)>, <tf.Tensor: shape=(30, 1), dtype=float32, numpy=
array([[nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan]], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>, <tf.Tensor: shape=(34, 40), dtype=float32, numpy=
array([[nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       ...,
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(40,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan], dtype=float32)>, <tf.Tensor: shape=(40, 80), dtype=float32, numpy=
array([[nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       ...,
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(80,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan], dtype=float32)>, <tf.Tensor: shape=(80, 30), dtype=float32, numpy=
array([[nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       ...,
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(30,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan], dtype=float32)>, <tf.Tensor: shape=(30, 30), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(30, 30), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(60,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan], dtype=float32)>, <tf.Tensor: shape=(30, 45), dtype=float32, numpy=
array([[nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       ...,
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(30, 45), dtype=float32, numpy=
array([[nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       ...,
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(90,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
      dtype=float32)>, <tf.Tensor: shape=(45, 24), dtype=float32, numpy=
array([[nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       ...,
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(45, 24), dtype=float32, numpy=
array([[nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       ...,
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(48,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan], dtype=float32)>, <tf.Tensor: shape=(24, 12), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],
      dtype=float32)>, <tf.Tensor: shape=(24, 12), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],
      dtype=float32)>, <tf.Tensor: shape=(24,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
      dtype=float32)>, <tf.Tensor: shape=(30, 30), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(30, 30), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(60,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan], dtype=float32)>, <tf.Tensor: shape=(30, 30), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(30, 30), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(60,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan], dtype=float32)>, <tf.Tensor: shape=(60, 30), dtype=float32, numpy=
array([[nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       ...,
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(60, 30), dtype=float32, numpy=
array([[nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       ...,
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(60,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan], dtype=float32)>]
[Actor] Episode 7 Step 2 Loss: nan
Y Pred
tf.Tensor(
[[[nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]
  ...
  [nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]]

 [[nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]
  ...
  [nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]]

 [[nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]
  ...
  [nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]]

 ...

 [[nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]
  ...
  [nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]]

 [[nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]
  ...
  [nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]]

 [[nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]
  ...
  [nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]]], shape=(1125, 350, 12), dtype=float32)
omega
tf.Tensor(
[[0.]
 [0.]
 [0.]
 ...
 [0.]
 [0.]
 [0.]], shape=(1125, 1), dtype=float32)
mu
tf.Tensor(
[[nan nan nan ... nan nan nan]
 [nan nan nan ... nan nan nan]
 [nan nan nan ... nan nan nan]
 ...
 [nan nan nan ... nan nan nan]
 [nan nan nan ... nan nan nan]
 [nan nan nan ... nan nan nan]], shape=(1125, 12), dtype=float32)
mean
tf.Tensor(
[[nan nan nan ... nan nan nan]
 [nan nan nan ... nan nan nan]
 [nan nan nan ... nan nan nan]
 ...
 [nan nan nan ... nan nan nan]
 [nan nan nan ... nan nan nan]
 [nan nan nan ... nan nan nan]], shape=(1125, 12), dtype=float32)
grads
[<tf.Tensor: shape=(6, 30), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(30,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan], dtype=float32)>, <tf.Tensor: shape=(30, 60), dtype=float32, numpy=
array([[nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       ...,
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(60,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan], dtype=float32)>, <tf.Tensor: shape=(60, 30), dtype=float32, numpy=
array([[nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       ...,
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(30,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan], dtype=float32)>, <tf.Tensor: shape=(30, 25), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],
      dtype=float32)>, <tf.Tensor: shape=(25,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
      dtype=float32)>, <tf.Tensor: shape=(25, 12), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],
      dtype=float32)>, <tf.Tensor: shape=(12,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
      dtype=float32)>, <tf.Tensor: shape=(30, 25), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],
      dtype=float32)>, <tf.Tensor: shape=(25,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
      dtype=float32)>, <tf.Tensor: shape=(25, 12), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],
      dtype=float32)>, <tf.Tensor: shape=(12,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
      dtype=float32)>, <tf.Tensor: shape=(30, 1), dtype=float32, numpy=
array([[nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan]], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>, <tf.Tensor: shape=(34, 40), dtype=float32, numpy=
array([[nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       ...,
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(40,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan], dtype=float32)>, <tf.Tensor: shape=(40, 80), dtype=float32, numpy=
array([[nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       ...,
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(80,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan], dtype=float32)>, <tf.Tensor: shape=(80, 30), dtype=float32, numpy=
array([[nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       ...,
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(30,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan], dtype=float32)>, <tf.Tensor: shape=(30, 30), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(30, 30), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(60,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan], dtype=float32)>, <tf.Tensor: shape=(30, 45), dtype=float32, numpy=
array([[nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       ...,
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(30, 45), dtype=float32, numpy=
array([[nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       ...,
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(90,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
      dtype=float32)>, <tf.Tensor: shape=(45, 24), dtype=float32, numpy=
array([[nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       ...,
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(45, 24), dtype=float32, numpy=
array([[nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       ...,
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(48,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan], dtype=float32)>, <tf.Tensor: shape=(24, 12), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],
      dtype=float32)>, <tf.Tensor: shape=(24, 12), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],
      dtype=float32)>, <tf.Tensor: shape=(24,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
      dtype=float32)>, <tf.Tensor: shape=(30, 30), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(30, 30), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(60,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan], dtype=float32)>, <tf.Tensor: shape=(30, 30), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(30, 30), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(60,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan], dtype=float32)>, <tf.Tensor: shape=(60, 30), dtype=float32, numpy=
array([[nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       ...,
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(60, 30), dtype=float32, numpy=
array([[nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       ...,
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(60,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan], dtype=float32)>]
[Actor] Episode 7 Step 3 Loss: nan
Y Pred
tf.Tensor(
[[[nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]
  ...
  [nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]]

 [[nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]
  ...
  [nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]]

 [[nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]
  ...
  [nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]]

 ...

 [[nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]
  ...
  [nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]]

 [[nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]
  ...
  [nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]]

 [[nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]
  ...
  [nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]]], shape=(1125, 350, 12), dtype=float32)
omega
tf.Tensor(
[[0.]
 [0.]
 [0.]
 ...
 [0.]
 [0.]
 [0.]], shape=(1125, 1), dtype=float32)
mu
tf.Tensor(
[[nan nan nan ... nan nan nan]
 [nan nan nan ... nan nan nan]
 [nan nan nan ... nan nan nan]
 ...
 [nan nan nan ... nan nan nan]
 [nan nan nan ... nan nan nan]
 [nan nan nan ... nan nan nan]], shape=(1125, 12), dtype=float32)
mean
tf.Tensor(
[[nan nan nan ... nan nan nan]
 [nan nan nan ... nan nan nan]
 [nan nan nan ... nan nan nan]
 ...
 [nan nan nan ... nan nan nan]
 [nan nan nan ... nan nan nan]
 [nan nan nan ... nan nan nan]], shape=(1125, 12), dtype=float32)
grads
[<tf.Tensor: shape=(6, 30), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(30,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan], dtype=float32)>, <tf.Tensor: shape=(30, 60), dtype=float32, numpy=
array([[nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       ...,
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(60,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan], dtype=float32)>, <tf.Tensor: shape=(60, 30), dtype=float32, numpy=
array([[nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       ...,
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(30,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan], dtype=float32)>, <tf.Tensor: shape=(30, 25), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],
      dtype=float32)>, <tf.Tensor: shape=(25,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
      dtype=float32)>, <tf.Tensor: shape=(25, 12), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],
      dtype=float32)>, <tf.Tensor: shape=(12,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
      dtype=float32)>, <tf.Tensor: shape=(30, 25), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],
      dtype=float32)>, <tf.Tensor: shape=(25,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
      dtype=float32)>, <tf.Tensor: shape=(25, 12), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],
      dtype=float32)>, <tf.Tensor: shape=(12,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
      dtype=float32)>, <tf.Tensor: shape=(30, 1), dtype=float32, numpy=
array([[nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan]], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>, <tf.Tensor: shape=(34, 40), dtype=float32, numpy=
array([[nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       ...,
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(40,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan], dtype=float32)>, <tf.Tensor: shape=(40, 80), dtype=float32, numpy=
array([[nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       ...,
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(80,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan], dtype=float32)>, <tf.Tensor: shape=(80, 30), dtype=float32, numpy=
array([[nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       ...,
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(30,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan], dtype=float32)>, <tf.Tensor: shape=(30, 30), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(30, 30), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(60,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan], dtype=float32)>, <tf.Tensor: shape=(30, 45), dtype=float32, numpy=
array([[nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       ...,
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(30, 45), dtype=float32, numpy=
array([[nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       ...,
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(90,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
      dtype=float32)>, <tf.Tensor: shape=(45, 24), dtype=float32, numpy=
array([[nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       ...,
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(45, 24), dtype=float32, numpy=
array([[nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       ...,
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(48,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan], dtype=float32)>, <tf.Tensor: shape=(24, 12), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],
      dtype=float32)>, <tf.Tensor: shape=(24, 12), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],
      dtype=float32)>, <tf.Tensor: shape=(24,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
      dtype=float32)>, <tf.Tensor: shape=(30, 30), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(30, 30), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(60,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan], dtype=float32)>, <tf.Tensor: shape=(30, 30), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(30, 30), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(60,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan], dtype=float32)>, <tf.Tensor: shape=(60, 30), dtype=float32, numpy=
array([[nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       ...,
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(60, 30), dtype=float32, numpy=
array([[nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       ...,
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(60,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan], dtype=float32)>]
[Actor] Episode 7 Step 4 Loss: nan
Y Pred
tf.Tensor(
[[[nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]
  ...
  [nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]]

 [[nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]
  ...
  [nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]]

 [[nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]
  ...
  [nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]]

 ...

 [[nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]
  ...
  [nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]]

 [[nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]
  ...
  [nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]]

 [[nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]
  ...
  [nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]]], shape=(1125, 350, 12), dtype=float32)
omega
tf.Tensor(
[[0.]
 [0.]
 [0.]
 ...
 [0.]
 [0.]
 [0.]], shape=(1125, 1), dtype=float32)
mu
tf.Tensor(
[[nan nan nan ... nan nan nan]
 [nan nan nan ... nan nan nan]
 [nan nan nan ... nan nan nan]
 ...
 [nan nan nan ... nan nan nan]
 [nan nan nan ... nan nan nan]
 [nan nan nan ... nan nan nan]], shape=(1125, 12), dtype=float32)
mean
tf.Tensor(
[[nan nan nan ... nan nan nan]
 [nan nan nan ... nan nan nan]
 [nan nan nan ... nan nan nan]
 ...
 [nan nan nan ... nan nan nan]
 [nan nan nan ... nan nan nan]
 [nan nan nan ... nan nan nan]], shape=(1125, 12), dtype=float32)
grads
[<tf.Tensor: shape=(6, 30), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(30,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan], dtype=float32)>, <tf.Tensor: shape=(30, 60), dtype=float32, numpy=
array([[nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       ...,
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(60,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan], dtype=float32)>, <tf.Tensor: shape=(60, 30), dtype=float32, numpy=
array([[nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       ...,
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(30,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan], dtype=float32)>, <tf.Tensor: shape=(30, 25), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],
      dtype=float32)>, <tf.Tensor: shape=(25,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
      dtype=float32)>, <tf.Tensor: shape=(25, 12), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],
      dtype=float32)>, <tf.Tensor: shape=(12,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
      dtype=float32)>, <tf.Tensor: shape=(30, 25), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],
      dtype=float32)>, <tf.Tensor: shape=(25,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
      dtype=float32)>, <tf.Tensor: shape=(25, 12), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],
      dtype=float32)>, <tf.Tensor: shape=(12,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
      dtype=float32)>, <tf.Tensor: shape=(30, 1), dtype=float32, numpy=
array([[nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan],
       [nan]], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>, <tf.Tensor: shape=(34, 40), dtype=float32, numpy=
array([[nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       ...,
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(40,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan], dtype=float32)>, <tf.Tensor: shape=(40, 80), dtype=float32, numpy=
array([[nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       ...,
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(80,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan], dtype=float32)>, <tf.Tensor: shape=(80, 30), dtype=float32, numpy=
array([[nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       ...,
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(30,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan], dtype=float32)>, <tf.Tensor: shape=(30, 30), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(30, 30), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(60,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan], dtype=float32)>, <tf.Tensor: shape=(30, 45), dtype=float32, numpy=
array([[nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       ...,
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(30, 45), dtype=float32, numpy=
array([[nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       ...,
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(90,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
      dtype=float32)>, <tf.Tensor: shape=(45, 24), dtype=float32, numpy=
array([[nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       ...,
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(45, 24), dtype=float32, numpy=
array([[nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       ...,
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(48,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan], dtype=float32)>, <tf.Tensor: shape=(24, 12), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],
      dtype=float32)>, <tf.Tensor: shape=(24, 12), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],
      dtype=float32)>, <tf.Tensor: shape=(24,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
      dtype=float32)>, <tf.Tensor: shape=(30, 30), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(30, 30), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(60,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan], dtype=float32)>, <tf.Tensor: shape=(30, 30), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(30, 30), dtype=float32, numpy=
array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(60,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan], dtype=float32)>, <tf.Tensor: shape=(60, 30), dtype=float32, numpy=
array([[nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       ...,
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(60, 30), dtype=float32, numpy=
array([[nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       ...,
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(60,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan], dtype=float32)>]
[Actor] Episode 7 Step 5 Loss: nan
