2021-05-03 07:11:29.548087: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-03 07:11:29.548130: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1620006096.283036900, 1.056000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1620006096.284895392, 1.056000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1620006096.286296173, 1.057000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1620006097.734052689, 2.069000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1620006099.090542697, 3.001000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1620006100.178653416, 3.800000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1620006101.281106376, 4.605000000]: Ready to take commands for planning group back_left_leg.[0m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[92m [RDDPG] 0: episode_reward:-2.10678002392578 steps:1[00m
[RDDPG] Episode Done
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[92m [RDDPG] 1: episode_reward:-1.5070202695312536 steps:2[00m
[RDDPG] Episode Done
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[92m [RDDPG] 2: episode_reward:-1.5739290526123058 steps:4[00m
[RDDPG] Episode Done
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[92m [RDDPG] 3: episode_reward:-1.6743150028076186 steps:6[00m
[RDDPG] Episode Done
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[92m [RDDPG] 4: episode_reward:-2.4013309418945274 steps:9[00m
[RDDPG] Episode Done
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[92m [RDDPG] 5: episode_reward:-2.9057634440917774 steps:11[00m
[RDDPG] Episode Done
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[92m [RDDPG] 6: episode_reward:-1.840111593750004 steps:12[00m
[RDDPG] Episode Done
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[92m [RDDPG] 7: episode_reward:-4.1149536296386735 steps:15[00m
[RDDPG] Episode Done
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
Traceback (most recent call last):
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 175, in train
    state0 = None
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/evaluator.py", line 22, in __call__
    observation = env.reset()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/env.py", line 16, in reset
    self._state, self._reward = self.quadruped.reset()
  File "/home/shandilya/Desktop/CNS/DDP/src/simulations/ws/src/quadruped/scripts/quadruped.py", line 857, in reset
    rospy.sleep(1.0)
  File "/opt/ros/noetic/lib/python3/dist-packages/rospy/timer.py", line 165, in sleep
    raise rospy.exceptions.ROSInterruptException("ROS shutdown request")
rospy.exceptions.ROSInterruptException: ROS shutdown request
2021-05-03 07:13:27.638963: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-03 07:13:27.639005: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1620006214.568236087, 1.082000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1620006214.571019812, 1.083000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1620006214.571167150, 1.083000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1620006216.091258554, 2.138000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1620006217.347520198, 3.001000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1620006218.442416424, 3.801000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1620006219.582313883, 4.601000000]: Ready to take commands for planning group back_left_leg.[0m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
Traceback (most recent call last):
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 87, in train
    validate_reward = self.evaluate(
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/evaluator.py", line 37, in __call__
    observation, reward, done, info = env.step(
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/env.py", line 36, in step
    observation =  self.quadruped.step(
  File "/home/shandilya/Desktop/CNS/DDP/src/simulations/ws/src/quadruped/scripts/quadruped.py", line 1216, in step
    self.set_observation(action, desired_motion)
  File "/home/shandilya/Desktop/CNS/DDP/src/simulations/ws/src/quadruped/scripts/quadruped.py", line 1073, in set_observation
    rospy.wait_for_service('/gazebo/get_model_state')
  File "/opt/ros/noetic/lib/python3/dist-packages/rospy/impl/tcpros_service.py", line 166, in wait_for_service
    raise ROSInterruptException("rospy shutdown")
rospy.exceptions.ROSInterruptException: rospy shutdown
2021-05-03 07:21:14.544321: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-03 07:21:14.544356: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1620006681.601954538, 1.380000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1620006681.604921335, 1.385000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1620006681.605102883, 1.385000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1620006683.184789277, 2.507000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1620006683.960621551, 3.001000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1620006684.541308687, 3.400000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1620006685.126258961, 3.801000000]: Ready to take commands for planning group back_left_leg.[0m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000000: mean_reward:-1.5447985281188994[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[92m [RDDPG] 0: episode_reward:-1.175510164062494 steps:1[00m
[RDDPG] Episode Done
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[92m [RDDPG] 1: episode_reward:-1.7778881264648616 steps:2[00m
[RDDPG] Episode Done
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[92m [RDDPG] 2: episode_reward:-1.4448975408935507 steps:3[00m
[RDDPG] Episode Done
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[92m [RDDPG] 3: episode_reward:-2.437187889282247 steps:6[00m
[RDDPG] Episode Done
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[92m [RDDPG] 4: episode_reward:-1.5767268024902394 steps:7[00m
[RDDPG] Episode Done
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[92m [RDDPG] 5: episode_reward:-2.249871449707046 steps:9[00m
[RDDPG] Episode Done
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[92m [RDDPG] 6: episode_reward:-1.2523706237792944 steps:10[00m
[RDDPG] Episode Done
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
Traceback (most recent call last):
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 192, in train
    validate_reward = self.evaluate(
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/evaluator.py", line 37, in __call__
    observation, reward, done, info = env.step(
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/env.py", line 36, in step
    observation =  self.quadruped.step(
  File "/home/shandilya/Desktop/CNS/DDP/src/simulations/ws/src/quadruped/scripts/quadruped.py", line 1217, in step
    rospy.sleep(15.0/60.0)
  File "/opt/ros/noetic/lib/python3/dist-packages/rospy/timer.py", line 165, in sleep
    raise rospy.exceptions.ROSInterruptException("ROS shutdown request")
rospy.exceptions.ROSInterruptException: ROS shutdown request
2021-05-03 07:25:11.194041: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-03 07:25:11.194076: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1620006918.037535878, 1.084000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1620006918.040150836, 1.085000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1620006918.042418448, 1.088000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1620006919.834407981, 2.247000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1620006920.863011172, 3.001000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1620006921.945244874, 3.801000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1620006923.089380636, 4.602000000]: Ready to take commands for planning group back_left_leg.[0m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000000: mean_reward:-1.8449449662944821[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 0: episode_reward:-4.917141188476597 steps:5[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000005: mean_reward:-1.6677209307971954[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 1: episode_reward:-1.6056082714843725 steps:6[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 2: episode_reward:-2.4114595666503806 steps:9[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 3: episode_reward:-3.442751650878894 steps:12[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 4: episode_reward:-2.912814464843748 steps:14[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 5: episode_reward:-1.7922596948242213 steps:15[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000015: mean_reward:-1.5867240905044593[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 6: episode_reward:-2.9312811545410877 steps:17[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 7: episode_reward:-1.8594256835937413 steps:19[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 8: episode_reward:-4.058769458251917 steps:23[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 9: episode_reward:-1.4660119426269471 steps:24[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 10: episode_reward:-1.480947230590815 steps:25[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000025: mean_reward:-1.9252544056005534[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 11: episode_reward:-2.5216073059081663 steps:27[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 12: episode_reward:-2.0818329423827917 steps:28[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 13: episode_reward:-2.4491773763428 steps:30[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
Traceback (most recent call last):
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 192, in train
    validate_reward = self.evaluate(
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/evaluator.py", line 37, in __call__
    observation, reward, done, info = env.step(
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/env.py", line 36, in step
    observation =  self.quadruped.step(
  File "/home/shandilya/Desktop/CNS/DDP/src/simulations/ws/src/quadruped/scripts/quadruped.py", line 1217, in step
    rospy.sleep(15.0/60.0)
  File "/opt/ros/noetic/lib/python3/dist-packages/rospy/timer.py", line 165, in sleep
    raise rospy.exceptions.ROSInterruptException("ROS shutdown request")
rospy.exceptions.ROSInterruptException: ROS shutdown request
2021-05-03 07:49:12.724699: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-03 07:49:12.724756: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1620008359.735790952, 1.315000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1620008359.737946858, 1.317000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1620008359.738376880, 1.317000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1620008361.490834948, 2.511000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1620008362.340990856, 3.136000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1620008363.432400336, 3.937000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1620008364.563975782, 4.737000000]: Ready to take commands for planning group back_left_leg.[0m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000000: mean_reward:5391.784486808352[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 0: episode_reward:1.8653293778780855 steps:2[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 1: episode_reward:5.452890335242435 steps:4[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 2: episode_reward:-1.5564837308349737 steps:5[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000005: mean_reward:15831.519664380114[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 3: episode_reward:-1.2685764384765643 steps:6[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 4: episode_reward:52.29467727866711 steps:8[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 5: episode_reward:22927.724231137025 steps:11[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 6: episode_reward:21198.92748577086 steps:14[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 7: episode_reward:-1.7017416350097665 steps:15[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000015: mean_reward:19215.21156785565[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 8: episode_reward:106189.34779507213 steps:21[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 9: episode_reward:8.770021249984461 steps:23[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 10: episode_reward:-1.9624703022460577 steps:24[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 11: episode_reward:-2.4003961269531326 steps:25[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000025: mean_reward:46623.263355711846[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 12: episode_reward:-1.659824083496141 steps:26[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 13: episode_reward:29.262904056181014 steps:28[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 14: episode_reward:26668.78491543494 steps:31[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 15: episode_reward:140371.84886979705 steps:37[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 16: episode_reward:19.927762007336984 steps:39[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 17: episode_reward:-1.5213551408691781 steps:40[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
Traceback (most recent call last):
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 192, in train
    validate_reward = self.evaluate(
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/evaluator.py", line 37, in __call__
    observation, reward, done, info = env.step(
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/env.py", line 36, in step
    observation =  self.quadruped.step(
  File "/home/shandilya/Desktop/CNS/DDP/src/simulations/ws/src/quadruped/scripts/quadruped.py", line 1216, in step
    self.set_observation(action, desired_motion)
  File "/home/shandilya/Desktop/CNS/DDP/src/simulations/ws/src/quadruped/scripts/quadruped.py", line 1073, in set_observation
    rospy.wait_for_service('/gazebo/get_model_state')
  File "/opt/ros/noetic/lib/python3/dist-packages/rospy/impl/tcpros_service.py", line 166, in wait_for_service
    raise ROSInterruptException("rospy shutdown")
rospy.exceptions.ROSInterruptException: rospy shutdown
2021-05-03 08:07:56.386540: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-03 08:07:56.386574: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1620009483.093406706, 0.852000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1620009483.095392824, 0.852000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1620009483.095506628, 0.852000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1620009484.655642188, 1.957000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1620009485.509562809, 2.600000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1620009486.678624015, 3.400000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1620009487.774148616, 4.201000000]: Ready to take commands for planning group back_left_leg.[0m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000000: mean_reward:187183.2586871109[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 0: episode_reward:11.92903897740994 steps:2[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 1: episode_reward:-1.4635386437988385 steps:3[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 2: episode_reward:7.281301610880971 steps:5[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000005: mean_reward:45981.608732525594[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 3: episode_reward:-1.5049014506836385 steps:6[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 4: episode_reward:67696.30122897122 steps:11[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 5: episode_reward:-1.9745087687988798 steps:12[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 6: episode_reward:20798.588671929818 steps:15[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000015: mean_reward:17520.342416712203[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 7: episode_reward:-1.3514563172607494 steps:16[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 8: episode_reward:-1.9854180786132396 steps:17[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 9: episode_reward:0.0598374206479555 steps:19[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 10: episode_reward:85499.3648835129 steps:24[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 11: episode_reward:25070.486327855164 steps:29[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 12: episode_reward:26.649581012914236 steps:31[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 13: episode_reward:-1.6516652768554825 steps:32[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 14: episode_reward:12.650639940849778 steps:34[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 15: episode_reward:-1.4999219748534953 steps:35[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000035: mean_reward:66686.20186519525[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 16: episode_reward:28579.394013847163 steps:38[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 17: episode_reward:42.25881882335384 steps:40[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000040: mean_reward:4830.157383868236[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 18: episode_reward:2.67379056517993 steps:42[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 19: episode_reward:14.928147021968385 steps:44[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 20: episode_reward:7.327324759068111 steps:46[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 21: episode_reward:12.827541185468311 steps:48[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 22: episode_reward:219906.1225278811 steps:53[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 23: episode_reward:6.817110940597955 steps:55[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000055: mean_reward:15157.61661507673[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 24: episode_reward:4.480673094628973 steps:57[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 25: episode_reward:-1.9372177297363486 steps:58[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 26: episode_reward:8.757151483472809 steps:60[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[93m [RDDPG] Step_0000060: mean_reward:60982.62097527863[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 27: episode_reward:27139.37960221236 steps:64[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 28: episode_reward:-2.099234212402476 steps:65[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000065: mean_reward:22857.143961928618[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 29: episode_reward:-1.935038657714608 steps:66[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 30: episode_reward:-1.3373719791259946 steps:67[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 31: episode_reward:10.312534241130615 steps:69[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 32: episode_reward:-1.5426551103515522 steps:70[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000070: mean_reward:56410.8009814205[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 33: episode_reward:24.49917285957291 steps:72[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 34: episode_reward:-1.4174902905272706 steps:73[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 35: episode_reward:-1.403216291381821 steps:74[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 36: episode_reward:-1.3100882543944925 steps:75[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[93m [RDDPG] Step_0000075: mean_reward:59746.59979454569[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 37: episode_reward:-1.6207004560544855 steps:76[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 38: episode_reward:5.1489559807768455 steps:79[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 39: episode_reward:-1.9110195839850501 steps:80[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000080: mean_reward:29004.26673318168[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 40: episode_reward:-1.7392613896484201 steps:81[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 41: episode_reward:112.82473461674694 steps:84[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 42: episode_reward:-1.2916325324705984 steps:85[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[93m [RDDPG] Step_0000085: mean_reward:163585.80343076834[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 43: episode_reward:36853.82646416267 steps:88[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 44: episode_reward:-1.9561593427737503 steps:89[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 45: episode_reward:37.316845913508736 steps:91[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 46: episode_reward:2008.6993753198763 steps:94[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 47: episode_reward:-1.594065952636277 steps:95[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[93m [RDDPG] Step_0000095: mean_reward:24477.12393294115[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 48: episode_reward:-1.5438449963377954 steps:96[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 49: episode_reward:166.64135873566366 steps:98[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 50: episode_reward:13.886425081540956 steps:100[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000100: mean_reward:117793.66966692568[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 51: episode_reward:42.65860765129996 steps:103[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 52: episode_reward:-1.4181615219727928 steps:104[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 53: episode_reward:65463.46160424445 steps:109[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 54: episode_reward:238.00518757648587 steps:112[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 55: episode_reward:62936.45484413573 steps:118[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 56: episode_reward:-1.4984208577878584 steps:119[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 57: episode_reward:95.05695507301434 steps:121[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 58: episode_reward:7.88557369445123 steps:123[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 59: episode_reward:49343.02816219231 steps:126[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 60: episode_reward:-2.0579910546878035 steps:127[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 61: episode_reward:28.601192779272115 steps:129[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 62: episode_reward:65072.42527005699 steps:132[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 63: episode_reward:16.295669220174048 steps:135[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[93m [RDDPG] Step_0000135: mean_reward:203999.39362579468[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 64: episode_reward:-1.8810898535160168 steps:136[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 65: episode_reward:-1.5022221049805116 steps:137[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 66: episode_reward:0.9404725389576836 steps:139[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 67: episode_reward:0.3462415070106015 steps:141[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 68: episode_reward:27.916065183550344 steps:143[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 69: episode_reward:-1.2562934580078855 steps:144[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 70: episode_reward:6.024242995751303 steps:146[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 71: episode_reward:1.2796879082454868 steps:148[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 72: episode_reward:-2.1149533229988737 steps:149[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 73: episode_reward:4.84949044360912 steps:151[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 74: episode_reward:20.60531453605303 steps:153[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 75: episode_reward:-1.7062180932607425 steps:154[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 76: episode_reward:-1.746665367187714 steps:155[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[93m [RDDPG] Step_0000155: mean_reward:24633.14597744185[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 77: episode_reward:373.6774877197359 steps:158[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 78: episode_reward:109774.3078550573 steps:164[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 79: episode_reward:8.73980163839944 steps:166[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 80: episode_reward:-2.054697777099869 steps:167[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 81: episode_reward:16.61760522645374 steps:170[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000170: mean_reward:7570.00849228608[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 82: episode_reward:176.50754257896946 steps:172[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 83: episode_reward:-1.415628654784945 steps:173[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 84: episode_reward:21705.07399723109 steps:178[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 85: episode_reward:-2.2515887731935758 steps:179[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 86: episode_reward:15.74204530526554 steps:181[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 87: episode_reward:13.11459394071149 steps:183[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 88: episode_reward:1.2076678659477555 steps:185[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000185: mean_reward:25530.861215509656[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 89: episode_reward:11314.160048099353 steps:188[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 90: episode_reward:2.717713168735635 steps:190[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000190: mean_reward:61702.21264686405[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 91: episode_reward:47.11486690311221 steps:192[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 92: episode_reward:27.101683774365625 steps:194[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 93: episode_reward:57029.550022730386 steps:198[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 94: episode_reward:40571.63755459253 steps:201[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 95: episode_reward:-1.297294284057624 steps:202[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 96: episode_reward:13.635525984427899 steps:204[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 97: episode_reward:4.889719086563269 steps:206[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 98: episode_reward:16.717330182782 steps:209[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 99: episode_reward:11.843904989055893 steps:211[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 100: episode_reward:-1.65818598339873 steps:212[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 101: episode_reward:-1.4474694760742082 steps:213[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 102: episode_reward:-2.2200037536614277 steps:214[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 103: episode_reward:-1.6246766699210973 steps:215[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000215: mean_reward:15355.27211633269[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 104: episode_reward:25869.409464305176 steps:218[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 105: episode_reward:9.773417085434962 steps:220[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[93m [RDDPG] Step_0000220: mean_reward:71340.01944596448[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 106: episode_reward:-1.7925513442385386 steps:221[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 107: episode_reward:12.073530082994612 steps:224[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 108: episode_reward:11795.933015414548 steps:227[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 109: episode_reward:38221.54576171687 steps:231[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 110: episode_reward:-1.661947497558812 steps:232[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 111: episode_reward:17432.417861194037 steps:235[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[93m [RDDPG] Step_0000235: mean_reward:372903.5300514229[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 112: episode_reward:2.142714137780393 steps:237[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 113: episode_reward:12905.198173033104 steps:240[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[93m [RDDPG] Step_0000240: mean_reward:143358.4214126562[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 114: episode_reward:-1.3088032587889515 steps:241[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 115: episode_reward:-1.4346212916258265 steps:242[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 116: episode_reward:7.5070269836244226 steps:244[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 117: episode_reward:-1.8959239702138073 steps:245[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000245: mean_reward:14366.06913893644[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 118: episode_reward:58678.709214015136 steps:250[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[93m [RDDPG] Step_0000250: mean_reward:50585.00461293481[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 119: episode_reward:52927.21366158738 steps:256[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 120: episode_reward:6285.671546319366 steps:259[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 121: episode_reward:146028.30841487486 steps:266[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 122: episode_reward:6.158370101090732 steps:268[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 123: episode_reward:-1.4811342143554338 steps:269[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 124: episode_reward:61.02055465635222 steps:271[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 125: episode_reward:8.875151057502544 steps:273[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 126: episode_reward:-1.9002712382801643 steps:274[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 127: episode_reward:6.5901763846266705 steps:276[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 128: episode_reward:-1.5920525703126693 steps:277[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 129: episode_reward:4.3945717375112405 steps:279[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 130: episode_reward:8.721520701526607 steps:281[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 131: episode_reward:4.741605994459927 steps:283[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 132: episode_reward:3.583502834674335 steps:285[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000285: mean_reward:49065.28632778325[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 133: episode_reward:73823.05267538705 steps:289[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 134: episode_reward:-1.8896509914565134 steps:290[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000290: mean_reward:180343.63502386853[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 135: episode_reward:-1.5593730878906622 steps:291[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 136: episode_reward:47.14473601316949 steps:293[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 137: episode_reward:-1.6340008300787154 steps:294[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 138: episode_reward:3.018693863682493 steps:296[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 139: episode_reward:3.668109988717614 steps:298[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 140: episode_reward:35453.857182073734 steps:301[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 141: episode_reward:-1.4775923525388244 steps:302[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 142: episode_reward:39176.78238748961 steps:306[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 143: episode_reward:301.1570616243732 steps:308[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 144: episode_reward:187.40402581245158 steps:310[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000310: mean_reward:15721.26514108496[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 145: episode_reward:71868.8487008946 steps:314[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 146: episode_reward:149236.07369859182 steps:318[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 147: episode_reward:9.429274741638332 steps:320[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[93m [RDDPG] Step_0000320: mean_reward:133356.85739571528[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 148: episode_reward:-1.280208728027253 steps:321[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 149: episode_reward:17936.20856335393 steps:324[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 150: episode_reward:0.7142118048367521 steps:326[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 151: episode_reward:42891.53373802448 steps:331[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 152: episode_reward:4845.9430557852775 steps:334[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 153: episode_reward:18.108386554269185 steps:336[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 154: episode_reward:-1.8913132995622917 steps:337[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 155: episode_reward:-1.253300825805904 steps:338[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 156: episode_reward:8.84299611913289 steps:340[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[93m [RDDPG] Step_0000340: mean_reward:69310.68932208276[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 157: episode_reward:33856.602689804226 steps:345[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[93m [RDDPG] Step_0000345: mean_reward:109571.73828676571[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 158: episode_reward:-1.5915436999514705 steps:346[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 159: episode_reward:20217.651061745124 steps:349[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 160: episode_reward:115.81279078613402 steps:351[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 161: episode_reward:-1.9698924575198813 steps:352[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 162: episode_reward:7752.377504996393 steps:355[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000355: mean_reward:21244.115614692553[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 163: episode_reward:39.96926395793356 steps:357[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 164: episode_reward:-1.140614325317386 steps:358[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 165: episode_reward:9235.43167826272 steps:362[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 166: episode_reward:-1.8325637595215034 steps:363[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 167: episode_reward:12.709581051953217 steps:366[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 168: episode_reward:12422.440271125148 steps:369[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 169: episode_reward:2.9650670408986817 steps:371[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 170: episode_reward:8.833442991177707 steps:373[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 171: episode_reward:11761.998513889826 steps:377[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 172: episode_reward:120.27298139472784 steps:379[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 173: episode_reward:5.502761245491258 steps:381[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 174: episode_reward:-2.4585380976531352 steps:382[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 175: episode_reward:8.673855575328679 steps:384[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 176: episode_reward:20.609838231720012 steps:386[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 177: episode_reward:-2.234345080079691 steps:387[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 178: episode_reward:7.900783748171928 steps:389[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 179: episode_reward:-1.5253551220694064 steps:390[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000390: mean_reward:26820.89688994406[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 180: episode_reward:-1.7067069750975286 steps:391[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 181: episode_reward:5969.335890359383 steps:394[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 182: episode_reward:-1.94187967773352 steps:395[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[93m [RDDPG] Step_0000395: mean_reward:169675.53769834264[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 183: episode_reward:9.495162687187984 steps:397[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 184: episode_reward:-1.9964150483388559 steps:398[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 185: episode_reward:184.66584483190925 steps:400[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000400: mean_reward:2229.2907053263075[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 186: episode_reward:35.36577209709426 steps:402[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
rddpg_torch.py:35: FutureWarning: arrays to stack must be passed as a "sequence" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.
  rdpg.train(params['train_episode_count'], checkpoint_path, True)
/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:130: UserWarning: Error detected in EluBackward. Traceback of forward call that caused the error:
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 290, in update_policy
    self.agent.actor_target(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 280, in forward
    action, robot_enc_state, z = self.cell(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 257, in forward
    rs_r = self.robot_state_enc(self.robot_enc_state)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/container.py", line 117, in forward
    input = module(input)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 477, in forward
    return F.elu(input, self.alpha, self.inplace)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/functional.py", line 1227, in elu
    result = torch._C._nn.elu(input, alpha)
 (Triggered internally at  /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:104.)
  Variable._execution_engine.run_backward(
Traceback (most recent call last):
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 318, in update_policy
    value_loss.backward()
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py", line 130, in backward
    Variable._execution_engine.run_backward(
RuntimeError: Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time.
2021-05-03 18:53:06.124388: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-03 18:53:06.124432: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1620048193.288214818, 1.491000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1620048193.290201909, 1.492000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1620048193.290673074, 1.492000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1620048194.647856580, 2.576000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1620048195.829355846, 3.513000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1620048196.791753252, 4.312000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1620048197.770737172, 5.112000000]: Ready to take commands for planning group back_left_leg.[0m
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000000: mean_reward:891.5610960462493[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 0: episode_reward:-1.6668907617187592 steps:1[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 1: episode_reward:29095.25466851409 steps:4[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
rddpg_torch.py:35: FutureWarning: arrays to stack must be passed as a "sequence" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.
  rdpg.train(params['train_episode_count'], checkpoint_path, True)
/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:130: UserWarning: Error detected in EluBackward. Traceback of forward call that caused the error:
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 290, in update_policy
    self.agent.actor_target(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 280, in forward
    action, robot_enc_state, z = self.cell(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 257, in forward
    rs_r = self.robot_state_enc(self.robot_enc_state)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/container.py", line 117, in forward
    input = module(input)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 477, in forward
    return F.elu(input, self.alpha, self.inplace)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/functional.py", line 1227, in elu
    result = torch._C._nn.elu(input, alpha)
 (Triggered internally at  /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:104.)
  Variable._execution_engine.run_backward(
Traceback (most recent call last):
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 318, in update_policy
    value_loss.backward()
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py", line 130, in backward
    Variable._execution_engine.run_backward(
RuntimeError: Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time.
2021-05-03 18:58:12.277157: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-03 18:58:12.277206: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1620048499.105264556, 1.363000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1620048499.107333859, 1.364000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1620048499.108438284, 1.365000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1620048500.519609720, 2.478000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1620048501.577409284, 3.300000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1620048502.543863589, 4.100000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1620048503.521418532, 4.900000000]: Ready to take commands for planning group back_left_leg.[0m
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000000: mean_reward:9367.847993385383[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 0: episode_reward:114.06092338302604 steps:3[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 1: episode_reward:5.356935258266301 steps:5[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[93m [RDDPG] Step_0000005: mean_reward:32939.817811661174[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
Updating
rddpg_torch.py:35: FutureWarning: arrays to stack must be passed as a "sequence" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.
  rdpg.train(params['train_episode_count'], checkpoint_path, True)
Updating
/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:130: UserWarning: Error detected in EluBackward. Traceback of forward call that caused the error:
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 290, in update_policy
    self.agent.actor_target(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 280, in forward
    action, robot_enc_state, z = self.cell(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 257, in forward
    rs_r = self.robot_state_enc(self.robot_enc_state)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/container.py", line 117, in forward
    input = module(input)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 477, in forward
    return F.elu(input, self.alpha, self.inplace)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/functional.py", line 1227, in elu
    result = torch._C._nn.elu(input, alpha)
 (Triggered internally at  /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:104.)
  Variable._execution_engine.run_backward(
Traceback (most recent call last):
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 318, in update_policy
    value_loss.backward()
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py", line 130, in backward
    Variable._execution_engine.run_backward(
RuntimeError: Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time.
2021-05-03 19:50:39.386594: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-03 19:50:39.386676: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1620051646.023702080, 1.053000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1620051646.026228468, 1.055000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1620051646.026366256, 1.055000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1620051647.466193880, 2.214000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1620051648.417344267, 3.000000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1620051649.456453553, 3.801000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1620051650.410795942, 4.601000000]: Ready to take commands for planning group back_left_leg.[0m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000000: mean_reward:4107.651126594262[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 0: episode_reward:242.89942593435558 steps:2[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 1: episode_reward:-1.7453131030273494 steps:3[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 2: episode_reward:34.317200358393386 steps:5[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[93m [RDDPG] Step_0000005: mean_reward:56453.575188766255[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Updating Policy
Updating
rddpg_torch.py:35: FutureWarning: arrays to stack must be passed as a "sequence" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.
  rdpg.train(params['train_episode_count'], checkpoint_path, True)
/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:130: UserWarning: Error detected in DivBackward0. Traceback of forward call that caused the error:
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 314, in update_policy
    value_loss /= len(experiences) # divide by trajectory length
 (Triggered internally at  /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:104.)
  Variable._execution_engine.run_backward(
Traceback (most recent call last):
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 336, in update_policy
    value_loss.backward()
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py", line 130, in backward
    Variable._execution_engine.run_backward(
RuntimeError: Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time.
2021-05-03 19:56:05.067003: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-03 19:56:05.067090: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1620051977.355559954, 249.474000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1620051977.364519863, 249.478000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1620051977.364706603, 249.478000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
Traceback (most recent call last):
  File "rddpg_torch.py", line 24, in <module>
    env = Env(
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/env.py", line 12, in __init__
    self.quadruped = Quadruped(params, experiment)
  File "/home/shandilya/Desktop/CNS/DDP/src/simulations/ws/src/quadruped/scripts/quadruped.py", line 489, in __init__
    self.kinematics = Kinematics(
  File "/home/shandilya/Desktop/CNS/DDP/src/simulations/ws/src/quadruped/scripts/kinematics.py", line 21, in __init__
    self.front_right_leg = moveit_commander.MoveGroupCommander(
  File "/opt/ros/noetic/lib/python3/dist-packages/moveit_commander/move_group.py", line 53, in __init__
    self._g = _moveit_move_group_interface.MoveGroupInterface(name, robot_description, ns, wait_for_servers)
RuntimeError: Unable to connect to move_group action server 'move_group' within allotted time (5s)
2021-05-03 19:56:37.429239: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-03 19:56:37.429354: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1620052007.321184912, 275.139000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1620052007.329776063, 275.145000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1620052007.330024095, 275.145000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1620052008.741618434, 275.935000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1620052009.097059815, 276.149000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1620052010.341130326, 276.549000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1620052011.707494391, 276.949000000]: Ready to take commands for planning group back_left_leg.[0m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000000: mean_reward:2408.676995879857[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 0: episode_reward:22.78088689004532 steps:2[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
Updating
rddpg_torch.py:35: FutureWarning: arrays to stack must be passed as a "sequence" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.
  rdpg.train(params['train_episode_count'], checkpoint_path, True)
/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:130: UserWarning: Error detected in AddmmBackward. Traceback of forward call that caused the error:
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 304, in update_policy
    current_q = self.agent.critic(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 351, in forward
    q = self.out_dense_seq(torch.cat([ms, rs, ac], -1))
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/container.py", line 117, in forward
    input = module(input)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 93, in forward
    return F.linear(input, self.weight, self.bias)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/functional.py", line 1690, in linear
    ret = torch.addmm(bias, input, weight.t())
 (Triggered internally at  /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:104.)
  Variable._execution_engine.run_backward(
Traceback (most recent call last):
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 336, in update_policy
    value_loss.backward()
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py", line 130, in backward
    Variable._execution_engine.run_backward(
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [72, 1]], which is output 0 of TBackward, is at version 2; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!
2021-05-03 20:01:55.814850: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-03 20:01:55.814905: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1620052323.086157069, 373.449000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1620052323.094556221, 373.454000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1620052323.094815417, 373.454000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1620052324.550993846, 374.420000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1620052325.253271755, 374.902000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1620052326.106635253, 375.504000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1620052327.158733469, 376.102000000]: Ready to take commands for planning group back_left_leg.[0m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000000: mean_reward:818.4749255492427[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 0: episode_reward:-1.8516344584962234 steps:1[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 1: episode_reward:76.10934163384695 steps:3[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 2: episode_reward:-2.4703597812501474 steps:4[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 3: episode_reward:-1.5890771972656978 steps:5[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000005: mean_reward:15991.335025275952[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Updating Policy
Updating
rddpg_torch.py:35: FutureWarning: arrays to stack must be passed as a "sequence" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.
  rdpg.train(params['train_episode_count'], checkpoint_path, True)
updating done
Updating
/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:130: UserWarning: Error detected in EluBackward. Traceback of forward call that caused the error:
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 290, in update_policy
    self.agent.actor_target(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 280, in forward
    action, robot_enc_state, z = self.cell(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 257, in forward
    rs_r = self.robot_state_enc(self.robot_enc_state)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/container.py", line 117, in forward
    input = module(input)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 477, in forward
    return F.elu(input, self.alpha, self.inplace)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/functional.py", line 1227, in elu
    result = torch._C._nn.elu(input, alpha)
 (Triggered internally at  /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:104.)
  Variable._execution_engine.run_backward(
Traceback (most recent call last):
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 318, in update_policy
    value_loss.backward()
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py", line 130, in backward
    Variable._execution_engine.run_backward(
RuntimeError: Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time.
2021-05-03 20:15:46.984282: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-03 20:15:46.984319: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1620053153.879287714, 1.330000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1620053153.881201807, 1.330000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1620053153.881317460, 1.330000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1620053155.595036454, 2.703000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1620053156.235746139, 3.248000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1620053156.954788559, 3.848000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1620053157.425877377, 4.248000000]: Ready to take commands for planning group back_left_leg.[0m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000000: mean_reward:79.83187296867891[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 0: episode_reward:-1.919901659179684 steps:1[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 1: episode_reward:120.95113218409008 steps:3[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 2: episode_reward:-1.8953927529296875 steps:4[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 3: episode_reward:-1.3651544006347736 steps:5[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000005: mean_reward:10697.934905122649[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Updating Policy
Updating
rddpg_torch.py:35: FutureWarning: arrays to stack must be passed as a "sequence" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.
  rdpg.train(params['train_episode_count'], checkpoint_path, True)
updating done
Updating
/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:130: UserWarning: Error detected in AddmmBackward. Traceback of forward call that caused the error:
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 323, in update_policy
    action, (robot_enc_state, z) = self.agent.actor(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 280, in forward
    action, robot_enc_state, z = self.cell(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 257, in forward
    rs_r = self.robot_state_enc(self.robot_enc_state)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/container.py", line 117, in forward
    input = module(input)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 93, in forward
    return F.linear(input, self.weight, self.bias)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/functional.py", line 1690, in linear
    ret = torch.addmm(bias, input, weight.t())
 (Triggered internally at  /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:104.)
  Variable._execution_engine.run_backward(
Traceback (most recent call last):
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 335, in update_policy
    policy_loss.backward(retain_graph=True)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py", line 130, in backward
    Variable._execution_engine.run_backward(
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [135, 30]], which is output 0 of TBackward, is at version 2; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!
2021-05-03 20:41:12.428277: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-03 20:41:12.428326: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1620054679.451591616, 1.122000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1620054679.456954817, 1.126000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1620054679.457168252, 1.126000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1620054681.024988326, 2.223000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1620054682.114462564, 3.004000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1620054683.318666036, 3.801000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1620054684.456272630, 4.600000000]: Ready to take commands for planning group back_left_leg.[0m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[93m [RDDPG] Step_0000000: mean_reward:1997.8238422621787[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 0: episode_reward:82.84954506992872 steps:2[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 1: episode_reward:-1.4981446508789025 steps:3[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 2: episode_reward:-0.001855439330423092 steps:5[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000005: mean_reward:-395.73080803772064[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
Updating
rddpg_torch.py:35: FutureWarning: arrays to stack must be passed as a "sequence" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.
  rdpg.train(params['train_episode_count'], checkpoint_path, True)
updating done
Updating
/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:130: UserWarning: Error detected in AddmmBackward. Traceback of forward call that caused the error:
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 323, in update_policy
    action, (robot_enc_state, z) = self.agent.actor(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 287, in forward
    action, robot_enc_state, z = self.cell(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 258, in forward
    rs_r = self.robot_state_enc(self.robot_enc_state)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/container.py", line 117, in forward
    input = module(input)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 93, in forward
    return F.linear(input, self.weight, self.bias)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/functional.py", line 1690, in linear
    ret = torch.addmm(bias, input, weight.t())
 (Triggered internally at  /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:104.)
  Variable._execution_engine.run_backward(
Traceback (most recent call last):
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 335, in update_policy
    policy_loss.backward(retain_graph=True)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py", line 130, in backward
    Variable._execution_engine.run_backward(
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [135, 30]], which is output 0 of TBackward, is at version 2; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!
2021-05-03 20:48:05.837584: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-03 20:48:05.837632: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1620055092.897280607, 1.138000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1620055092.902737944, 1.149000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1620055092.902971265, 1.150000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1620055094.498998520, 2.452000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1620055095.362898911, 3.200000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1620055096.382654904, 4.001000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1620055097.343962736, 4.801000000]: Ready to take commands for planning group back_left_leg.[0m
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[93m [RDDPG] Step_0000000: mean_reward:6164.843719125439[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 0: episode_reward:5.527627122770246 steps:2[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 1: episode_reward:-1.7726349316406542 steps:3[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 2: episode_reward:3.3930855859228974 steps:5[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[93m [RDDPG] Step_0000005: mean_reward:22953.00662260519[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
Updating
rddpg_torch.py:35: FutureWarning: arrays to stack must be passed as a "sequence" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.
  rdpg.train(params['train_episode_count'], checkpoint_path, True)
/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:130: UserWarning: Error detected in AddmmBackward. Traceback of forward call that caused the error:
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 323, in update_policy
    policy_loss = -self.agent.critic(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 358, in forward
    q = self.out_dense_seq(torch.cat([ms, rs, ac], -1))
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/container.py", line 117, in forward
    input = module(input)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 93, in forward
    return F.linear(input, self.weight, self.bias)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/functional.py", line 1690, in linear
    ret = torch.addmm(bias, input, weight.t())
 (Triggered internally at  /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:104.)
  Variable._execution_engine.run_backward(
Traceback (most recent call last):
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 337, in update_policy
    policy_loss.backward(retain_graph=True)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py", line 130, in backward
    Variable._execution_engine.run_backward(
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [72, 1]], which is output 0 of TBackward, is at version 2; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!
2021-05-03 20:55:51.591305: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-03 20:55:51.591357: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1620055558.571520181, 1.328000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1620055558.573553657, 1.328000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1620055558.573693756, 1.328000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1620055560.093176317, 2.498000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1620055561.046390586, 3.316000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1620055561.976705833, 4.115000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1620055563.003360367, 4.916000000]: Ready to take commands for planning group back_left_leg.[0m
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000000: mean_reward:5236.881220844038[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 0: episode_reward:8.914477233396845 steps:2[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 1: episode_reward:2.098480860146341 steps:4[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 2: episode_reward:-1.4376310043945297 steps:5[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000005: mean_reward:24638.435628182022[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Updating Policy
Updating
rddpg_torch.py:35: FutureWarning: arrays to stack must be passed as a "sequence" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.
  rdpg.train(params['train_episode_count'], checkpoint_path, True)
/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:130: UserWarning: Error detected in AddmmBackward. Traceback of forward call that caused the error:
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 323, in update_policy
    policy_loss = -self.agent.critic(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 358, in forward
    return self.out_dense_seq(torch.cat([ms, rs, ac], -1))
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/container.py", line 117, in forward
    input = module(input)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 93, in forward
    return F.linear(input, self.weight, self.bias)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/functional.py", line 1690, in linear
    ret = torch.addmm(bias, input, weight.t())
 (Triggered internally at  /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:104.)
  Variable._execution_engine.run_backward(
Traceback (most recent call last):
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 337, in update_policy
    policy_loss.backward(retain_graph=True)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py", line 130, in backward
    Variable._execution_engine.run_backward(
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [72, 1]], which is output 0 of TBackward, is at version 2; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!
2021-05-03 21:04:37.866744: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-03 21:04:37.866800: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1620056086.343244580, 1.447000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1620056086.348482655, 1.452000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1620056086.348762467, 1.452000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1620056087.796464225, 2.575000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1620056089.027927851, 3.521000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1620056090.040476496, 4.321000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1620056091.020297946, 5.121000000]: Ready to take commands for planning group back_left_leg.[0m
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000000: mean_reward:1654.416895451443[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 0: episode_reward:229.2763680030277 steps:4[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 1: episode_reward:-1.9228397304687448 steps:5[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000005: mean_reward:20623.05910419586[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Updating Policy
Updating
rddpg_torch.py:35: FutureWarning: arrays to stack must be passed as a "sequence" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.
  rdpg.train(params['train_episode_count'], checkpoint_path, True)
/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:130: UserWarning: Error detected in AddmmBackward. Traceback of forward call that caused the error:
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 323, in update_policy
    policy_loss = -self.agent.critic(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 358, in forward
    return self.out_dense_seq(torch.cat([ms, rs, ac], -1))
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/container.py", line 117, in forward
    input = module(input)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 93, in forward
    return F.linear(input, self.weight, self.bias)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/functional.py", line 1690, in linear
    ret = torch.addmm(bias, input, weight.t())
 (Triggered internally at  /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:104.)
  Variable._execution_engine.run_backward(
Traceback (most recent call last):
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 337, in update_policy
    policy_loss.backward(retain_graph=True)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py", line 130, in backward
    Variable._execution_engine.run_backward(
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [72, 1]], which is output 0 of TBackward, is at version 2; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!
2021-05-03 21:12:59.972576: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-03 21:12:59.972628: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1620056587.141101165, 1.377000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1620056587.143122865, 1.377000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1620056587.143469952, 1.381000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1620056588.675838126, 2.615000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1620056589.302009684, 3.038000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1620056589.807239835, 3.438000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1620056590.310307021, 3.837000000]: Ready to take commands for planning group back_left_leg.[0m
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000000: mean_reward:4120.433586703351[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 0: episode_reward:71473.91096802142 steps:3[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 1: episode_reward:49.00098949846637 steps:5[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[93m [RDDPG] Step_0000005: mean_reward:31518.647804412925[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Updating Policy
Updating
rddpg_torch.py:35: FutureWarning: arrays to stack must be passed as a "sequence" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.
  rdpg.train(params['train_episode_count'], checkpoint_path, True)
/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:130: UserWarning: Error detected in AddmmBackward. Traceback of forward call that caused the error:
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 322, in update_policy
    q_val = self.agent.critic(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 358, in forward
    return self.out_dense_seq(torch.cat([ms, rs, ac], -1))
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/container.py", line 117, in forward
    input = module(input)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 93, in forward
    return F.linear(input, self.weight, self.bias)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/functional.py", line 1690, in linear
    ret = torch.addmm(bias, input, weight.t())
 (Triggered internally at  /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:104.)
  Variable._execution_engine.run_backward(
Traceback (most recent call last):
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 336, in update_policy
    policy_loss.backward(retain_graph=True)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py", line 130, in backward
    Variable._execution_engine.run_backward(
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [72, 1]], which is output 0 of TBackward, is at version 2; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!
2021-05-03 23:47:43.908656: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-03 23:47:43.908699: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1620065870.781476936, 1.257000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1620065870.784302572, 1.258000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1620065870.784607353, 1.258000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1620065872.186987686, 2.415000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1620065873.103531835, 3.201000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1620065874.350844338, 4.201000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1620065875.307808681, 5.001000000]: Ready to take commands for planning group back_left_leg.[0m
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000000: mean_reward:685.6015695441047[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 0: episode_reward:-1.7918713586425636 steps:1[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 1: episode_reward:30565.28189537176 steps:4[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Updating Policy
Updating
rddpg_torch.py:35: FutureWarning: arrays to stack must be passed as a "sequence" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.
  rdpg.train(params['train_episode_count'], checkpoint_path, True)
updating done
Updating
/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:130: UserWarning: Error detected in AddmmBackward. Traceback of forward call that caused the error:
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 317, in update_policy
    action, (robot_enc_state, z) = self.agent.actor(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 287, in forward
    action, robot_enc_state, z = self.cell(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 258, in forward
    rs_r = self.robot_state_enc(self.robot_enc_state)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/container.py", line 117, in forward
    input = module(input)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 93, in forward
    return F.linear(input, self.weight, self.bias)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/functional.py", line 1690, in linear
    ret = torch.addmm(bias, input, weight.t())
 (Triggered internally at  /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:104.)
  Variable._execution_engine.run_backward(
Traceback (most recent call last):
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 334, in update_policy
    policy_loss.backward(retain_graph = True)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py", line 130, in backward
    Variable._execution_engine.run_backward(
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [135, 30]], which is output 0 of TBackward, is at version 2; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!
2021-05-03 23:54:09.229205: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-03 23:54:09.229276: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1620066256.342462002, 1.275000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1620066256.344192285, 1.276000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1620066256.344299525, 1.276000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1620066257.795207302, 2.407000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1620066258.835238388, 3.201000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1620066259.828639210, 4.001000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1620066261.032735124, 4.802000000]: Ready to take commands for planning group back_left_leg.[0m
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[93m [RDDPG] Step_0000000: mean_reward:19779.31898853792[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 0: episode_reward:-1.935493378906248 steps:1[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 1: episode_reward:33.233365161648415 steps:3[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 2: episode_reward:16.515067502055597 steps:5[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000005: mean_reward:33816.81711319146[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
Updating
rddpg_torch.py:35: FutureWarning: arrays to stack must be passed as a "sequence" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.
  rdpg.train(params['train_episode_count'], checkpoint_path, True)
updating done
Updating
/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:130: UserWarning: Error detected in AddmmBackward. Traceback of forward call that caused the error:
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 317, in update_policy
    action, (robot_enc_state, z) = self.agent.actor(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 287, in forward
    action, robot_enc_state, z = self.cell(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 258, in forward
    rs_r = self.robot_state_enc(self.robot_enc_state)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/container.py", line 117, in forward
    input = module(input)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 93, in forward
    return F.linear(input, self.weight, self.bias)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/functional.py", line 1690, in linear
    ret = torch.addmm(bias, input, weight.t())
 (Triggered internally at  /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:104.)
  Variable._execution_engine.run_backward(
Traceback (most recent call last):
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 334, in update_policy
    policy_loss.backward(retain_graph = True)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py", line 130, in backward
    Variable._execution_engine.run_backward(
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [135, 30]], which is output 0 of TBackward, is at version 2; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!
2021-05-04 00:54:10.507416: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-04 00:54:10.507462: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1620069857.152408853, 1.065000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1620069857.156193661, 1.068000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1620069857.156413853, 1.068000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1620069858.751652543, 2.368000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1620069859.778079954, 3.201000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1620069860.718199420, 4.001000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1620069861.682840040, 4.801000000]: Ready to take commands for planning group back_left_leg.[0m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[93m [RDDPG] Step_0000000: mean_reward:16070.467877670515[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 0: episode_reward:73.94608734941855 steps:2[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 1: episode_reward:14.713244959074071 steps:4[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Updating Policy
Updating
rddpg_torch.py:35: FutureWarning: arrays to stack must be passed as a "sequence" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.
  rdpg.train(params['train_episode_count'], checkpoint_path, True)
updating done
Updating
/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:130: UserWarning: Error detected in AddmmBackward. Traceback of forward call that caused the error:
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 317, in update_policy
    action, (robot_enc_state, z) = self.agent.actor(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 287, in forward
    action, robot_enc_state, z = self.cell(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 258, in forward
    rs_r = self.robot_state_enc(self.robot_enc_state)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/container.py", line 117, in forward
    input = module(input)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 93, in forward
    return F.linear(input, self.weight, self.bias)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/functional.py", line 1690, in linear
    ret = torch.addmm(bias, input, weight.t())
 (Triggered internally at  /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:104.)
  Variable._execution_engine.run_backward(
Traceback (most recent call last):
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 333, in update_policy
    policy_loss.backward(retain_graph = True)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py", line 130, in backward
    Variable._execution_engine.run_backward(
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [135, 30]], which is output 0 of TBackward, is at version 2; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!
2021-05-04 01:01:22.941826: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-04 01:01:22.941876: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1620070316.405700540, 23.505000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1620070316.408875709, 23.507000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1620070316.409168484, 23.507000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1620070317.911553888, 24.668000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1620070319.084650476, 25.601000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1620070320.057025478, 26.401000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1620070321.039665308, 27.200000000]: Ready to take commands for planning group back_left_leg.[0m
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000000: mean_reward:99.03951877838165[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 0: episode_reward:9393.607828666863 steps:4[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
Updating
rddpg_torch.py:35: FutureWarning: arrays to stack must be passed as a "sequence" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.
  rdpg.train(params['train_episode_count'], checkpoint_path, True)
updating done
Updating
/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:130: UserWarning: Error detected in AddmmBackward. Traceback of forward call that caused the error:
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 318, in update_policy
    action, (robot_enc_state, z) = self.agent.actor(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 287, in forward
    action, robot_enc_state, z = self.cell(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 258, in forward
    rs_r = self.robot_state_enc(self.robot_enc_state)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/container.py", line 117, in forward
    input = module(input)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 93, in forward
    return F.linear(input, self.weight, self.bias)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/functional.py", line 1690, in linear
    ret = torch.addmm(bias, input, weight.t())
 (Triggered internally at  /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:104.)
  Variable._execution_engine.run_backward(
Traceback (most recent call last):
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 334, in update_policy
    policy_loss.backward(retain_graph = True)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py", line 130, in backward
    Variable._execution_engine.run_backward(
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [135, 30]], which is output 0 of TBackward, is at version 2; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!
2021-05-04 01:06:14.653204: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-04 01:06:14.653363: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
Traceback (most recent call last):
  File "rddpg_torch.py", line 24, in <module>
    env = Env(
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/env.py", line 12, in __init__
    self.quadruped = Quadruped(params, experiment)
  File "/home/shandilya/Desktop/CNS/DDP/src/simulations/ws/src/quadruped/scripts/quadruped.py", line 485, in __init__
    self.all_legs = AllLegs(
  File "/home/shandilya/Desktop/CNS/DDP/src/simulations/ws/src/quadruped/scripts/quadruped.py", line 296, in __init__
    self.front_left = Leg(
  File "/home/shandilya/Desktop/CNS/DDP/src/simulations/ws/src/quadruped/scripts/quadruped.py", line 44, in __init__
    self.jta = actionlib.SimpleActionClient(
  File "/opt/ros/noetic/lib/python3/dist-packages/actionlib/simple_action_client.py", line 55, in __init__
    self.action_client = ActionClient(ns, ActionSpec)
  File "/opt/ros/noetic/lib/python3/dist-packages/actionlib/action_client.py", line 521, in __init__
    self.pub_queue_size = rospy.get_param('actionlib_client_pub_queue_size', 10)
  File "/opt/ros/noetic/lib/python3/dist-packages/rospy/client.py", line 467, in get_param
    return _param_server[param_name] #MasterProxy does all the magic for us
  File "/opt/ros/noetic/lib/python3/dist-packages/rospy/msproxy.py", line 121, in __getitem__
    code, msg, value = self.target.getParam(rospy.names.get_caller_id(), resolved_key)
  File "/usr/lib/python3.8/xmlrpc/client.py", line 1109, in __call__
    return self.__send(self.__name, args)
  File "/opt/ros/noetic/lib/python3/dist-packages/rospy/core.py", line 669, in _ServerProxy__request
    return xmlrpcclient.ServerProxy._ServerProxy__request(
  File "/usr/lib/python3.8/xmlrpc/client.py", line 1450, in __request
    response = self.__transport.request(
  File "/usr/lib/python3.8/xmlrpc/client.py", line 1153, in request
    return self.single_request(host, handler, request_body, verbose)
  File "/usr/lib/python3.8/xmlrpc/client.py", line 1165, in single_request
    http_conn = self.send_request(host, handler, request_body, verbose)
  File "/usr/lib/python3.8/xmlrpc/client.py", line 1278, in send_request
    self.send_content(connection, request_body)
  File "/usr/lib/python3.8/xmlrpc/client.py", line 1308, in send_content
    connection.endheaders(request_body)
  File "/usr/lib/python3.8/http/client.py", line 1250, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.8/http/client.py", line 1010, in _send_output
    self.send(msg)
  File "/usr/lib/python3.8/http/client.py", line 950, in send
    self.connect()
  File "/usr/lib/python3.8/http/client.py", line 921, in connect
    self.sock = self._create_connection(
  File "/usr/lib/python3.8/socket.py", line 808, in create_connection
    raise err
  File "/usr/lib/python3.8/socket.py", line 796, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused
2021-05-04 01:09:36.724446: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-04 01:09:36.724495: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1620070783.900787738, 1.275000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1620070783.902548773, 1.277000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1620070783.902778873, 1.277000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1620070785.434440628, 2.517000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1620070786.590196273, 3.402000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1620070787.549148085, 4.201000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1620070788.880656219, 5.200000000]: Ready to take commands for planning group back_left_leg.[0m
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000000: mean_reward:2787.8994590351367[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 0: episode_reward:78.28762402920412 steps:2[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 1: episode_reward:101.85525546923391 steps:4[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 2: episode_reward:-1.3588967226562485 steps:5[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[93m [RDDPG] Step_0000005: mean_reward:20943.280074953076[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Updating Policy
Updating
rddpg_torch.py:35: FutureWarning: arrays to stack must be passed as a "sequence" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.
  rdpg.train(params['train_episode_count'], checkpoint_path, True)
updating done
Updating
/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:130: UserWarning: Error detected in EluBackward. Traceback of forward call that caused the error:
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 318, in update_policy
    action, (robot_enc_state, z) = self.agent.actor(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 287, in forward
    action, robot_enc_state, z = self.cell(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 258, in forward
    rs_r = self.robot_state_enc(self.robot_enc_state)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/container.py", line 117, in forward
    input = module(input)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 477, in forward
    return F.elu(input, self.alpha, self.inplace)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/functional.py", line 1227, in elu
    result = torch._C._nn.elu(input, alpha)
 (Triggered internally at  /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:104.)
  Variable._execution_engine.run_backward(
Traceback (most recent call last):
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 334, in update_policy
    policy_loss.backward()
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py", line 130, in backward
    Variable._execution_engine.run_backward(
RuntimeError: Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time.
2021-05-04 01:18:41.043609: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-04 01:18:41.043657: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1620071328.723493092, 1.257000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1620071328.728754037, 1.260000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1620071328.729156102, 1.260000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1620071330.342371074, 2.426000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1620071331.378387787, 3.200000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1620071332.212815499, 3.800000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1620071333.317377134, 4.602000000]: Ready to take commands for planning group back_left_leg.[0m
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000000: mean_reward:4401.243456848027[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 0: episode_reward:101131.00703493913 steps:5[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000005: mean_reward:6190.032976451959[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Updating Policy
Updating
rddpg_torch.py:35: FutureWarning: arrays to stack must be passed as a "sequence" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.
  rdpg.train(params['train_episode_count'], checkpoint_path, True)
updating done
Updating
/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:130: UserWarning: Error detected in EluBackward. Traceback of forward call that caused the error:
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 312, in update_policy
    action, (robot_enc_state, z) = self.agent.actor(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 287, in forward
    action, robot_enc_state, z = self.cell(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 258, in forward
    rs_r = self.robot_state_enc(self.robot_enc_state)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/container.py", line 117, in forward
    input = module(input)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 477, in forward
    return F.elu(input, self.alpha, self.inplace)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/functional.py", line 1227, in elu
    result = torch._C._nn.elu(input, alpha)
 (Triggered internally at  /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:104.)
  Variable._execution_engine.run_backward(
Traceback (most recent call last):
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 334, in update_policy
    policy_loss.backward()
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py", line 130, in backward
    Variable._execution_engine.run_backward(
RuntimeError: Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time.
2021-05-04 01:25:47.239453: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-04 01:25:47.239503: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1620071757.204398914, 3.895000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1620071757.207053932, 3.895000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1620071757.207492576, 3.895000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1620071758.819379170, 5.138000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1620071759.916461158, 6.001000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1620071760.895117536, 6.804000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1620071761.859960444, 7.601000000]: Ready to take commands for planning group back_left_leg.[0m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000000: mean_reward:162.4772577839621[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 0: episode_reward:116.29623610537342 steps:2[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 1: episode_reward:-1.1595475228881835 steps:3[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 2: episode_reward:9.739214749988044 steps:5[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[93m [RDDPG] Step_0000005: mean_reward:71765.44775389899[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
Updating
rddpg_torch.py:35: FutureWarning: arrays to stack must be passed as a "sequence" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.
  rdpg.train(params['train_episode_count'], checkpoint_path, True)
updating done
Updating
/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:130: UserWarning: Error detected in EluBackward. Traceback of forward call that caused the error:
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 312, in update_policy
    action, (robot_enc_state, z) = self.agent.actor(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 287, in forward
    action, robot_enc_state, z = self.cell(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 258, in forward
    rs_r = self.robot_state_enc(self.robot_enc_state)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/container.py", line 117, in forward
    input = module(input)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 477, in forward
    return F.elu(input, self.alpha, self.inplace)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/functional.py", line 1227, in elu
    result = torch._C._nn.elu(input, alpha)
 (Triggered internally at  /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:104.)
  Variable._execution_engine.run_backward(
Traceback (most recent call last):
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 334, in update_policy
    policy_loss.backward(retain_graph = True)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py", line 130, in backward
    Variable._execution_engine.run_backward(
RuntimeError: Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time.
2021-05-04 01:30:00.109801: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-04 01:30:00.109849: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1620072007.408122036, 0.916000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1620072007.411416483, 0.917000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1620072007.412283615, 0.917000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1620072009.093335534, 2.006000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1620072010.171161236, 2.801000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1620072011.270790783, 3.600000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1620072012.426978058, 4.401000000]: Ready to take commands for planning group back_left_leg.[0m
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000000: mean_reward:49.285529309244204[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 0: episode_reward:-1.7489388190917938 steps:1[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 1: episode_reward:16035.987588406557 steps:4[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 2: episode_reward:-1.871559084472655 steps:5[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000005: mean_reward:5815.1616008978635[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
Updating
rddpg_torch.py:35: FutureWarning: arrays to stack must be passed as a "sequence" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.
  rdpg.train(params['train_episode_count'], checkpoint_path, True)
updating done
Updating
/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:130: UserWarning: Error detected in AddmmBackward. Traceback of forward call that caused the error:
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 312, in update_policy
    action, (robot_enc_state, z) = self.agent.actor(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 287, in forward
    action, robot_enc_state, z = self.cell(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 258, in forward
    rs_r = self.robot_state_enc(self.robot_enc_state)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/container.py", line 117, in forward
    input = module(input)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 93, in forward
    return F.linear(input, self.weight, self.bias)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/functional.py", line 1690, in linear
    ret = torch.addmm(bias, input, weight.t())
 (Triggered internally at  /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:104.)
  Variable._execution_engine.run_backward(
Traceback (most recent call last):
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 334, in update_policy
    policy_loss.backward(retain_graph = True)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py", line 130, in backward
    Variable._execution_engine.run_backward(
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [135, 30]], which is output 0 of TBackward, is at version 2; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!
2021-05-04 01:41:55.766426: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-04 01:41:55.766476: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1620072722.761680353, 1.221000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1620072722.764426612, 1.223000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1620072722.764576823, 1.223000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1620072724.319456619, 2.482000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1620072725.481177008, 3.400000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1620072726.419274423, 4.200000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1620072727.355275628, 5.002000000]: Ready to take commands for planning group back_left_leg.[0m
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000000: mean_reward:354.4953321769623[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 0: episode_reward:-1.3226425185546826 steps:1[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 1: episode_reward:3.3883183742016953 steps:3[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 2: episode_reward:4.532581881154591 steps:5[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000005: mean_reward:2764.183472739865[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Updating Policy
Updating
rddpg_torch.py:35: FutureWarning: arrays to stack must be passed as a "sequence" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.
  rdpg.train(params['train_episode_count'], checkpoint_path, True)
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Episode Done
[92m [RDDPG] 3: episode_reward:-1.9680018078613348 steps:6[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:130: UserWarning: Error detected in AddmmBackward. Traceback of forward call that caused the error:
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 315, in update_policy
    action, (robot_enc_state, z) = self.agent.actor(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 287, in forward
    action, robot_enc_state, z = self.cell(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 261, in forward
    self.pretrain_cell(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 201, in forward
    actions, Z, _, _ = self.rhythm_gen(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 187, in forward
    out = self.complex_mlp(z)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/container.py", line 117, in forward
    input = module(input)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 74, in forward
    return apply_complex(self.fc_r, self.fc_i, input)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 46, in apply_complex
    fr(y) + fi(x)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 93, in forward
    return F.linear(input, self.weight, self.bias)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/functional.py", line 1690, in linear
    ret = torch.addmm(bias, input, weight.t())
 (Triggered internally at  /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:104.)
  Variable._execution_engine.run_backward(
Traceback (most recent call last):
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 347, in update_policy
    policy_loss_total.backward()
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py", line 130, in backward
    Variable._execution_engine.run_backward(
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [100, 12]], which is output 0 of TBackward, is at version 7; expected version 6 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!
2021-05-04 01:50:19.932111: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-04 01:50:19.932148: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1620073229.358150984, 0.938000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1620073229.359643582, 0.939000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1620073229.359766167, 0.939000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1620073230.723412464, 1.733000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1620073232.465702365, 2.634000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1620073234.017329390, 3.435000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1620073235.510699036, 4.233000000]: Ready to take commands for planning group back_left_leg.[0m
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000000: mean_reward:96.06188860478407[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 0: episode_reward:-1.9423287048339721 steps:1[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 1: episode_reward:15.936382013727687 steps:3[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 2: episode_reward:24.86252674402938 steps:5[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000005: mean_reward:7538.915405192367[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Updating Policy
Updating
rddpg_torch.py:35: FutureWarning: arrays to stack must be passed as a "sequence" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.
  rdpg.train(params['train_episode_count'], checkpoint_path, True)
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Episode Done
[92m [RDDPG] 3: episode_reward:-2.0966352770995815 steps:6[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Last Step of episode
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Episode Done
[92m [RDDPG] 4: episode_reward:39769.811681425825 steps:12[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Last Step of episode
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Episode Done
[92m [RDDPG] 5: episode_reward:27194.108027295995 steps:18[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Last Step of episode
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Episode Done
[92m [RDDPG] 6: episode_reward:33170.06662380176 steps:24[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Last Step of episode
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Episode Done
[92m [RDDPG] 7: episode_reward:30052.85901144458 steps:30[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000030: mean_reward:81138.8715624433[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 8: episode_reward:-1.450777116699229 steps:31[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Last Step of episode
[RDDPG] Episode Done
[92m [RDDPG] 9: episode_reward:38699.567390188764 steps:37[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Last Step of episode
[RDDPG] Episode Done
[92m [RDDPG] 10: episode_reward:15091.63557881974 steps:43[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Last Step of episode
[RDDPG] Episode Done
[92m [RDDPG] 11: episode_reward:990.2358875427656 steps:49[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Last Step of episode
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 12: episode_reward:40941.82231978553 steps:55[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000055: mean_reward:4771.37890240026[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 13: episode_reward:4.29817136269605 steps:57[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Last Step of episode
[RDDPG] Episode Done
[92m [RDDPG] 14: episode_reward:61328.47953503886 steps:63[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Last Step of episode
[RDDPG] Episode Done
[92m [RDDPG] 15: episode_reward:17299.121636769472 steps:69[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Last Step of episode
[RDDPG] Episode Done
[92m [RDDPG] 16: episode_reward:582.1247494585903 steps:75[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000075: mean_reward:5186.761178156142[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Episode Done
[92m [RDDPG] 17: episode_reward:-1.4828904299316767 steps:76[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Last Step of episode
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Episode Done
[92m [RDDPG] 18: episode_reward:13343.139018841635 steps:82[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Last Step of episode
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Episode Done
[92m [RDDPG] 19: episode_reward:13692.046240448479 steps:88[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Last Step of episode
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Episode Done
[92m [RDDPG] 20: episode_reward:25221.883913193793 steps:94[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Last Step of episode
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Episode Done
[92m [RDDPG] 21: episode_reward:87287.78016801851 steps:100[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000100: mean_reward:13978.953734992505[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 22: episode_reward:-1.689288602539002 steps:101[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Last Step of episode
[RDDPG] Episode Done
[92m [RDDPG] 23: episode_reward:69819.64017597778 steps:107[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Last Step of episode
[RDDPG] Episode Done
[92m [RDDPG] 24: episode_reward:48653.57996306676 steps:113[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Last Step of episode
[RDDPG] Episode Done
[92m [RDDPG] 25: episode_reward:11804.037649862448 steps:119[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Quadruped Not Upright
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Episode Done
[92m [RDDPG] 26: episode_reward:42096.68377309579 steps:124[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Last Step of episode
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Episode Done
[92m [RDDPG] 27: episode_reward:110118.52399516033 steps:130[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
Traceback (most recent call last):
  File "/opt/ros/noetic/lib/python3/dist-packages/rospy/impl/tcpros_base.py", line 562, in connect
    self.read_header()
  File "/opt/ros/noetic/lib/python3/dist-packages/rospy/impl/tcpros_base.py", line 657, in read_header
    self._validate_header(read_ros_handshake_header(sock, self.read_buff, self.protocol.buff_size))
  File "/opt/ros/noetic/lib/python3/dist-packages/rosgraph/network.py", line 357, in read_ros_handshake_header
    d = sock.recv(buff_size)
ConnectionResetError: [Errno 104] Connection reset by peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/ros/noetic/lib/python3/dist-packages/rospy/impl/tcpros_service.py", line 509, in call
    transport.connect(dest_addr, dest_port, service_uri)
  File "/opt/ros/noetic/lib/python3/dist-packages/rospy/impl/tcpros_base.py", line 589, in connect
    raise TransportInitError(str(e)) #re-raise i/o error
rospy.exceptions.TransportInitError: [Errno 104] Connection reset by peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 192, in train
    validate_reward = self.evaluate(
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/evaluator.py", line 37, in __call__
    observation, reward, done, info = env.step(
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/env.py", line 43, in step
    self.quadruped.set_support_lines()
  File "/home/shandilya/Desktop/CNS/DDP/src/simulations/ws/src/quadruped/scripts/quadruped.py", line 1022, in set_support_lines
    current_pose = self.kinematics.get_end_effector_fk(
  File "/home/shandilya/Desktop/CNS/DDP/src/simulations/ws/src/quadruped/scripts/kinematics.py", line 84, in get_end_effector_fk
    msg = self.compute_fk_proxy(
  File "/opt/ros/noetic/lib/python3/dist-packages/rospy/impl/tcpros_service.py", line 442, in __call__
    return self.call(*args, **kwds)
  File "/opt/ros/noetic/lib/python3/dist-packages/rospy/impl/tcpros_service.py", line 512, in call
    raise ServiceException("unable to connect to service: %s"%e)
rospy.service.ServiceException: unable to connect to service: [Errno 104] Connection reset by peer
2021-05-04 02:09:26.142825: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-04 02:09:26.142862: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1620074384.826950509, 8.435000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1620074384.829574376, 8.436000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1620074384.830091062, 8.437000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1620074386.330981187, 9.386000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1620074387.780875546, 10.294000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1620074388.945397065, 11.094000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1620074390.114629339, 11.894000000]: Ready to take commands for planning group back_left_leg.[0m
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[93m [RDDPG] Step_0000000: mean_reward:17854.705107645983[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 0: episode_reward:-1.76068157812501 steps:1[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 1: episode_reward:5.387875132402882 steps:3[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 2: episode_reward:-1.7723629765625162 steps:4[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
Updating
rddpg_torch.py:35: FutureWarning: arrays to stack must be passed as a "sequence" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.
  rdpg.train(params['train_episode_count'], checkpoint_path, True)
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Last Step of episode
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Episode Done
[92m [RDDPG] 3: episode_reward:32986.42144933438 steps:10[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
Traceback (most recent call last):
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 192, in train
    validate_reward = self.evaluate(
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/evaluator.py", line 37, in __call__
    observation, reward, done, info = env.step(
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/env.py", line 36, in step
    observation =  self.quadruped.step(
  File "/home/shandilya/Desktop/CNS/DDP/src/simulations/ws/src/quadruped/scripts/quadruped.py", line 1216, in step
    self.set_observation(action, desired_motion)
  File "/home/shandilya/Desktop/CNS/DDP/src/simulations/ws/src/quadruped/scripts/quadruped.py", line 1073, in set_observation
    rospy.wait_for_service('/gazebo/get_model_state')
  File "/opt/ros/noetic/lib/python3/dist-packages/rospy/impl/tcpros_service.py", line 166, in wait_for_service
    raise ROSInterruptException("rospy shutdown")
rospy.exceptions.ROSInterruptException: rospy shutdown
2021-05-04 02:13:40.601590: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-04 02:13:40.601637: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1620074627.779526864, 1.288000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1620074627.786464488, 1.289000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1620074627.787753291, 1.290000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1620074629.462452309, 2.401000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1620074630.341358736, 2.966000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1620074631.482900651, 3.765000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1620074632.373980128, 4.365000000]: Ready to take commands for planning group back_left_leg.[0m
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000000: mean_reward:875.4974806661132[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 0: episode_reward:-596.5795077704729 steps:2[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 1: episode_reward:-290.4248115234332 steps:3[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
rddpg_torch.py:35: FutureWarning: arrays to stack must be passed as a "sequence" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.
  rdpg.train(params['train_episode_count'], checkpoint_path, True)
[RDDPG] Updating Policy
[RDDPG] Last Step of episode
[RDDPG] Episode Done
[92m [RDDPG] 2: episode_reward:34296.04619270422 steps:9[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Updating Policy
[RDDPG] Updating Policy
[RDDPG] Last Step of episode
[RDDPG] Episode Done
[92m [RDDPG] 3: episode_reward:20614.58666233369 steps:15[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
Traceback (most recent call last):
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 192, in train
    validate_reward = self.evaluate(
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/evaluator.py", line 22, in __call__
    observation = env.reset()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/env.py", line 16, in reset
    self._state, self._reward = self.quadruped.reset()
  File "/home/shandilya/Desktop/CNS/DDP/src/simulations/ws/src/quadruped/scripts/quadruped.py", line 958, in reset
    rospy.sleep(2.0)
  File "/opt/ros/noetic/lib/python3/dist-packages/rospy/timer.py", line 165, in sleep
    raise rospy.exceptions.ROSInterruptException("ROS shutdown request")
rospy.exceptions.ROSInterruptException: ROS shutdown request
2021-05-04 02:19:08.346186: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-04 02:19:08.346233: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1620074955.795017516, 1.478000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1620074955.796776868, 1.479000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1620074955.796904087, 1.479000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1620074957.239381506, 2.413000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1620074957.896874004, 2.866000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1620074958.722451159, 3.466000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1620074959.655460755, 4.065000000]: Ready to take commands for planning group back_left_leg.[0m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000000: mean_reward:1532.1245702644865[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 0: episode_reward:-597.794805959667 steps:2[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 1: episode_reward:-261.15305126952876 steps:3[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 2: episode_reward:-45.42006349182143 steps:4[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 3: episode_reward:5174.459528823608 steps:7[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 4: episode_reward:-70.21879174804661 steps:8[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 5: episode_reward:-196.23515954589928 steps:9[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 6: episode_reward:-685.8505503021024 steps:11[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 7: episode_reward:-417.2279057655011 steps:13[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 8: episode_reward:-488.4508845188526 steps:16[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 9: episode_reward:8825.33192418847 steps:19[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 10: episode_reward:-345.46053841293315 steps:21[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 11: episode_reward:30267.712087416836 steps:25[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[93m [RDDPG] Step_0000025: mean_reward:55781.17250912765[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 12: episode_reward:16508.161367769702 steps:31[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 13: episode_reward:4320.9761961495 steps:35[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 14: episode_reward:-359.2090993651137 steps:36[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 15: episode_reward:4226.268390012268 steps:40[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 16: episode_reward:-311.93177929687687 steps:41[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 17: episode_reward:-534.9196761698986 steps:43[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 18: episode_reward:-237.6176860351046 steps:44[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 19: episode_reward:10195.383686742922 steps:48[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 20: episode_reward:614.578669316762 steps:51[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 21: episode_reward:-253.57293444828198 steps:52[00m
[RDDPG] Resetting Environment
Traceback (most recent call last):
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 130, in train
    state, reward, done, info = self.env.step(
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/env.py", line 36, in step
    observation =  self.quadruped.step(
  File "/home/shandilya/Desktop/CNS/DDP/src/simulations/ws/src/quadruped/scripts/quadruped.py", line 1217, in step
    rospy.sleep(15.0/60.0)
  File "/opt/ros/noetic/lib/python3/dist-packages/rospy/timer.py", line 165, in sleep
    raise rospy.exceptions.ROSInterruptException("ROS shutdown request")
rospy.exceptions.ROSInterruptException: ROS shutdown request
2021-05-04 02:47:48.086793: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-04 02:47:48.086841: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1620076675.000006128, 1.380000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1620076675.002528237, 1.382000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1620076675.002627293, 1.382000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1620076676.719531078, 2.494000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1620076677.465085593, 3.003000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1620076678.288888205, 3.602000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1620076679.127273432, 4.203000000]: Ready to take commands for planning group back_left_leg.[0m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000000: mean_reward:1824.419394240902[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 0: episode_reward:-484.877484937361 steps:2[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 1: episode_reward:-320.2624773874046 steps:4[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 2: episode_reward:-201.19856542968628 steps:5[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 3: episode_reward:-276.3161565677496 steps:7[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 4: episode_reward:-258.65957109803026 steps:9[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 5: episode_reward:-592.9240318915627 steps:11[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 6: episode_reward:11943.162347244195 steps:14[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 7: episode_reward:-422.6756544706833 steps:16[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 8: episode_reward:-162.29966290283812 steps:17[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 9: episode_reward:13243.326517568308 steps:21[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 10: episode_reward:-301.08929233402824 steps:23[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 11: episode_reward:-449.4674345365022 steps:25[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 12: episode_reward:-410.6484021840348 steps:28[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 13: episode_reward:731.1018934185772 steps:31[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 14: episode_reward:-376.9946596543331 steps:33[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 15: episode_reward:-109.44174203490715 steps:34[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 16: episode_reward:-504.04550298865115 steps:36[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 17: episode_reward:-255.74228637695387 steps:37[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 18: episode_reward:-655.3736188644301 steps:39[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 19: episode_reward:-604.8061503151112 steps:42[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 20: episode_reward:-459.00810826425226 steps:45[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 21: episode_reward:-146.1538090209942 steps:46[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 22: episode_reward:-102.88332815551657 steps:47[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 23: episode_reward:-86.35416815186026 steps:48[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 24: episode_reward:20413.404498351472 steps:55[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 25: episode_reward:-106.16930187988834 steps:56[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 26: episode_reward:-146.2552530517586 steps:57[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 27: episode_reward:-462.10064172536175 steps:59[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 28: episode_reward:-123.14889312744828 steps:60[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 29: episode_reward:-416.99984213561197 steps:63[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 30: episode_reward:-71.20402935791282 steps:64[00m
[RDDPG] Resetting Environment
2021-05-04 02:53:31.477353: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-04 02:53:31.477527: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 31: episode_reward:-222.82081611775547 steps:66[00m
[RDDPG] Resetting Environment
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
shutdown request: [/joint_position_node_exp45] Reason: new node registered with same name
[DDPG] Waiting for joint trajectory action
Traceback (most recent call last):
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 114, in train
    state0 = deepcopy(self.env.reset())
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/env.py", line 16, in reset
    self._state, self._reward = self.quadruped.reset()
  File "/home/shandilya/Desktop/CNS/DDP/src/simulations/ws/src/quadruped/scripts/quadruped.py", line 857, in reset
    rospy.sleep(1.0)
  File "/opt/ros/noetic/lib/python3/dist-packages/rospy/timer.py", line 165, in sleep
    raise rospy.exceptions.ROSInterruptException("ROS shutdown request")
rospy.exceptions.ROSInterruptException: ROS shutdown request
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1620077048.277478926, 176.922000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1620077048.280581240, 176.922000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1620077048.283502113, 176.923000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
Traceback (most recent call last):
  File "rddpg_torch.py", line 24, in <module>
    env = Env(
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/env.py", line 12, in __init__
    self.quadruped = Quadruped(params, experiment)
  File "/home/shandilya/Desktop/CNS/DDP/src/simulations/ws/src/quadruped/scripts/quadruped.py", line 489, in __init__
    self.kinematics = Kinematics(
  File "/home/shandilya/Desktop/CNS/DDP/src/simulations/ws/src/quadruped/scripts/kinematics.py", line 21, in __init__
    self.front_right_leg = moveit_commander.MoveGroupCommander(
  File "/opt/ros/noetic/lib/python3/dist-packages/moveit_commander/move_group.py", line 53, in __init__
    self._g = _moveit_move_group_interface.MoveGroupInterface(name, robot_description, ns, wait_for_servers)
RuntimeError: Unable to connect to move_group action server 'move_group' within allotted time (5s)
2021-05-04 02:54:33.784048: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-04 02:54:33.784106: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1620077081.353706099, 0.926000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1620077081.355693119, 0.927000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1620077081.356904114, 0.928000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1620077083.099792902, 1.972000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1620077084.388604968, 2.801000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1620077085.598146596, 3.600000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1620077086.814809525, 4.400000000]: Ready to take commands for planning group back_left_leg.[0m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000000: mean_reward:122.53482245520988[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 0: episode_reward:-106.6422441406257 steps:1[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 1: episode_reward:-671.4686103445936 steps:3[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 2: episode_reward:3996.696917994505 steps:7[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 3: episode_reward:-266.6070459488014 steps:9[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 4: episode_reward:-511.7176724130174 steps:11[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 5: episode_reward:-346.88170907556844 steps:13[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 6: episode_reward:-218.0959011230479 steps:14[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 7: episode_reward:-185.8490412597667 steps:15[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 8: episode_reward:-142.52913014975783 steps:17[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 9: episode_reward:-317.415116088869 steps:18[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 10: episode_reward:-120.3634191284174 steps:19[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 11: episode_reward:-517.3038848345498 steps:21[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 12: episode_reward:4105.995734102757 steps:26[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 13: episode_reward:-230.74901293945115 steps:27[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 14: episode_reward:-182.83607714843887 steps:28[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 15: episode_reward:-361.22931562148324 steps:30[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 16: episode_reward:-471.35609003477055 steps:32[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 17: episode_reward:6883.973951674704 steps:35[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 18: episode_reward:-89.13134252929547 steps:36[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 19: episode_reward:-150.1651852814736 steps:38[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 20: episode_reward:-492.8923203612002 steps:40[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 21: episode_reward:-441.6059567188595 steps:42[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 22: episode_reward:297.11894221811554 steps:45[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 23: episode_reward:-211.88127038573515 steps:46[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 24: episode_reward:-245.33187695312597 steps:47[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 25: episode_reward:-121.88960321044678 steps:48[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 26: episode_reward:6292.59850910332 steps:51[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 27: episode_reward:-568.9191014822761 steps:53[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 28: episode_reward:-181.9237138671887 steps:54[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 29: episode_reward:-147.6189512281258 steps:56[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 30: episode_reward:10292.639367413565 steps:59[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 31: episode_reward:-423.04769551048764 steps:61[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 32: episode_reward:-35.198352447509144 steps:62[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 33: episode_reward:-477.9926632380865 steps:64[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 34: episode_reward:14677.945212248049 steps:67[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 35: episode_reward:-75.16330619812395 steps:68[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 36: episode_reward:-417.4621140344318 steps:70[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 37: episode_reward:1165.858389603299 steps:73[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 38: episode_reward:-288.92387419749105 steps:75[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 39: episode_reward:-559.5339521818081 steps:77[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 40: episode_reward:-370.6483323286903 steps:79[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 41: episode_reward:5387.659125778613 steps:82[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 42: episode_reward:-314.115813663625 steps:84[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 43: episode_reward:8280.990022351434 steps:87[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 44: episode_reward:-83.78896005249211 steps:88[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 45: episode_reward:-514.1123739745185 steps:90[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 46: episode_reward:382.7026564146778 steps:93[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 47: episode_reward:-229.8218520507977 steps:94[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 48: episode_reward:-106.08504150390675 steps:95[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 49: episode_reward:-138.41535828687162 steps:97[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 50: episode_reward:9601.8978384341 steps:100[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 51: episode_reward:1682.0085460705498 steps:103[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 52: episode_reward:-526.3292421009044 steps:105[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 53: episode_reward:5328.634836900884 steps:109[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 54: episode_reward:1603.963303956636 steps:112[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 55: episode_reward:317.39327568287496 steps:115[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 56: episode_reward:-194.80727757441676 steps:117[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 57: episode_reward:4004.483026521872 steps:120[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 58: episode_reward:-405.5301559659134 steps:122[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 59: episode_reward:-165.40783886720484 steps:123[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 60: episode_reward:-121.2007862548837 steps:124[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 61: episode_reward:-204.91960314940744 steps:125[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 62: episode_reward:3623.4823257634407 steps:129[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 63: episode_reward:-512.4983594322642 steps:131[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 64: episode_reward:-306.3774938073095 steps:133[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 65: episode_reward:-93.87372491454494 steps:134[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 66: episode_reward:-343.6285762389191 steps:136[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 67: episode_reward:-175.88116380349675 steps:138[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 68: episode_reward:-216.56065434568802 steps:140[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 69: episode_reward:-88.38845214844082 steps:141[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 70: episode_reward:-424.03774361592536 steps:143[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 71: episode_reward:-184.0053043823148 steps:144[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 72: episode_reward:-357.55529157419346 steps:146[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 73: episode_reward:-231.66581217535173 steps:148[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 74: episode_reward:-266.37429508547206 steps:150[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 75: episode_reward:-258.6574400591339 steps:153[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 76: episode_reward:11089.785348307552 steps:156[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 77: episode_reward:-420.453896101096 steps:158[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 78: episode_reward:-209.42566687928735 steps:160[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 79: episode_reward:2845.649477461854 steps:163[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 80: episode_reward:257.15409614632136 steps:166[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 81: episode_reward:-292.7820751589081 steps:168[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 82: episode_reward:-288.54838821024543 steps:170[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 83: episode_reward:-52.201423645016526 steps:171[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 84: episode_reward:16432.00189727764 steps:174[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 85: episode_reward:19710.85552952941 steps:179[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 86: episode_reward:-585.2898203135131 steps:182[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 87: episode_reward:7591.511515016881 steps:185[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 88: episode_reward:-255.31265974590087 steps:187[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 89: episode_reward:-309.96692110871095 steps:189[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 90: episode_reward:-232.63432519530582 steps:190[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 91: episode_reward:-329.699793090816 steps:191[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 92: episode_reward:-62.83509381103575 steps:192[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 93: episode_reward:-173.17060224301414 steps:194[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 94: episode_reward:-242.6478020679413 steps:196[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 95: episode_reward:-582.7178190732672 steps:198[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 96: episode_reward:8623.534399655302 steps:202[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 97: episode_reward:-529.2515846779767 steps:205[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 98: episode_reward:4708.433491823233 steps:208[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 99: episode_reward:-107.22195874022174 steps:209[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 100: episode_reward:1027.187997676958 steps:212[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 101: episode_reward:-132.26653967285267 steps:213[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 102: episode_reward:-705.3316771544211 steps:216[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 103: episode_reward:-714.8532697963135 steps:218[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 104: episode_reward:-83.26445666504499 steps:219[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 105: episode_reward:-244.3805618695906 steps:221[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 106: episode_reward:51174.33804326503 steps:225[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 107: episode_reward:-529.4545707777207 steps:227[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 108: episode_reward:-580.4480717771321 steps:229[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 109: episode_reward:-267.3261264648857 steps:230[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 110: episode_reward:-740.9431579375544 steps:232[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 111: episode_reward:-162.68075878907868 steps:233[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 112: episode_reward:-117.90799859618234 steps:234[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 113: episode_reward:-486.92499160768136 steps:236[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 114: episode_reward:-199.38050317382633 steps:237[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 115: episode_reward:-206.6487521972766 steps:238[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 116: episode_reward:-433.7672373421334 steps:240[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 117: episode_reward:-291.07733064678223 steps:242[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 118: episode_reward:-141.7682279052411 steps:243[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 119: episode_reward:-916.2535851845134 steps:246[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 120: episode_reward:-136.51248962402425 steps:247[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 121: episode_reward:-48.5810695037988 steps:248[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 122: episode_reward:-552.4289991223753 steps:250[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 123: episode_reward:-270.6387502668855 steps:252[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 124: episode_reward:-421.6204793720231 steps:254[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 125: episode_reward:15717.008405631806 steps:258[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 126: episode_reward:-222.61851599123992 steps:259[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 127: episode_reward:4768.523227659785 steps:263[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 128: episode_reward:-191.49207275390452 steps:264[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 129: episode_reward:9664.068170348377 steps:269[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 130: episode_reward:-353.12909876021484 steps:271[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 131: episode_reward:-260.61286303713604 steps:272[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 132: episode_reward:-203.78763867188266 steps:273[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 133: episode_reward:-143.2595199279816 steps:274[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 134: episode_reward:-143.5761889343461 steps:275[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 135: episode_reward:-243.17585840529765 steps:277[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 136: episode_reward:-533.9582352041213 steps:279[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 137: episode_reward:-254.94790821737223 steps:281[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 138: episode_reward:-70.05496051024525 steps:282[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 139: episode_reward:8297.7343206307 steps:287[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 140: episode_reward:-263.50009266317204 steps:289[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 141: episode_reward:-179.65016796871444 steps:290[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 142: episode_reward:24889.757241050775 steps:294[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 143: episode_reward:5680.659292641163 steps:298[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 144: episode_reward:-294.71921236115674 steps:300[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 145: episode_reward:-214.3094868164012 steps:301[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 146: episode_reward:6290.925069085362 steps:306[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 147: episode_reward:-128.20214117431476 steps:307[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 148: episode_reward:6045.057139529468 steps:310[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 149: episode_reward:-173.2420977172993 steps:311[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 150: episode_reward:-672.1938046304044 steps:313[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 151: episode_reward:-288.75846649622406 steps:315[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 152: episode_reward:22976.200336613452 steps:319[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 153: episode_reward:-469.2895071248966 steps:321[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 154: episode_reward:-555.8253281519496 steps:323[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 155: episode_reward:-581.3136269113013 steps:325[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 156: episode_reward:-159.75406439206614 steps:326[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 157: episode_reward:-214.9222321606328 steps:328[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 158: episode_reward:-154.25851538085035 steps:329[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 159: episode_reward:-41.28183456421798 steps:330[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 160: episode_reward:-431.52721196415257 steps:333[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 161: episode_reward:-55.461640319822166 steps:334[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 162: episode_reward:-126.49054376219966 steps:335[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 163: episode_reward:-344.48058605110987 steps:337[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 164: episode_reward:-126.83624139403557 steps:338[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 165: episode_reward:4043.878765438739 steps:342[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 166: episode_reward:-413.7685574481257 steps:344[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 167: episode_reward:-181.6176009521968 steps:345[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 168: episode_reward:-1086.3004326178889 steps:347[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 169: episode_reward:-199.46350055571122 steps:349[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 170: episode_reward:-116.54435123859213 steps:352[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 171: episode_reward:-232.08731439780078 steps:354[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 172: episode_reward:-226.56334454351264 steps:355[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 173: episode_reward:-625.7228563731143 steps:357[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 174: episode_reward:-158.27189831547486 steps:358[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 175: episode_reward:-165.87921899409577 steps:359[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 176: episode_reward:2257.5503410532174 steps:362[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 177: episode_reward:-221.0746763915274 steps:363[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 178: episode_reward:-321.50644760070924 steps:365[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 179: episode_reward:27522.470902314075 steps:371[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 180: episode_reward:-472.69232082529714 steps:373[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 181: episode_reward:11000.654946904304 steps:376[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 182: episode_reward:-261.89236421665873 steps:378[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 183: episode_reward:-102.10411093141514 steps:379[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 184: episode_reward:-617.046144845418 steps:381[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 185: episode_reward:-310.681457077377 steps:383[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 186: episode_reward:-364.8988761964829 steps:385[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 187: episode_reward:-472.9888611520636 steps:387[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 188: episode_reward:-619.7064525037474 steps:389[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 189: episode_reward:-37.12992989966435 steps:391[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 190: episode_reward:3088.88992594009 steps:394[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 191: episode_reward:-331.5901779197414 steps:396[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 192: episode_reward:1604.4526055667425 steps:399[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 193: episode_reward:-510.44620376267557 steps:401[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
rddpg_torch.py:35: FutureWarning: arrays to stack must be passed as a "sequence" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.
  rdpg.train(params['train_episode_count'], checkpoint_path, True)
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 194: episode_reward:36608.45854056779 steps:423[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 195: episode_reward:16058.850993188596 steps:430[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 196: episode_reward:60827.092417074986 steps:449[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 197: episode_reward:11821.562533592145 steps:454[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 198: episode_reward:13639.30163487089 steps:459[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 199: episode_reward:5900.366657760486 steps:464[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 200: episode_reward:5215.195860423254 steps:469[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 201: episode_reward:7086.510290222608 steps:474[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 202: episode_reward:-162.08832503623998 steps:479[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 203: episode_reward:17685.20436539049 steps:495[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 204: episode_reward:9433.880197410266 steps:504[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 205: episode_reward:5234.74896563529 steps:512[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 206: episode_reward:3198.073702685382 steps:521[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 207: episode_reward:3245.567189588152 steps:529[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 208: episode_reward:12577.246986415978 steps:538[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 209: episode_reward:10673.001256525273 steps:550[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 210: episode_reward:20416.36378323588 steps:562[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 211: episode_reward:20373.82368592329 steps:572[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 212: episode_reward:12098.540356261181 steps:579[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 213: episode_reward:12582.673970050513 steps:589[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Updating Policy
[RDDPG] Episode Done
[92m [RDDPG] 214: episode_reward:18748.209561019023 steps:600[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 215: episode_reward:-138.72242364631202 steps:604[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 216: episode_reward:-115.89367752929701 steps:608[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 217: episode_reward:-133.47287014108576 steps:612[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 218: episode_reward:3823.963911629744 steps:627[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 219: episode_reward:1434.3156047636312 steps:638[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 220: episode_reward:7958.818952072301 steps:644[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 221: episode_reward:25426.065764524832 steps:664[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 222: episode_reward:18647.751072117426 steps:678[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 223: episode_reward:-93.04200216896919 steps:685[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 224: episode_reward:6644.96705028856 steps:693[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 225: episode_reward:-178.82201517743349 steps:699[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 226: episode_reward:-41.481581793567585 steps:710[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 227: episode_reward:6885.455268064313 steps:722[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 228: episode_reward:8519.248787668404 steps:734[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 229: episode_reward:28641.053927187855 steps:744[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 230: episode_reward:10230.590156690432 steps:752[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 231: episode_reward:13190.688234056553 steps:762[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 232: episode_reward:19854.90922201255 steps:770[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 233: episode_reward:-206.5177263253525 steps:776[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 234: episode_reward:-2829.955114654879 steps:791[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 235: episode_reward:-97.80400901606748 steps:795[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 236: episode_reward:6230.776935199501 steps:799[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 237: episode_reward:24580.403915970073 steps:810[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 238: episode_reward:19184.817125780126 steps:819[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 239: episode_reward:50981.26946985073 steps:832[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Updating Policy
[RDDPG] Updating Policy
[RDDPG] Updating Policy
[RDDPG] Updating Policy
[RDDPG] Updating Policy
[RDDPG] Updating Policy
[RDDPG] Updating Policy
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 240: episode_reward:194707.98476612577 steps:983[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 241: episode_reward:12764.586943682001 steps:995[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 242: episode_reward:1533.8837677605566 steps:1008[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 243: episode_reward:7287.344073969253 steps:1021[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 244: episode_reward:357.0424442000741 steps:1028[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 245: episode_reward:8158.1305805301945 steps:1048[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 246: episode_reward:14379.29723215474 steps:1066[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 247: episode_reward:28075.53484403311 steps:1078[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 248: episode_reward:8562.427078831895 steps:1089[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 249: episode_reward:-61.14432758500094 steps:1092[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Updating Policy
[RDDPG] Updating Policy
[RDDPG] Updating Policy
[RDDPG] Updating Policy
[RDDPG] Updating Policy
[RDDPG] Updating Policy
[RDDPG] Updating Policy
[RDDPG] Updating Policy
[RDDPG] Updating Policy
[RDDPG] Last Step of episode
[RDDPG] Episode Done
[92m [RDDPG] 250: episode_reward:157365.01467860112 steps:1292[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 251: episode_reward:-137.19264072847642 steps:1295[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 252: episode_reward:7227.708465489055 steps:1303[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Updating Policy
[RDDPG] Updating Policy
[RDDPG] Updating Policy
[RDDPG] Updating Policy
[RDDPG] Updating Policy
[RDDPG] Updating Policy
[RDDPG] Updating Policy
[RDDPG] Updating Policy
[RDDPG] Updating Policy
[RDDPG] Last Step of episode
[RDDPG] Episode Done
[92m [RDDPG] 253: episode_reward:227079.02377929687 steps:1503[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 254: episode_reward:32312.155860455794 steps:1519[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 255: episode_reward:34842.646787531514 steps:1535[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 256: episode_reward:-99.22117362194389 steps:1539[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 257: episode_reward:-2046.7480785320336 steps:1548[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 258: episode_reward:190.2604484468673 steps:1559[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 259: episode_reward:9877.910990058623 steps:1566[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 260: episode_reward:26655.14843372475 steps:1575[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 261: episode_reward:34537.81559369884 steps:1588[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 262: episode_reward:2236.7918001163357 steps:1598[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 263: episode_reward:191.03913871470814 steps:1604[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 264: episode_reward:-189.59547849499356 steps:1614[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Updating Policy
[RDDPG] Updating Policy
[RDDPG] Updating Policy
[RDDPG] Updating Policy
[RDDPG] Updating Policy
[RDDPG] Updating Policy
[RDDPG] Updating Policy
[RDDPG] Updating Policy
[RDDPG] Updating Policy
[RDDPG] Last Step of episode
[RDDPG] Episode Done
[92m [RDDPG] 265: episode_reward:312436.89277712494 steps:1814[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 266: episode_reward:12706.494536160146 steps:1822[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 267: episode_reward:5118.093170145003 steps:1835[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 268: episode_reward:42300.96018139583 steps:1850[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 269: episode_reward:12185.673912355158 steps:1855[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 270: episode_reward:20877.64564474786 steps:1864[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 271: episode_reward:16729.157837029874 steps:1871[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 272: episode_reward:27893.637323325518 steps:1887[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
2021-05-04 12:59:25.322712: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-04 12:59:25.322975: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2021-05-04 13:00:14.325690: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-04 13:00:14.325739: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1620113501.592114230, 62.029000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1620113501.598608360, 62.032000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1620113501.598709244, 62.032000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1620113503.512998793, 62.945000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1620113504.751092903, 63.601000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1620113505.432289739, 64.007000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1620113506.237514449, 64.401000000]: Ready to take commands for planning group back_left_leg.[0m
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000000: mean_reward:1157.6134677658035[00m
[RDDPG] Resetting Environment
[RDDPG] Last Step of episode
[RDDPG] Episode Done
[92m [RDDPG] 0: episode_reward:17611.850098424962 steps:4[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 1: episode_reward:-396.96168925701465 steps:6[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 2: episode_reward:-484.95307672701347 steps:8[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 3: episode_reward:-96.85408593749891 steps:9[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 4: episode_reward:-77.34208251953177 steps:10[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 5: episode_reward:-147.16944378661609 steps:11[00m
[RDDPG] Resetting Environment
[RDDPG] Last Step of episode
[RDDPG] Updating Policy
rddpg_torch.py:35: FutureWarning: arrays to stack must be passed as a "sequence" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.
  rdpg.train(params['train_episode_count'], checkpoint_path, True)
/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:130: UserWarning: Error detected in EluBackward. Traceback of forward call that caused the error:
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 314, in update_policy
    action, (robot_enc_state, z) = self.agent.actor(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 287, in forward
    action, robot_enc_state, z = self.cell(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 258, in forward
    rs_r = self.robot_state_enc(self.robot_enc_state)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/container.py", line 117, in forward
    input = module(input)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 477, in forward
    return F.elu(input, self.alpha, self.inplace)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/functional.py", line 1227, in elu
    result = torch._C._nn.elu(input, alpha)
 (Triggered internally at  /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:104.)
  Variable._execution_engine.run_backward(
Traceback (most recent call last):
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 336, in update_policy
    policy_loss.backward()
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py", line 130, in backward
    Variable._execution_engine.run_backward(
RuntimeError: Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time.
2021-05-04 13:06:45.937981: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-04 13:06:45.938017: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1620113813.289006925, 1.116000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1620113813.291766971, 1.118000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1620113813.292011009, 1.118000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1620113814.850826626, 2.133000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1620113816.272063121, 3.000000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1620113817.557558387, 3.801000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1620113818.825810684, 4.600000000]: Ready to take commands for planning group back_left_leg.[0m
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000000: mean_reward:756.8055723758283[00m
[RDDPG] Resetting Environment
[RDDPG] Last Step of episode
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 0: episode_reward:-664.8161435660701 steps:4[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 1: episode_reward:-691.1982902793518 steps:6[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 2: episode_reward:-288.5565187522129 steps:8[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 3: episode_reward:-81.316551635741 steps:9[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 4: episode_reward:-516.9676497486041 steps:11[00m
[RDDPG] Resetting Environment
[RDDPG] Last Step of episode
[RDDPG] Updating Policy
rddpg_torch.py:35: FutureWarning: arrays to stack must be passed as a "sequence" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.
  rdpg.train(params['train_episode_count'], checkpoint_path, True)
/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:130: UserWarning: Error detected in EluBackward. Traceback of forward call that caused the error:
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 314, in update_policy
    action, (robot_enc_state, z) = self.agent.actor(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 287, in forward
    action, robot_enc_state, z = self.cell(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 258, in forward
    rs_r = self.robot_state_enc(self.robot_enc_state)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/container.py", line 117, in forward
    input = module(input)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 477, in forward
    return F.elu(input, self.alpha, self.inplace)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/functional.py", line 1227, in elu
    result = torch._C._nn.elu(input, alpha)
 (Triggered internally at  /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:104.)
  Variable._execution_engine.run_backward(
Traceback (most recent call last):
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 336, in update_policy
    policy_loss.backward()#retain_graph = True)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py", line 130, in backward
    Variable._execution_engine.run_backward(
RuntimeError: Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time.
2021-05-04 13:20:42.041830: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-04 13:20:42.041886: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2021-05-04 13:23:09.830357: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-04 13:23:09.830397: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1620114796.719966116, 1.042000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1620114796.723154944, 1.045000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1620114796.723295391, 1.045000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1620114798.248942648, 2.164000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1620114799.429658893, 3.002000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1620114800.507592802, 3.802000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1620114801.542857469, 4.603000000]: Ready to take commands for planning group back_left_leg.[0m
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[93m [RDDPG] Step_0000000: mean_reward:665.6388646424573[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 0: episode_reward:-312.75856678214996 steps:2[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 1: episode_reward:243.8706258738826 steps:5[00m
[RDDPG] Resetting Environment
[RDDPG] Last Step of episode
[RDDPG] Episode Done
[92m [RDDPG] 2: episode_reward:19915.749425672147 steps:9[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 3: episode_reward:-365.8895598251662 steps:11[00m
[RDDPG] Resetting Environment
[RDDPG] Last Step of episode
[RDDPG] Updating Policy
Traceback (most recent call last):
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 238, in update_policy
    start = time.perf_counter()
NameError: name 'time' is not defined
2021-05-04 13:27:34.840964: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-04 13:27:34.841015: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1620115062.399940934, 1.146000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1620115062.404723776, 1.155000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1620115062.405258278, 1.156000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1620115064.016092967, 2.216000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1620115064.914675622, 2.809000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1620115065.786992784, 3.408000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1620115066.429247299, 3.808000000]: Ready to take commands for planning group back_left_leg.[0m
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000000: mean_reward:-173.31352596398835[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 0: episode_reward:-100.5466450195299 steps:1[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 1: episode_reward:-259.13573668046837 steps:3[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 2: episode_reward:-575.5324602961402 steps:5[00m
[RDDPG] Resetting Environment
[RDDPG] Last Step of episode
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 3: episode_reward:2096.2872830600936 steps:9[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 4: episode_reward:-248.43687939452596 steps:10[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 5: episode_reward:-287.65971630859315 steps:11[00m
[RDDPG] Resetting Environment
[RDDPG] Last Step of episode
[RDDPG] Updating Policy
rddpg_torch.py:35: FutureWarning: arrays to stack must be passed as a "sequence" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.
  rdpg.train(params['train_episode_count'], checkpoint_path, True)
[RDDPG] Update Time: 3.25616
[RDDPG] Episode Done
[92m [RDDPG] 6: episode_reward:-20.11053491113137 steps:15[00m
[RDDPG] Resetting Environment
[RDDPG] Last Step of episode
[RDDPG] Episode Done
[92m [RDDPG] 7: episode_reward:-103.9444161058523 steps:19[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Update Time: 2.95366
[RDDPG] Last Step of episode
[RDDPG] Episode Done
[92m [RDDPG] 8: episode_reward:-116.2352833412819 steps:23[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Update Time: 3.75133
[RDDPG] Last Step of episode
[RDDPG] Episode Done
[92m [RDDPG] 9: episode_reward:-146.60021381006322 steps:27[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Update Time: 3.32343
[RDDPG] Last Step of episode
[RDDPG] Episode Done
[92m [RDDPG] 10: episode_reward:-160.8634620058292 steps:31[00m
[RDDPG] Resetting Environment
[RDDPG] Last Step of episode
[RDDPG] Updating Policy
[RDDPG] Update Time: 3.92043
[RDDPG] Episode Done
[92m [RDDPG] 11: episode_reward:-133.16577859008478 steps:35[00m
[RDDPG] Resetting Environment
[RDDPG] Last Step of episode
[RDDPG] Episode Done
[92m [RDDPG] 12: episode_reward:-143.18603701298179 steps:39[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Update Time: 3.66823
[RDDPG] Last Step of episode
[RDDPG] Episode Done
[92m [RDDPG] 13: episode_reward:4289.476230063934 steps:43[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Update Time: 4.20726
[RDDPG] Last Step of episode
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 14: episode_reward:6710.992867201611 steps:47[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Update Time: 2.73009
[RDDPG] Last Step of episode
[RDDPG] Episode Done
[92m [RDDPG] 15: episode_reward:17586.481139730226 steps:51[00m
[RDDPG] Resetting Environment
[RDDPG] Last Step of episode
[RDDPG] Updating Policy
[RDDPG] Update Time: 3.97870
[RDDPG] Episode Done
[92m [RDDPG] 16: episode_reward:3318.5238710895455 steps:55[00m
[RDDPG] Resetting Environment
[RDDPG] Last Step of episode
[RDDPG] Episode Done
[92m [RDDPG] 17: episode_reward:4469.216596594433 steps:59[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Update Time: 3.64892
[RDDPG] Last Step of episode
[RDDPG] Episode Done
[92m [RDDPG] 18: episode_reward:4029.448273193282 steps:63[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Update Time: 3.04644
[RDDPG] Last Step of episode
[RDDPG] Episode Done
[92m [RDDPG] 19: episode_reward:2984.514922093608 steps:67[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Update Time: 3.23297
[RDDPG] Last Step of episode
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 20: episode_reward:3579.7027051901887 steps:71[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 21: episode_reward:-158.76540319621915 steps:74[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Update Time: 3.53863
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 22: episode_reward:-217.69405686610355 steps:77[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Updating Policy
[RDDPG] Update Time: 3.76020
[RDDPG] Episode Done
[92m [RDDPG] 23: episode_reward:-138.0495282121596 steps:80[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 24: episode_reward:6211.190595974148 steps:83[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Update Time: 3.52898
[RDDPG] Last Step of episode
[RDDPG] Episode Done
[92m [RDDPG] 25: episode_reward:14883.533406524293 steps:87[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Updating Policy
[RDDPG] Update Time: 3.18240
[RDDPG] Episode Done
[92m [RDDPG] 26: episode_reward:-129.80811755200654 steps:90[00m
[RDDPG] Resetting Environment
[RDDPG] Last Step of episode
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 27: episode_reward:13140.114134101947 steps:94[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Update Time: 3.55718
[RDDPG] Last Step of episode
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 28: episode_reward:9511.709100723678 steps:98[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Update Time: 2.40869
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 29: episode_reward:6857.917609309164 steps:101[00m
[RDDPG] Resetting Environment
[RDDPG] Last Step of episode
[RDDPG] Quadruped Not Upright
[RDDPG] Updating Policy
[RDDPG] Update Time: 3.43177
[RDDPG] Episode Done
[92m [RDDPG] 30: episode_reward:6799.046273900292 steps:105[00m
[RDDPG] Resetting Environment
[RDDPG] Last Step of episode
[RDDPG] Episode Done
[92m [RDDPG] 31: episode_reward:13178.12229744108 steps:109[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Update Time: 3.10184
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 32: episode_reward:2067.51160055456 steps:112[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 33: episode_reward:-247.02494070088636 steps:114[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Update Time: 2.97429
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 34: episode_reward:-164.8688285251772 steps:116[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 35: episode_reward:-218.3629531973408 steps:118[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Updating Policy
[RDDPG] Update Time: 2.43844
[RDDPG] Episode Done
[92m [RDDPG] 36: episode_reward:-270.8974874748637 steps:120[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 37: episode_reward:6695.118832465937 steps:123[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Updating Policy
[RDDPG] Update Time: 3.39513
[RDDPG] Episode Done
[92m [RDDPG] 38: episode_reward:-185.03279524410917 steps:125[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 39: episode_reward:162.66440796520538 steps:128[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Update Time: 3.35544
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 40: episode_reward:11719.596239072878 steps:131[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 41: episode_reward:-339.15811782380723 steps:133[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Update Time: 3.43790
[RDDPG] Last Step of episode
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 42: episode_reward:10318.132766790623 steps:137[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Updating Policy
[RDDPG] Update Time: 2.80126
[RDDPG] Episode Done
[92m [RDDPG] 43: episode_reward:1673.2423108120913 steps:140[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 44: episode_reward:-340.2690786132289 steps:141[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 45: episode_reward:-323.9621168623556 steps:143[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 46: episode_reward:-271.5378586425836 steps:144[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Updating Policy
[RDDPG] Update Time: 3.81851
[RDDPG] Episode Done
[92m [RDDPG] 47: episode_reward:-142.24956631470488 steps:145[00m
[RDDPG] Resetting Environment
[RDDPG] Last Step of episode
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 48: episode_reward:7097.900315622462 steps:149[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Updating Policy
[RDDPG] Update Time: 3.25280
[RDDPG] Episode Done
[92m [RDDPG] 49: episode_reward:-190.35870251465863 steps:150[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 50: episode_reward:-186.23915423583816 steps:151[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 51: episode_reward:-359.32621411134795 steps:152[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 52: episode_reward:-363.9868710937124 steps:153[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 53: episode_reward:-349.3289853516003 steps:154[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Updating Policy
[RDDPG] Update Time: 2.95841
[RDDPG] Episode Done
[92m [RDDPG] 54: episode_reward:-279.1541706543119 steps:155[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 55: episode_reward:-233.19184185792267 steps:156[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 56: episode_reward:-386.4943964843374 steps:157[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 57: episode_reward:-143.07746359253687 steps:158[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 58: episode_reward:-359.3916860351459 steps:159[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Updating Policy
[RDDPG] Update Time: 3.13990
[RDDPG] Episode Done
[92m [RDDPG] 59: episode_reward:-132.79067590330615 steps:160[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 60: episode_reward:-199.00942578124577 steps:161[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 61: episode_reward:27.41670636395805 steps:164[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Updating Policy
[RDDPG] Update Time: 3.60411
[RDDPG] Episode Done
[92m [RDDPG] 62: episode_reward:-380.07420703118953 steps:165[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 63: episode_reward:-437.02795849609583 steps:166[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 64: episode_reward:-340.1653462985694 steps:168[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Updating Policy
[RDDPG] Update Time: 2.19038
[RDDPG] Episode Done
[92m [RDDPG] 65: episode_reward:-479.95394092643994 steps:170[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 66: episode_reward:12023.236186329026 steps:173[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Updating Policy
[RDDPG] Update Time: 3.01963
[RDDPG] Episode Done
[92m [RDDPG] 67: episode_reward:-203.43377971734958 steps:175[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 68: episode_reward:-253.5721801260231 steps:177[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 69: episode_reward:-394.2856128738164 steps:179[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Update Time: 3.29165
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 70: episode_reward:-296.9523279828226 steps:181[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 71: episode_reward:-163.38005290866133 steps:183[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Updating Policy
[RDDPG] Update Time: 2.28024
[RDDPG] Episode Done
[92m [RDDPG] 72: episode_reward:-340.5339634899714 steps:185[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 73: episode_reward:-334.5567415941681 steps:187[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 74: episode_reward:-341.88306424750624 steps:189[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Update Time: 2.97078
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 75: episode_reward:-248.87680655613445 steps:191[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 76: episode_reward:-285.6900429715155 steps:193[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Updating Policy
[RDDPG] Update Time: 2.56154
[RDDPG] Episode Done
[92m [RDDPG] 77: episode_reward:-300.4529810851303 steps:195[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 78: episode_reward:-267.648673731807 steps:197[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 79: episode_reward:-406.75003995676786 steps:199[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Update Time: 3.16111
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 80: episode_reward:-249.0748824261055 steps:201[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 81: episode_reward:-321.84007375995486 steps:203[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Updating Policy
[RDDPG] Update Time: 3.45773
[RDDPG] Episode Done
[92m [RDDPG] 82: episode_reward:-131.9614984959877 steps:205[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 83: episode_reward:-208.683709597155 steps:207[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 84: episode_reward:-337.9066372775564 steps:209[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Update Time: 4.06839
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 85: episode_reward:-315.2141964030933 steps:211[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 86: episode_reward:-173.33537727068267 steps:213[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Updating Policy
[RDDPG] Update Time: 3.21713
[RDDPG] Episode Done
[92m [RDDPG] 87: episode_reward:-288.6717499410129 steps:215[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 88: episode_reward:-170.51600343561196 steps:217[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 89: episode_reward:-273.45830105177964 steps:219[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Update Time: 2.39333
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 90: episode_reward:-162.6220476385228 steps:221[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 91: episode_reward:-273.1841590976527 steps:223[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Updating Policy
[RDDPG] Update Time: 3.13636
[RDDPG] Episode Done
[92m [RDDPG] 92: episode_reward:-290.9418908460671 steps:225[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 93: episode_reward:-276.4244242604584 steps:227[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 94: episode_reward:-61.46349437355516 steps:229[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Update Time: 3.04458
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 95: episode_reward:-207.3854695407056 steps:231[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 96: episode_reward:-310.9272595541736 steps:233[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Updating Policy
[RDDPG] Update Time: 2.99484
[RDDPG] Episode Done
[92m [RDDPG] 97: episode_reward:-117.36164097111472 steps:235[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 98: episode_reward:-235.42536874115564 steps:237[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 99: episode_reward:-260.2319629048822 steps:239[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Update Time: 3.13477
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 100: episode_reward:-340.9755545509125 steps:241[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 101: episode_reward:-292.449729629932 steps:243[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Updating Policy
[RDDPG] Update Time: 4.01518
[RDDPG] Episode Done
[92m [RDDPG] 102: episode_reward:-270.5255306162984 steps:245[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 103: episode_reward:-268.01876893192104 steps:247[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 104: episode_reward:-304.84599659616197 steps:249[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Update Time: 2.86411
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 105: episode_reward:-272.820634583804 steps:251[00m
[RDDPG] Resetting Environment
Traceback (most recent call last):
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 131, in train
    state, reward, done, info = self.env.step(
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/env.py", line 36, in step
    observation =  self.quadruped.step(
  File "/home/shandilya/Desktop/CNS/DDP/src/simulations/ws/src/quadruped/scripts/quadruped.py", line 1217, in step
    rospy.sleep(15.0/60.0)
  File "/opt/ros/noetic/lib/python3/dist-packages/rospy/timer.py", line 165, in sleep
    raise rospy.exceptions.ROSInterruptException("ROS shutdown request")
rospy.exceptions.ROSInterruptException: ROS shutdown request
2021-05-04 13:49:42.931054: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-04 13:49:42.931103: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1620116389.900293191, 1.090000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1620116389.903165709, 1.092000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1620116389.903501170, 1.092000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1620116391.436118175, 2.154000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1620116392.285906991, 2.800000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1620116393.432899000, 3.601000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1620116394.521878667, 4.401000000]: Ready to take commands for planning group back_left_leg.[0m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[93m [RDDPG] Step_0000000: mean_reward:6352.298474408503[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 0: episode_reward:-405.7542963971527 steps:2[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 1: episode_reward:3682.1072418932854 steps:6[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 2: episode_reward:-121.72213787841866 steps:7[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 3: episode_reward:-240.52749737549323 steps:8[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 4: episode_reward:-121.3113931274418 steps:9[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 5: episode_reward:-205.01366406249565 steps:10[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 6: episode_reward:-128.15677221679965 steps:11[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 7: episode_reward:-630.2210873936788 steps:13[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 8: episode_reward:-80.38568835449267 steps:14[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 9: episode_reward:-157.62266839599468 steps:15[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 10: episode_reward:-66.70131481933844 steps:16[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 11: episode_reward:-638.8068824611462 steps:18[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 12: episode_reward:-694.1399100259814 steps:20[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 13: episode_reward:-150.97095037706885 steps:22[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 14: episode_reward:11449.009322289468 steps:26[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 15: episode_reward:-283.7624096627156 steps:29[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 16: episode_reward:-378.1617672118987 steps:30[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 17: episode_reward:-183.24294366455462 steps:31[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 18: episode_reward:13534.1436844989 steps:35[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 19: episode_reward:-377.613388756775 steps:37[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 20: episode_reward:-267.5926167439174 steps:39[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 21: episode_reward:-128.4053293762189 steps:40[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 22: episode_reward:-152.20112200926522 steps:41[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 23: episode_reward:7257.185661043537 steps:44[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 24: episode_reward:-94.96406860351317 steps:45[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 25: episode_reward:212.7136909552949 steps:48[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 26: episode_reward:-267.5162698233017 steps:50[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 27: episode_reward:-257.50364608978794 steps:52[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 28: episode_reward:-376.43162937225105 steps:54[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 29: episode_reward:-98.65363903808647 steps:55[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 30: episode_reward:-58.864989669797446 steps:56[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 31: episode_reward:-292.3485288453369 steps:58[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 32: episode_reward:-269.03589746093894 steps:59[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 33: episode_reward:-644.8378008321777 steps:61[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 34: episode_reward:-396.1726571485547 steps:63[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 35: episode_reward:-217.92217187500037 steps:64[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 36: episode_reward:-805.0768513549012 steps:66[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 37: episode_reward:-555.0833643497622 steps:68[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 38: episode_reward:-383.6931199219834 steps:70[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 39: episode_reward:-585.2210592935229 steps:72[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 40: episode_reward:-316.25670962368207 steps:74[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 41: episode_reward:-321.2675195738422 steps:76[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 42: episode_reward:-68.19567979431098 steps:77[00m
[RDDPG] Resetting Environment
Traceback (most recent call last):
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 115, in train
    state0 = deepcopy(self.env.reset())
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/env.py", line 16, in reset
    self._state, self._reward = self.quadruped.reset()
  File "/home/shandilya/Desktop/CNS/DDP/src/simulations/ws/src/quadruped/scripts/quadruped.py", line 958, in reset
    #rospy.sleep(2.0)
  File "/opt/ros/noetic/lib/python3/dist-packages/rospy/timer.py", line 165, in sleep
    raise rospy.exceptions.ROSInterruptException("ROS shutdown request")
rospy.exceptions.ROSInterruptException: ROS shutdown request
2021-05-04 13:58:05.442308: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-04 13:58:05.442362: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1620116892.596757197, 1.000000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1620116892.599039673, 1.001000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1620116892.599374955, 1.001000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1620116894.013774562, 2.030000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1620116895.131120978, 2.801000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1620116896.230236019, 3.601000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1620116897.311753740, 4.400000000]: Ready to take commands for planning group back_left_leg.[0m
[RDDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
Traceback (most recent call last):
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 88, in train
    validate_reward = self.evaluate(
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/evaluator.py", line 22, in __call__
    observation = env.reset()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/env.py", line 16, in reset
    self._state, self._reward = self.quadruped.reset()
  File "/home/shandilya/Desktop/CNS/DDP/src/simulations/ws/src/quadruped/scripts/quadruped.py", line 965, in reset
    A, B = AB
ValueError: not enough values to unpack (expected 2, got 1)
2021-05-04 13:59:41.892961: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-04 13:59:41.892995: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1620116988.339703922, 1.004000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1620116988.342987847, 1.005000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1620116988.343338505, 1.005000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1620116990.188908486, 2.257000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1620116991.179739614, 3.001000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1620116992.289250066, 3.801000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1620116993.377244276, 4.601000000]: Ready to take commands for planning group back_left_leg.[0m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[93m [RDDPG] Step_0000000: mean_reward:31154.018759291474[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 0: episode_reward:8158.70338909847 steps:3[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 1: episode_reward:27187.075344285506 steps:11[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 2: episode_reward:22879.549483034178 steps:15[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 3: episode_reward:40187.24205738 steps:24[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 4: episode_reward:4873.515501589055 steps:26[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 5: episode_reward:4293.054029262193 steps:28[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 6: episode_reward:24649.565458131747 steps:32[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 7: episode_reward:22523.464315406618 steps:36[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 8: episode_reward:5101.274457143429 steps:39[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 9: episode_reward:2932.5254138157493 steps:41[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 10: episode_reward:11480.279430831137 steps:44[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 11: episode_reward:11620.463864692496 steps:47[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 12: episode_reward:10137.841031473787 steps:50[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 13: episode_reward:5240.9491704114935 steps:52[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 14: episode_reward:6154.47617738491 steps:54[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 15: episode_reward:4352.99962198431 steps:58[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 16: episode_reward:6545.652555579224 steps:60[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 17: episode_reward:26887.023478396906 steps:64[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 18: episode_reward:7557.615319770049 steps:67[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 19: episode_reward:3844.833504170856 steps:69[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 20: episode_reward:17090.581136231194 steps:75[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 21: episode_reward:21935.134709322767 steps:80[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 22: episode_reward:9862.450032006256 steps:83[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 23: episode_reward:-123.43721740724071 steps:84[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 24: episode_reward:17423.371366840798 steps:87[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 25: episode_reward:42796.22994367374 steps:94[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 26: episode_reward:2409.150938881294 steps:96[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 27: episode_reward:4220.726446614157 steps:98[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 28: episode_reward:-246.16518024852112 steps:100[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 29: episode_reward:21355.889712121294 steps:108[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
rddpg_torch.py:35: FutureWarning: arrays to stack must be passed as a "sequence" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.
  rdpg.train(params['train_episode_count'], checkpoint_path, True)
[RDDPG] Update Time: 10.52126
[RDDPG] Last Step of episode
[RDDPG] Quadruped Not Upright
[RDDPG] Updating Policy
[RDDPG] Update Time: 11.77425
[RDDPG] Episode Done
[92m [RDDPG] 30: episode_reward:26514.004994096522 steps:120[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Update Time: 12.97662
[RDDPG] Last Step of episode
[RDDPG] Episode Done
[92m [RDDPG] 31: episode_reward:30017.97154698303 steps:132[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 32: episode_reward:5922.697025847687 steps:134[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Update Time: 11.60221
[RDDPG] Last Step of episode
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 33: episode_reward:39765.24990670321 steps:146[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Update Time: 9.36009
[RDDPG] Last Step of episode
[RDDPG] Episode Done
[92m [RDDPG] 34: episode_reward:28286.076265118583 steps:158[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Update Time: 8.73382
[RDDPG] Last Step of episode
[RDDPG] Quadruped Not Upright
[RDDPG] Updating Policy
[RDDPG] Update Time: 6.30744
[RDDPG] Episode Done
[92m [RDDPG] 35: episode_reward:21514.354100835502 steps:170[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 36: episode_reward:1945.756108395894 steps:172[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Update Time: 12.74436
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 37: episode_reward:7002.773904374634 steps:181[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Update Time: 10.95640
[RDDPG] Last Step of episode
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 38: episode_reward:34725.274731306235 steps:193[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 39: episode_reward:8138.46340636908 steps:198[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Update Time: 12.45322
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 40: episode_reward:5462.457347256696 steps:204[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 41: episode_reward:11968.421214808388 steps:209[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Update Time: 10.86574
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 42: episode_reward:3776.439634373421 steps:214[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Update Time: 8.07723
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 43: episode_reward:7923.520271567668 steps:223[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Update Time: 12.04508
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 44: episode_reward:17148.993028991652 steps:232[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 45: episode_reward:6440.471740877716 steps:235[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Updating Policy
[RDDPG] Update Time: 12.50675
[RDDPG] Episode Done
[92m [RDDPG] 46: episode_reward:10290.850211075254 steps:240[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 47: episode_reward:1750.3700463259731 steps:244[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 48: episode_reward:2011.1161109473471 steps:249[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Update Time: 1.68316
Traceback (most recent call last):
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 131, in train
    state, reward, done, info = self.env.step(
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/env.py", line 36, in step
    observation =  self.quadruped.step(
  File "/home/shandilya/Desktop/CNS/DDP/src/simulations/ws/src/quadruped/scripts/quadruped.py", line 1216, in step
    self.set_observation(action, desired_motion)
  File "/home/shandilya/Desktop/CNS/DDP/src/simulations/ws/src/quadruped/scripts/quadruped.py", line 1073, in set_observation
    rospy.wait_for_service('/gazebo/get_model_state')
  File "/opt/ros/noetic/lib/python3/dist-packages/rospy/impl/tcpros_service.py", line 166, in wait_for_service
    raise ROSInterruptException("rospy shutdown")
rospy.exceptions.ROSInterruptException: rospy shutdown
2021-05-04 14:08:16.497485: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-04 14:08:16.497524: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1620117503.588179977, 1.225000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1620117503.593651838, 1.227000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1620117503.594087600, 1.227000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1620117505.109905652, 2.304000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1620117506.240039389, 3.084000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1620117507.379898388, 3.885000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1620117508.463548773, 4.684000000]: Ready to take commands for planning group back_left_leg.[0m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000000: mean_reward:18313.66800707766[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 0: episode_reward:10326.286360359993 steps:4[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 1: episode_reward:1776.4988793814302 steps:6[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 2: episode_reward:12388.13794999711 steps:9[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 3: episode_reward:6454.567056474539 steps:11[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 4: episode_reward:2819.8827851869783 steps:13[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 5: episode_reward:3559.716074722042 steps:16[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 6: episode_reward:18309.18552602581 steps:21[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 7: episode_reward:52572.83811720019 steps:30[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 8: episode_reward:4762.704506222974 steps:32[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 9: episode_reward:8379.822790048938 steps:34[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 10: episode_reward:14206.54282311995 steps:38[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 11: episode_reward:29159.886565010183 steps:42[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 12: episode_reward:3335.620394132768 steps:44[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 13: episode_reward:15996.84612777935 steps:46[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 14: episode_reward:21427.963575014706 steps:52[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 15: episode_reward:5006.822920010278 steps:54[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 16: episode_reward:15969.091275711717 steps:58[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 17: episode_reward:55470.87728307046 steps:63[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 18: episode_reward:16211.994411453676 steps:68[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 19: episode_reward:15692.528527686467 steps:71[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 20: episode_reward:89918.64969709575 steps:79[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 21: episode_reward:10521.4711351621 steps:81[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 22: episode_reward:13328.378854988649 steps:85[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 23: episode_reward:7769.830755294766 steps:87[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 24: episode_reward:6503.609060319815 steps:89[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 25: episode_reward:17224.99084786401 steps:92[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 26: episode_reward:6238.232719508508 steps:95[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 27: episode_reward:5501.031066834199 steps:97[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 28: episode_reward:2192.9301119707097 steps:100[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 29: episode_reward:1508.9132965041817 steps:102[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 30: episode_reward:2371.647595515136 steps:104[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 31: episode_reward:13551.5004950996 steps:108[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 32: episode_reward:9715.226254104316 steps:111[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 33: episode_reward:7038.934854292181 steps:113[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 34: episode_reward:8476.932996762222 steps:117[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 35: episode_reward:23893.57052866758 steps:121[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 36: episode_reward:1349.0174939235233 steps:123[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 37: episode_reward:6579.414793535035 steps:129[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 38: episode_reward:12616.027175391038 steps:133[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 39: episode_reward:8143.781269402647 steps:136[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 40: episode_reward:5905.75414360126 steps:138[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 41: episode_reward:6556.549283075453 steps:141[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 42: episode_reward:8165.597151824458 steps:144[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 43: episode_reward:10522.116778693133 steps:149[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 44: episode_reward:3705.804382873383 steps:152[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 45: episode_reward:10361.56272541785 steps:154[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 46: episode_reward:781.529281728368 steps:156[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 47: episode_reward:2128.8279012257276 steps:158[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 48: episode_reward:12859.181199295463 steps:160[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 49: episode_reward:12669.438678376095 steps:165[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 50: episode_reward:7472.463107455854 steps:167[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 51: episode_reward:1556.5173339652285 steps:170[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 52: episode_reward:3370.6917008169976 steps:173[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 53: episode_reward:-94.87903906251557 steps:174[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 54: episode_reward:3385.0932609700913 steps:176[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 55: episode_reward:5516.994669857575 steps:178[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 56: episode_reward:2436.9701519948526 steps:181[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 57: episode_reward:1617.0711532701396 steps:184[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 58: episode_reward:1014.899053147152 steps:186[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 59: episode_reward:10549.463583018905 steps:189[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 60: episode_reward:5368.61323644176 steps:193[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 61: episode_reward:2884.6691302420268 steps:195[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 62: episode_reward:5066.604361153111 steps:198[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 63: episode_reward:-111.75917724609162 steps:199[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 64: episode_reward:-74.83110107422968 steps:200[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 65: episode_reward:5632.659496831703 steps:202[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 66: episode_reward:19616.50390258208 steps:205[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 67: episode_reward:2152.0825040314394 steps:207[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 68: episode_reward:51785.18420591032 steps:212[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 69: episode_reward:30937.832805024438 steps:218[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 70: episode_reward:8547.587844538546 steps:220[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 71: episode_reward:-124.29900000000468 steps:221[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 72: episode_reward:4491.432530920721 steps:223[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 73: episode_reward:19584.320175529574 steps:227[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 74: episode_reward:48227.61327577682 steps:236[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 75: episode_reward:36881.111038277464 steps:242[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 76: episode_reward:2278.2204512796966 steps:244[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 77: episode_reward:5248.046862447512 steps:248[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 78: episode_reward:65528.22024816038 steps:254[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 79: episode_reward:2286.7473696960246 steps:256[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 80: episode_reward:3102.2975175702936 steps:258[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 81: episode_reward:5277.966181586242 steps:260[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 82: episode_reward:16166.569162987922 steps:265[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 83: episode_reward:36146.06691190753 steps:269[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 84: episode_reward:12903.038860347979 steps:272[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 85: episode_reward:60934.49537238866 steps:280[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 86: episode_reward:8666.604713730134 steps:284[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 87: episode_reward:10823.507095802737 steps:287[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 88: episode_reward:28576.79593991367 steps:292[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 89: episode_reward:38231.17328954655 steps:298[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 90: episode_reward:11470.170348270263 steps:300[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 91: episode_reward:20044.94633767936 steps:303[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 92: episode_reward:1838.5059293103675 steps:306[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 93: episode_reward:5456.624546139313 steps:308[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 94: episode_reward:12379.595750750674 steps:314[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 95: episode_reward:2926.0032978309596 steps:317[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 96: episode_reward:21613.96921040136 steps:321[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 97: episode_reward:3616.5513970893426 steps:323[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 98: episode_reward:18681.087138108887 steps:329[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 99: episode_reward:25934.421273722674 steps:335[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 100: episode_reward:3706.563014844055 steps:338[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 101: episode_reward:3405.121643470197 steps:340[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 102: episode_reward:1357.042384332862 steps:342[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 103: episode_reward:7729.538636220984 steps:344[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 104: episode_reward:7848.037010119568 steps:348[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 105: episode_reward:2691.814996904165 steps:350[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 106: episode_reward:1069.2417783134297 steps:352[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 107: episode_reward:3746.8110250453956 steps:354[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 108: episode_reward:4001.334342669578 steps:356[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 109: episode_reward:-121.30327978515005 steps:357[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 110: episode_reward:9911.290195148373 steps:359[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 111: episode_reward:1184.948233371431 steps:361[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 112: episode_reward:1538.862073240331 steps:363[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 113: episode_reward:13487.588129642323 steps:367[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 114: episode_reward:7483.156641079565 steps:370[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 115: episode_reward:17226.24824803049 steps:377[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 116: episode_reward:5222.055652621012 steps:380[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 117: episode_reward:36176.91995600057 steps:388[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 118: episode_reward:10930.868663821802 steps:391[00m
[RDDPG] Resetting Environment
