2021-05-03 07:11:29.548087: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-03 07:11:29.548130: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1620006096.283036900, 1.056000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1620006096.284895392, 1.056000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1620006096.286296173, 1.057000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1620006097.734052689, 2.069000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1620006099.090542697, 3.001000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1620006100.178653416, 3.800000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1620006101.281106376, 4.605000000]: Ready to take commands for planning group back_left_leg.[0m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[92m [RDDPG] 0: episode_reward:-2.10678002392578 steps:1[00m
[RDDPG] Episode Done
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[92m [RDDPG] 1: episode_reward:-1.5070202695312536 steps:2[00m
[RDDPG] Episode Done
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[92m [RDDPG] 2: episode_reward:-1.5739290526123058 steps:4[00m
[RDDPG] Episode Done
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[92m [RDDPG] 3: episode_reward:-1.6743150028076186 steps:6[00m
[RDDPG] Episode Done
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[92m [RDDPG] 4: episode_reward:-2.4013309418945274 steps:9[00m
[RDDPG] Episode Done
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[92m [RDDPG] 5: episode_reward:-2.9057634440917774 steps:11[00m
[RDDPG] Episode Done
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[92m [RDDPG] 6: episode_reward:-1.840111593750004 steps:12[00m
[RDDPG] Episode Done
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[92m [RDDPG] 7: episode_reward:-4.1149536296386735 steps:15[00m
[RDDPG] Episode Done
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
Traceback (most recent call last):
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 175, in train
    state0 = None
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/evaluator.py", line 22, in __call__
    observation = env.reset()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/env.py", line 16, in reset
    self._state, self._reward = self.quadruped.reset()
  File "/home/shandilya/Desktop/CNS/DDP/src/simulations/ws/src/quadruped/scripts/quadruped.py", line 857, in reset
    rospy.sleep(1.0)
  File "/opt/ros/noetic/lib/python3/dist-packages/rospy/timer.py", line 165, in sleep
    raise rospy.exceptions.ROSInterruptException("ROS shutdown request")
rospy.exceptions.ROSInterruptException: ROS shutdown request
2021-05-03 07:13:27.638963: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-03 07:13:27.639005: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1620006214.568236087, 1.082000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1620006214.571019812, 1.083000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1620006214.571167150, 1.083000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1620006216.091258554, 2.138000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1620006217.347520198, 3.001000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1620006218.442416424, 3.801000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1620006219.582313883, 4.601000000]: Ready to take commands for planning group back_left_leg.[0m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
Traceback (most recent call last):
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 87, in train
    validate_reward = self.evaluate(
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/evaluator.py", line 37, in __call__
    observation, reward, done, info = env.step(
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/env.py", line 36, in step
    observation =  self.quadruped.step(
  File "/home/shandilya/Desktop/CNS/DDP/src/simulations/ws/src/quadruped/scripts/quadruped.py", line 1216, in step
    self.set_observation(action, desired_motion)
  File "/home/shandilya/Desktop/CNS/DDP/src/simulations/ws/src/quadruped/scripts/quadruped.py", line 1073, in set_observation
    rospy.wait_for_service('/gazebo/get_model_state')
  File "/opt/ros/noetic/lib/python3/dist-packages/rospy/impl/tcpros_service.py", line 166, in wait_for_service
    raise ROSInterruptException("rospy shutdown")
rospy.exceptions.ROSInterruptException: rospy shutdown
2021-05-03 07:21:14.544321: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-03 07:21:14.544356: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1620006681.601954538, 1.380000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1620006681.604921335, 1.385000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1620006681.605102883, 1.385000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1620006683.184789277, 2.507000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1620006683.960621551, 3.001000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1620006684.541308687, 3.400000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1620006685.126258961, 3.801000000]: Ready to take commands for planning group back_left_leg.[0m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000000: mean_reward:-1.5447985281188994[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[92m [RDDPG] 0: episode_reward:-1.175510164062494 steps:1[00m
[RDDPG] Episode Done
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[92m [RDDPG] 1: episode_reward:-1.7778881264648616 steps:2[00m
[RDDPG] Episode Done
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[92m [RDDPG] 2: episode_reward:-1.4448975408935507 steps:3[00m
[RDDPG] Episode Done
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[92m [RDDPG] 3: episode_reward:-2.437187889282247 steps:6[00m
[RDDPG] Episode Done
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[92m [RDDPG] 4: episode_reward:-1.5767268024902394 steps:7[00m
[RDDPG] Episode Done
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[92m [RDDPG] 5: episode_reward:-2.249871449707046 steps:9[00m
[RDDPG] Episode Done
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[92m [RDDPG] 6: episode_reward:-1.2523706237792944 steps:10[00m
[RDDPG] Episode Done
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
Traceback (most recent call last):
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 192, in train
    validate_reward = self.evaluate(
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/evaluator.py", line 37, in __call__
    observation, reward, done, info = env.step(
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/env.py", line 36, in step
    observation =  self.quadruped.step(
  File "/home/shandilya/Desktop/CNS/DDP/src/simulations/ws/src/quadruped/scripts/quadruped.py", line 1217, in step
    rospy.sleep(15.0/60.0)
  File "/opt/ros/noetic/lib/python3/dist-packages/rospy/timer.py", line 165, in sleep
    raise rospy.exceptions.ROSInterruptException("ROS shutdown request")
rospy.exceptions.ROSInterruptException: ROS shutdown request
2021-05-03 07:25:11.194041: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-03 07:25:11.194076: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1620006918.037535878, 1.084000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1620006918.040150836, 1.085000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1620006918.042418448, 1.088000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1620006919.834407981, 2.247000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1620006920.863011172, 3.001000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1620006921.945244874, 3.801000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1620006923.089380636, 4.602000000]: Ready to take commands for planning group back_left_leg.[0m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000000: mean_reward:-1.8449449662944821[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 0: episode_reward:-4.917141188476597 steps:5[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000005: mean_reward:-1.6677209307971954[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 1: episode_reward:-1.6056082714843725 steps:6[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 2: episode_reward:-2.4114595666503806 steps:9[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 3: episode_reward:-3.442751650878894 steps:12[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 4: episode_reward:-2.912814464843748 steps:14[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 5: episode_reward:-1.7922596948242213 steps:15[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000015: mean_reward:-1.5867240905044593[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 6: episode_reward:-2.9312811545410877 steps:17[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 7: episode_reward:-1.8594256835937413 steps:19[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 8: episode_reward:-4.058769458251917 steps:23[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 9: episode_reward:-1.4660119426269471 steps:24[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 10: episode_reward:-1.480947230590815 steps:25[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000025: mean_reward:-1.9252544056005534[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 11: episode_reward:-2.5216073059081663 steps:27[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 12: episode_reward:-2.0818329423827917 steps:28[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 13: episode_reward:-2.4491773763428 steps:30[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
Traceback (most recent call last):
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 192, in train
    validate_reward = self.evaluate(
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/evaluator.py", line 37, in __call__
    observation, reward, done, info = env.step(
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/env.py", line 36, in step
    observation =  self.quadruped.step(
  File "/home/shandilya/Desktop/CNS/DDP/src/simulations/ws/src/quadruped/scripts/quadruped.py", line 1217, in step
    rospy.sleep(15.0/60.0)
  File "/opt/ros/noetic/lib/python3/dist-packages/rospy/timer.py", line 165, in sleep
    raise rospy.exceptions.ROSInterruptException("ROS shutdown request")
rospy.exceptions.ROSInterruptException: ROS shutdown request
2021-05-03 07:49:12.724699: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-03 07:49:12.724756: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1620008359.735790952, 1.315000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1620008359.737946858, 1.317000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1620008359.738376880, 1.317000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1620008361.490834948, 2.511000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1620008362.340990856, 3.136000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1620008363.432400336, 3.937000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1620008364.563975782, 4.737000000]: Ready to take commands for planning group back_left_leg.[0m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000000: mean_reward:5391.784486808352[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 0: episode_reward:1.8653293778780855 steps:2[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 1: episode_reward:5.452890335242435 steps:4[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 2: episode_reward:-1.5564837308349737 steps:5[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000005: mean_reward:15831.519664380114[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 3: episode_reward:-1.2685764384765643 steps:6[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 4: episode_reward:52.29467727866711 steps:8[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 5: episode_reward:22927.724231137025 steps:11[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 6: episode_reward:21198.92748577086 steps:14[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 7: episode_reward:-1.7017416350097665 steps:15[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000015: mean_reward:19215.21156785565[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 8: episode_reward:106189.34779507213 steps:21[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 9: episode_reward:8.770021249984461 steps:23[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 10: episode_reward:-1.9624703022460577 steps:24[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 11: episode_reward:-2.4003961269531326 steps:25[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000025: mean_reward:46623.263355711846[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 12: episode_reward:-1.659824083496141 steps:26[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 13: episode_reward:29.262904056181014 steps:28[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 14: episode_reward:26668.78491543494 steps:31[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 15: episode_reward:140371.84886979705 steps:37[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 16: episode_reward:19.927762007336984 steps:39[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 17: episode_reward:-1.5213551408691781 steps:40[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
Traceback (most recent call last):
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 192, in train
    validate_reward = self.evaluate(
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/evaluator.py", line 37, in __call__
    observation, reward, done, info = env.step(
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/env.py", line 36, in step
    observation =  self.quadruped.step(
  File "/home/shandilya/Desktop/CNS/DDP/src/simulations/ws/src/quadruped/scripts/quadruped.py", line 1216, in step
    self.set_observation(action, desired_motion)
  File "/home/shandilya/Desktop/CNS/DDP/src/simulations/ws/src/quadruped/scripts/quadruped.py", line 1073, in set_observation
    rospy.wait_for_service('/gazebo/get_model_state')
  File "/opt/ros/noetic/lib/python3/dist-packages/rospy/impl/tcpros_service.py", line 166, in wait_for_service
    raise ROSInterruptException("rospy shutdown")
rospy.exceptions.ROSInterruptException: rospy shutdown
2021-05-03 08:07:56.386540: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-03 08:07:56.386574: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1620009483.093406706, 0.852000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1620009483.095392824, 0.852000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1620009483.095506628, 0.852000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1620009484.655642188, 1.957000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1620009485.509562809, 2.600000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1620009486.678624015, 3.400000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1620009487.774148616, 4.201000000]: Ready to take commands for planning group back_left_leg.[0m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000000: mean_reward:187183.2586871109[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 0: episode_reward:11.92903897740994 steps:2[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 1: episode_reward:-1.4635386437988385 steps:3[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 2: episode_reward:7.281301610880971 steps:5[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000005: mean_reward:45981.608732525594[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 3: episode_reward:-1.5049014506836385 steps:6[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 4: episode_reward:67696.30122897122 steps:11[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 5: episode_reward:-1.9745087687988798 steps:12[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 6: episode_reward:20798.588671929818 steps:15[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000015: mean_reward:17520.342416712203[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 7: episode_reward:-1.3514563172607494 steps:16[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 8: episode_reward:-1.9854180786132396 steps:17[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 9: episode_reward:0.0598374206479555 steps:19[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 10: episode_reward:85499.3648835129 steps:24[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 11: episode_reward:25070.486327855164 steps:29[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 12: episode_reward:26.649581012914236 steps:31[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 13: episode_reward:-1.6516652768554825 steps:32[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 14: episode_reward:12.650639940849778 steps:34[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 15: episode_reward:-1.4999219748534953 steps:35[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000035: mean_reward:66686.20186519525[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 16: episode_reward:28579.394013847163 steps:38[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 17: episode_reward:42.25881882335384 steps:40[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000040: mean_reward:4830.157383868236[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 18: episode_reward:2.67379056517993 steps:42[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 19: episode_reward:14.928147021968385 steps:44[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 20: episode_reward:7.327324759068111 steps:46[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 21: episode_reward:12.827541185468311 steps:48[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 22: episode_reward:219906.1225278811 steps:53[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 23: episode_reward:6.817110940597955 steps:55[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000055: mean_reward:15157.61661507673[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 24: episode_reward:4.480673094628973 steps:57[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 25: episode_reward:-1.9372177297363486 steps:58[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 26: episode_reward:8.757151483472809 steps:60[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[93m [RDDPG] Step_0000060: mean_reward:60982.62097527863[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 27: episode_reward:27139.37960221236 steps:64[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 28: episode_reward:-2.099234212402476 steps:65[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000065: mean_reward:22857.143961928618[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 29: episode_reward:-1.935038657714608 steps:66[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 30: episode_reward:-1.3373719791259946 steps:67[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 31: episode_reward:10.312534241130615 steps:69[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 32: episode_reward:-1.5426551103515522 steps:70[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000070: mean_reward:56410.8009814205[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 33: episode_reward:24.49917285957291 steps:72[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 34: episode_reward:-1.4174902905272706 steps:73[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 35: episode_reward:-1.403216291381821 steps:74[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 36: episode_reward:-1.3100882543944925 steps:75[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[93m [RDDPG] Step_0000075: mean_reward:59746.59979454569[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 37: episode_reward:-1.6207004560544855 steps:76[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 38: episode_reward:5.1489559807768455 steps:79[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 39: episode_reward:-1.9110195839850501 steps:80[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000080: mean_reward:29004.26673318168[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 40: episode_reward:-1.7392613896484201 steps:81[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 41: episode_reward:112.82473461674694 steps:84[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 42: episode_reward:-1.2916325324705984 steps:85[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[93m [RDDPG] Step_0000085: mean_reward:163585.80343076834[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 43: episode_reward:36853.82646416267 steps:88[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 44: episode_reward:-1.9561593427737503 steps:89[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 45: episode_reward:37.316845913508736 steps:91[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 46: episode_reward:2008.6993753198763 steps:94[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 47: episode_reward:-1.594065952636277 steps:95[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[93m [RDDPG] Step_0000095: mean_reward:24477.12393294115[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 48: episode_reward:-1.5438449963377954 steps:96[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 49: episode_reward:166.64135873566366 steps:98[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 50: episode_reward:13.886425081540956 steps:100[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000100: mean_reward:117793.66966692568[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 51: episode_reward:42.65860765129996 steps:103[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 52: episode_reward:-1.4181615219727928 steps:104[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 53: episode_reward:65463.46160424445 steps:109[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 54: episode_reward:238.00518757648587 steps:112[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 55: episode_reward:62936.45484413573 steps:118[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 56: episode_reward:-1.4984208577878584 steps:119[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 57: episode_reward:95.05695507301434 steps:121[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 58: episode_reward:7.88557369445123 steps:123[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 59: episode_reward:49343.02816219231 steps:126[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 60: episode_reward:-2.0579910546878035 steps:127[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 61: episode_reward:28.601192779272115 steps:129[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 62: episode_reward:65072.42527005699 steps:132[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 63: episode_reward:16.295669220174048 steps:135[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[93m [RDDPG] Step_0000135: mean_reward:203999.39362579468[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 64: episode_reward:-1.8810898535160168 steps:136[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 65: episode_reward:-1.5022221049805116 steps:137[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 66: episode_reward:0.9404725389576836 steps:139[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 67: episode_reward:0.3462415070106015 steps:141[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 68: episode_reward:27.916065183550344 steps:143[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 69: episode_reward:-1.2562934580078855 steps:144[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 70: episode_reward:6.024242995751303 steps:146[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 71: episode_reward:1.2796879082454868 steps:148[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 72: episode_reward:-2.1149533229988737 steps:149[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 73: episode_reward:4.84949044360912 steps:151[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 74: episode_reward:20.60531453605303 steps:153[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 75: episode_reward:-1.7062180932607425 steps:154[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 76: episode_reward:-1.746665367187714 steps:155[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[93m [RDDPG] Step_0000155: mean_reward:24633.14597744185[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 77: episode_reward:373.6774877197359 steps:158[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 78: episode_reward:109774.3078550573 steps:164[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 79: episode_reward:8.73980163839944 steps:166[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 80: episode_reward:-2.054697777099869 steps:167[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 81: episode_reward:16.61760522645374 steps:170[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000170: mean_reward:7570.00849228608[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 82: episode_reward:176.50754257896946 steps:172[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 83: episode_reward:-1.415628654784945 steps:173[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 84: episode_reward:21705.07399723109 steps:178[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 85: episode_reward:-2.2515887731935758 steps:179[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 86: episode_reward:15.74204530526554 steps:181[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 87: episode_reward:13.11459394071149 steps:183[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 88: episode_reward:1.2076678659477555 steps:185[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000185: mean_reward:25530.861215509656[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 89: episode_reward:11314.160048099353 steps:188[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 90: episode_reward:2.717713168735635 steps:190[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000190: mean_reward:61702.21264686405[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 91: episode_reward:47.11486690311221 steps:192[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 92: episode_reward:27.101683774365625 steps:194[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 93: episode_reward:57029.550022730386 steps:198[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 94: episode_reward:40571.63755459253 steps:201[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 95: episode_reward:-1.297294284057624 steps:202[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 96: episode_reward:13.635525984427899 steps:204[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 97: episode_reward:4.889719086563269 steps:206[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 98: episode_reward:16.717330182782 steps:209[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 99: episode_reward:11.843904989055893 steps:211[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 100: episode_reward:-1.65818598339873 steps:212[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 101: episode_reward:-1.4474694760742082 steps:213[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 102: episode_reward:-2.2200037536614277 steps:214[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 103: episode_reward:-1.6246766699210973 steps:215[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000215: mean_reward:15355.27211633269[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 104: episode_reward:25869.409464305176 steps:218[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 105: episode_reward:9.773417085434962 steps:220[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[93m [RDDPG] Step_0000220: mean_reward:71340.01944596448[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 106: episode_reward:-1.7925513442385386 steps:221[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 107: episode_reward:12.073530082994612 steps:224[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 108: episode_reward:11795.933015414548 steps:227[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 109: episode_reward:38221.54576171687 steps:231[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 110: episode_reward:-1.661947497558812 steps:232[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 111: episode_reward:17432.417861194037 steps:235[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[93m [RDDPG] Step_0000235: mean_reward:372903.5300514229[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 112: episode_reward:2.142714137780393 steps:237[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 113: episode_reward:12905.198173033104 steps:240[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[93m [RDDPG] Step_0000240: mean_reward:143358.4214126562[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 114: episode_reward:-1.3088032587889515 steps:241[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 115: episode_reward:-1.4346212916258265 steps:242[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 116: episode_reward:7.5070269836244226 steps:244[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 117: episode_reward:-1.8959239702138073 steps:245[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000245: mean_reward:14366.06913893644[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 118: episode_reward:58678.709214015136 steps:250[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[93m [RDDPG] Step_0000250: mean_reward:50585.00461293481[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 119: episode_reward:52927.21366158738 steps:256[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 120: episode_reward:6285.671546319366 steps:259[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 121: episode_reward:146028.30841487486 steps:266[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 122: episode_reward:6.158370101090732 steps:268[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 123: episode_reward:-1.4811342143554338 steps:269[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 124: episode_reward:61.02055465635222 steps:271[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 125: episode_reward:8.875151057502544 steps:273[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 126: episode_reward:-1.9002712382801643 steps:274[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 127: episode_reward:6.5901763846266705 steps:276[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 128: episode_reward:-1.5920525703126693 steps:277[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 129: episode_reward:4.3945717375112405 steps:279[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 130: episode_reward:8.721520701526607 steps:281[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 131: episode_reward:4.741605994459927 steps:283[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 132: episode_reward:3.583502834674335 steps:285[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000285: mean_reward:49065.28632778325[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 133: episode_reward:73823.05267538705 steps:289[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 134: episode_reward:-1.8896509914565134 steps:290[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000290: mean_reward:180343.63502386853[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 135: episode_reward:-1.5593730878906622 steps:291[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 136: episode_reward:47.14473601316949 steps:293[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 137: episode_reward:-1.6340008300787154 steps:294[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 138: episode_reward:3.018693863682493 steps:296[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 139: episode_reward:3.668109988717614 steps:298[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 140: episode_reward:35453.857182073734 steps:301[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 141: episode_reward:-1.4775923525388244 steps:302[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 142: episode_reward:39176.78238748961 steps:306[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 143: episode_reward:301.1570616243732 steps:308[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 144: episode_reward:187.40402581245158 steps:310[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000310: mean_reward:15721.26514108496[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 145: episode_reward:71868.8487008946 steps:314[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 146: episode_reward:149236.07369859182 steps:318[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 147: episode_reward:9.429274741638332 steps:320[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[93m [RDDPG] Step_0000320: mean_reward:133356.85739571528[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 148: episode_reward:-1.280208728027253 steps:321[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 149: episode_reward:17936.20856335393 steps:324[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 150: episode_reward:0.7142118048367521 steps:326[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 151: episode_reward:42891.53373802448 steps:331[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 152: episode_reward:4845.9430557852775 steps:334[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 153: episode_reward:18.108386554269185 steps:336[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 154: episode_reward:-1.8913132995622917 steps:337[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 155: episode_reward:-1.253300825805904 steps:338[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 156: episode_reward:8.84299611913289 steps:340[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[93m [RDDPG] Step_0000340: mean_reward:69310.68932208276[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 157: episode_reward:33856.602689804226 steps:345[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[93m [RDDPG] Step_0000345: mean_reward:109571.73828676571[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 158: episode_reward:-1.5915436999514705 steps:346[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 159: episode_reward:20217.651061745124 steps:349[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 160: episode_reward:115.81279078613402 steps:351[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 161: episode_reward:-1.9698924575198813 steps:352[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 162: episode_reward:7752.377504996393 steps:355[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000355: mean_reward:21244.115614692553[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 163: episode_reward:39.96926395793356 steps:357[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 164: episode_reward:-1.140614325317386 steps:358[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 165: episode_reward:9235.43167826272 steps:362[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 166: episode_reward:-1.8325637595215034 steps:363[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 167: episode_reward:12.709581051953217 steps:366[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 168: episode_reward:12422.440271125148 steps:369[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 169: episode_reward:2.9650670408986817 steps:371[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 170: episode_reward:8.833442991177707 steps:373[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 171: episode_reward:11761.998513889826 steps:377[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 172: episode_reward:120.27298139472784 steps:379[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 173: episode_reward:5.502761245491258 steps:381[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 174: episode_reward:-2.4585380976531352 steps:382[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 175: episode_reward:8.673855575328679 steps:384[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 176: episode_reward:20.609838231720012 steps:386[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 177: episode_reward:-2.234345080079691 steps:387[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 178: episode_reward:7.900783748171928 steps:389[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 179: episode_reward:-1.5253551220694064 steps:390[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000390: mean_reward:26820.89688994406[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 180: episode_reward:-1.7067069750975286 steps:391[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 181: episode_reward:5969.335890359383 steps:394[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 182: episode_reward:-1.94187967773352 steps:395[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[93m [RDDPG] Step_0000395: mean_reward:169675.53769834264[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 183: episode_reward:9.495162687187984 steps:397[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 184: episode_reward:-1.9964150483388559 steps:398[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 185: episode_reward:184.66584483190925 steps:400[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000400: mean_reward:2229.2907053263075[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 186: episode_reward:35.36577209709426 steps:402[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
rddpg_torch.py:35: FutureWarning: arrays to stack must be passed as a "sequence" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.
  rdpg.train(params['train_episode_count'], checkpoint_path, True)
/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:130: UserWarning: Error detected in EluBackward. Traceback of forward call that caused the error:
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 290, in update_policy
    self.agent.actor_target(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 280, in forward
    action, robot_enc_state, z = self.cell(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 257, in forward
    rs_r = self.robot_state_enc(self.robot_enc_state)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/container.py", line 117, in forward
    input = module(input)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 477, in forward
    return F.elu(input, self.alpha, self.inplace)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/functional.py", line 1227, in elu
    result = torch._C._nn.elu(input, alpha)
 (Triggered internally at  /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:104.)
  Variable._execution_engine.run_backward(
Traceback (most recent call last):
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 318, in update_policy
    value_loss.backward()
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py", line 130, in backward
    Variable._execution_engine.run_backward(
RuntimeError: Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time.
2021-05-03 18:53:06.124388: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-03 18:53:06.124432: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1620048193.288214818, 1.491000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1620048193.290201909, 1.492000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1620048193.290673074, 1.492000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1620048194.647856580, 2.576000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1620048195.829355846, 3.513000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1620048196.791753252, 4.312000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1620048197.770737172, 5.112000000]: Ready to take commands for planning group back_left_leg.[0m
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000000: mean_reward:891.5610960462493[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 0: episode_reward:-1.6668907617187592 steps:1[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 1: episode_reward:29095.25466851409 steps:4[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
rddpg_torch.py:35: FutureWarning: arrays to stack must be passed as a "sequence" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.
  rdpg.train(params['train_episode_count'], checkpoint_path, True)
/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:130: UserWarning: Error detected in EluBackward. Traceback of forward call that caused the error:
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 290, in update_policy
    self.agent.actor_target(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 280, in forward
    action, robot_enc_state, z = self.cell(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 257, in forward
    rs_r = self.robot_state_enc(self.robot_enc_state)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/container.py", line 117, in forward
    input = module(input)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 477, in forward
    return F.elu(input, self.alpha, self.inplace)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/functional.py", line 1227, in elu
    result = torch._C._nn.elu(input, alpha)
 (Triggered internally at  /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:104.)
  Variable._execution_engine.run_backward(
Traceback (most recent call last):
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 318, in update_policy
    value_loss.backward()
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py", line 130, in backward
    Variable._execution_engine.run_backward(
RuntimeError: Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time.
2021-05-03 18:58:12.277157: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-03 18:58:12.277206: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1620048499.105264556, 1.363000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1620048499.107333859, 1.364000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1620048499.108438284, 1.365000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1620048500.519609720, 2.478000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1620048501.577409284, 3.300000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1620048502.543863589, 4.100000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1620048503.521418532, 4.900000000]: Ready to take commands for planning group back_left_leg.[0m
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000000: mean_reward:9367.847993385383[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 0: episode_reward:114.06092338302604 steps:3[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 1: episode_reward:5.356935258266301 steps:5[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[93m [RDDPG] Step_0000005: mean_reward:32939.817811661174[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
Updating
rddpg_torch.py:35: FutureWarning: arrays to stack must be passed as a "sequence" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.
  rdpg.train(params['train_episode_count'], checkpoint_path, True)
Updating
/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:130: UserWarning: Error detected in EluBackward. Traceback of forward call that caused the error:
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 290, in update_policy
    self.agent.actor_target(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 280, in forward
    action, robot_enc_state, z = self.cell(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 257, in forward
    rs_r = self.robot_state_enc(self.robot_enc_state)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/container.py", line 117, in forward
    input = module(input)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 477, in forward
    return F.elu(input, self.alpha, self.inplace)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/functional.py", line 1227, in elu
    result = torch._C._nn.elu(input, alpha)
 (Triggered internally at  /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:104.)
  Variable._execution_engine.run_backward(
Traceback (most recent call last):
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 318, in update_policy
    value_loss.backward()
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py", line 130, in backward
    Variable._execution_engine.run_backward(
RuntimeError: Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time.
2021-05-03 19:50:39.386594: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-03 19:50:39.386676: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1620051646.023702080, 1.053000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1620051646.026228468, 1.055000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1620051646.026366256, 1.055000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1620051647.466193880, 2.214000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1620051648.417344267, 3.000000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1620051649.456453553, 3.801000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1620051650.410795942, 4.601000000]: Ready to take commands for planning group back_left_leg.[0m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000000: mean_reward:4107.651126594262[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 0: episode_reward:242.89942593435558 steps:2[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 1: episode_reward:-1.7453131030273494 steps:3[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 2: episode_reward:34.317200358393386 steps:5[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[93m [RDDPG] Step_0000005: mean_reward:56453.575188766255[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Updating Policy
Updating
rddpg_torch.py:35: FutureWarning: arrays to stack must be passed as a "sequence" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.
  rdpg.train(params['train_episode_count'], checkpoint_path, True)
/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:130: UserWarning: Error detected in DivBackward0. Traceback of forward call that caused the error:
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 314, in update_policy
    value_loss /= len(experiences) # divide by trajectory length
 (Triggered internally at  /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:104.)
  Variable._execution_engine.run_backward(
Traceback (most recent call last):
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 336, in update_policy
    value_loss.backward()
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py", line 130, in backward
    Variable._execution_engine.run_backward(
RuntimeError: Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time.
2021-05-03 19:56:05.067003: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-03 19:56:05.067090: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1620051977.355559954, 249.474000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1620051977.364519863, 249.478000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1620051977.364706603, 249.478000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
Traceback (most recent call last):
  File "rddpg_torch.py", line 24, in <module>
    env = Env(
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/env.py", line 12, in __init__
    self.quadruped = Quadruped(params, experiment)
  File "/home/shandilya/Desktop/CNS/DDP/src/simulations/ws/src/quadruped/scripts/quadruped.py", line 489, in __init__
    self.kinematics = Kinematics(
  File "/home/shandilya/Desktop/CNS/DDP/src/simulations/ws/src/quadruped/scripts/kinematics.py", line 21, in __init__
    self.front_right_leg = moveit_commander.MoveGroupCommander(
  File "/opt/ros/noetic/lib/python3/dist-packages/moveit_commander/move_group.py", line 53, in __init__
    self._g = _moveit_move_group_interface.MoveGroupInterface(name, robot_description, ns, wait_for_servers)
RuntimeError: Unable to connect to move_group action server 'move_group' within allotted time (5s)
2021-05-03 19:56:37.429239: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-03 19:56:37.429354: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1620052007.321184912, 275.139000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1620052007.329776063, 275.145000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1620052007.330024095, 275.145000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1620052008.741618434, 275.935000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1620052009.097059815, 276.149000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1620052010.341130326, 276.549000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1620052011.707494391, 276.949000000]: Ready to take commands for planning group back_left_leg.[0m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000000: mean_reward:2408.676995879857[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 0: episode_reward:22.78088689004532 steps:2[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
Updating
rddpg_torch.py:35: FutureWarning: arrays to stack must be passed as a "sequence" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.
  rdpg.train(params['train_episode_count'], checkpoint_path, True)
/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:130: UserWarning: Error detected in AddmmBackward. Traceback of forward call that caused the error:
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 304, in update_policy
    current_q = self.agent.critic(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 351, in forward
    q = self.out_dense_seq(torch.cat([ms, rs, ac], -1))
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/container.py", line 117, in forward
    input = module(input)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 93, in forward
    return F.linear(input, self.weight, self.bias)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/functional.py", line 1690, in linear
    ret = torch.addmm(bias, input, weight.t())
 (Triggered internally at  /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:104.)
  Variable._execution_engine.run_backward(
Traceback (most recent call last):
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 336, in update_policy
    value_loss.backward()
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py", line 130, in backward
    Variable._execution_engine.run_backward(
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [72, 1]], which is output 0 of TBackward, is at version 2; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!
2021-05-03 20:01:55.814850: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-03 20:01:55.814905: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1620052323.086157069, 373.449000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1620052323.094556221, 373.454000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1620052323.094815417, 373.454000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1620052324.550993846, 374.420000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1620052325.253271755, 374.902000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1620052326.106635253, 375.504000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1620052327.158733469, 376.102000000]: Ready to take commands for planning group back_left_leg.[0m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000000: mean_reward:818.4749255492427[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 0: episode_reward:-1.8516344584962234 steps:1[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 1: episode_reward:76.10934163384695 steps:3[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 2: episode_reward:-2.4703597812501474 steps:4[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 3: episode_reward:-1.5890771972656978 steps:5[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000005: mean_reward:15991.335025275952[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Updating Policy
Updating
rddpg_torch.py:35: FutureWarning: arrays to stack must be passed as a "sequence" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.
  rdpg.train(params['train_episode_count'], checkpoint_path, True)
updating done
Updating
/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:130: UserWarning: Error detected in EluBackward. Traceback of forward call that caused the error:
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 290, in update_policy
    self.agent.actor_target(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 280, in forward
    action, robot_enc_state, z = self.cell(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 257, in forward
    rs_r = self.robot_state_enc(self.robot_enc_state)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/container.py", line 117, in forward
    input = module(input)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 477, in forward
    return F.elu(input, self.alpha, self.inplace)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/functional.py", line 1227, in elu
    result = torch._C._nn.elu(input, alpha)
 (Triggered internally at  /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:104.)
  Variable._execution_engine.run_backward(
Traceback (most recent call last):
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 318, in update_policy
    value_loss.backward()
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py", line 130, in backward
    Variable._execution_engine.run_backward(
RuntimeError: Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time.
2021-05-03 20:15:46.984282: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-03 20:15:46.984319: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1620053153.879287714, 1.330000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1620053153.881201807, 1.330000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1620053153.881317460, 1.330000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1620053155.595036454, 2.703000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1620053156.235746139, 3.248000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1620053156.954788559, 3.848000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1620053157.425877377, 4.248000000]: Ready to take commands for planning group back_left_leg.[0m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000000: mean_reward:79.83187296867891[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 0: episode_reward:-1.919901659179684 steps:1[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 1: episode_reward:120.95113218409008 steps:3[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 2: episode_reward:-1.8953927529296875 steps:4[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 3: episode_reward:-1.3651544006347736 steps:5[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000005: mean_reward:10697.934905122649[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Updating Policy
Updating
rddpg_torch.py:35: FutureWarning: arrays to stack must be passed as a "sequence" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.
  rdpg.train(params['train_episode_count'], checkpoint_path, True)
updating done
Updating
/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:130: UserWarning: Error detected in AddmmBackward. Traceback of forward call that caused the error:
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 323, in update_policy
    action, (robot_enc_state, z) = self.agent.actor(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 280, in forward
    action, robot_enc_state, z = self.cell(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 257, in forward
    rs_r = self.robot_state_enc(self.robot_enc_state)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/container.py", line 117, in forward
    input = module(input)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 93, in forward
    return F.linear(input, self.weight, self.bias)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/functional.py", line 1690, in linear
    ret = torch.addmm(bias, input, weight.t())
 (Triggered internally at  /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:104.)
  Variable._execution_engine.run_backward(
Traceback (most recent call last):
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 335, in update_policy
    policy_loss.backward(retain_graph=True)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py", line 130, in backward
    Variable._execution_engine.run_backward(
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [135, 30]], which is output 0 of TBackward, is at version 2; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!
2021-05-03 20:41:12.428277: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-03 20:41:12.428326: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1620054679.451591616, 1.122000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1620054679.456954817, 1.126000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1620054679.457168252, 1.126000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1620054681.024988326, 2.223000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1620054682.114462564, 3.004000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1620054683.318666036, 3.801000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1620054684.456272630, 4.600000000]: Ready to take commands for planning group back_left_leg.[0m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[93m [RDDPG] Step_0000000: mean_reward:1997.8238422621787[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 0: episode_reward:82.84954506992872 steps:2[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 1: episode_reward:-1.4981446508789025 steps:3[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 2: episode_reward:-0.001855439330423092 steps:5[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000005: mean_reward:-395.73080803772064[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
Updating
rddpg_torch.py:35: FutureWarning: arrays to stack must be passed as a "sequence" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.
  rdpg.train(params['train_episode_count'], checkpoint_path, True)
updating done
Updating
/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:130: UserWarning: Error detected in AddmmBackward. Traceback of forward call that caused the error:
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 323, in update_policy
    action, (robot_enc_state, z) = self.agent.actor(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 287, in forward
    action, robot_enc_state, z = self.cell(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 258, in forward
    rs_r = self.robot_state_enc(self.robot_enc_state)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/container.py", line 117, in forward
    input = module(input)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 93, in forward
    return F.linear(input, self.weight, self.bias)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/functional.py", line 1690, in linear
    ret = torch.addmm(bias, input, weight.t())
 (Triggered internally at  /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:104.)
  Variable._execution_engine.run_backward(
Traceback (most recent call last):
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 335, in update_policy
    policy_loss.backward(retain_graph=True)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py", line 130, in backward
    Variable._execution_engine.run_backward(
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [135, 30]], which is output 0 of TBackward, is at version 2; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!
2021-05-03 20:48:05.837584: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-03 20:48:05.837632: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1620055092.897280607, 1.138000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1620055092.902737944, 1.149000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1620055092.902971265, 1.150000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1620055094.498998520, 2.452000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1620055095.362898911, 3.200000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1620055096.382654904, 4.001000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1620055097.343962736, 4.801000000]: Ready to take commands for planning group back_left_leg.[0m
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[93m [RDDPG] Step_0000000: mean_reward:6164.843719125439[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 0: episode_reward:5.527627122770246 steps:2[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 1: episode_reward:-1.7726349316406542 steps:3[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 2: episode_reward:3.3930855859228974 steps:5[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[93m [RDDPG] Step_0000005: mean_reward:22953.00662260519[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
Updating
rddpg_torch.py:35: FutureWarning: arrays to stack must be passed as a "sequence" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.
  rdpg.train(params['train_episode_count'], checkpoint_path, True)
/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:130: UserWarning: Error detected in AddmmBackward. Traceback of forward call that caused the error:
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 323, in update_policy
    policy_loss = -self.agent.critic(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 358, in forward
    q = self.out_dense_seq(torch.cat([ms, rs, ac], -1))
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/container.py", line 117, in forward
    input = module(input)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 93, in forward
    return F.linear(input, self.weight, self.bias)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/functional.py", line 1690, in linear
    ret = torch.addmm(bias, input, weight.t())
 (Triggered internally at  /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:104.)
  Variable._execution_engine.run_backward(
Traceback (most recent call last):
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 337, in update_policy
    policy_loss.backward(retain_graph=True)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py", line 130, in backward
    Variable._execution_engine.run_backward(
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [72, 1]], which is output 0 of TBackward, is at version 2; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!
2021-05-03 20:55:51.591305: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-03 20:55:51.591357: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1620055558.571520181, 1.328000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1620055558.573553657, 1.328000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1620055558.573693756, 1.328000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1620055560.093176317, 2.498000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1620055561.046390586, 3.316000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1620055561.976705833, 4.115000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1620055563.003360367, 4.916000000]: Ready to take commands for planning group back_left_leg.[0m
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000000: mean_reward:5236.881220844038[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 0: episode_reward:8.914477233396845 steps:2[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 1: episode_reward:2.098480860146341 steps:4[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 2: episode_reward:-1.4376310043945297 steps:5[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000005: mean_reward:24638.435628182022[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Updating Policy
Updating
rddpg_torch.py:35: FutureWarning: arrays to stack must be passed as a "sequence" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.
  rdpg.train(params['train_episode_count'], checkpoint_path, True)
/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:130: UserWarning: Error detected in AddmmBackward. Traceback of forward call that caused the error:
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 323, in update_policy
    policy_loss = -self.agent.critic(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 358, in forward
    return self.out_dense_seq(torch.cat([ms, rs, ac], -1))
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/container.py", line 117, in forward
    input = module(input)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 93, in forward
    return F.linear(input, self.weight, self.bias)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/functional.py", line 1690, in linear
    ret = torch.addmm(bias, input, weight.t())
 (Triggered internally at  /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:104.)
  Variable._execution_engine.run_backward(
Traceback (most recent call last):
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 337, in update_policy
    policy_loss.backward(retain_graph=True)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py", line 130, in backward
    Variable._execution_engine.run_backward(
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [72, 1]], which is output 0 of TBackward, is at version 2; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!
2021-05-03 21:04:37.866744: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-03 21:04:37.866800: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1620056086.343244580, 1.447000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1620056086.348482655, 1.452000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1620056086.348762467, 1.452000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1620056087.796464225, 2.575000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1620056089.027927851, 3.521000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1620056090.040476496, 4.321000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1620056091.020297946, 5.121000000]: Ready to take commands for planning group back_left_leg.[0m
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000000: mean_reward:1654.416895451443[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 0: episode_reward:229.2763680030277 steps:4[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 1: episode_reward:-1.9228397304687448 steps:5[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000005: mean_reward:20623.05910419586[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Updating Policy
Updating
rddpg_torch.py:35: FutureWarning: arrays to stack must be passed as a "sequence" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.
  rdpg.train(params['train_episode_count'], checkpoint_path, True)
/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:130: UserWarning: Error detected in AddmmBackward. Traceback of forward call that caused the error:
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 323, in update_policy
    policy_loss = -self.agent.critic(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 358, in forward
    return self.out_dense_seq(torch.cat([ms, rs, ac], -1))
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/container.py", line 117, in forward
    input = module(input)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 93, in forward
    return F.linear(input, self.weight, self.bias)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/functional.py", line 1690, in linear
    ret = torch.addmm(bias, input, weight.t())
 (Triggered internally at  /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:104.)
  Variable._execution_engine.run_backward(
Traceback (most recent call last):
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 337, in update_policy
    policy_loss.backward(retain_graph=True)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py", line 130, in backward
    Variable._execution_engine.run_backward(
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [72, 1]], which is output 0 of TBackward, is at version 2; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!
2021-05-03 21:12:59.972576: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-03 21:12:59.972628: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1620056587.141101165, 1.377000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1620056587.143122865, 1.377000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1620056587.143469952, 1.381000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1620056588.675838126, 2.615000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1620056589.302009684, 3.038000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1620056589.807239835, 3.438000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1620056590.310307021, 3.837000000]: Ready to take commands for planning group back_left_leg.[0m
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000000: mean_reward:4120.433586703351[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 0: episode_reward:71473.91096802142 steps:3[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 1: episode_reward:49.00098949846637 steps:5[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[93m [RDDPG] Step_0000005: mean_reward:31518.647804412925[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Updating Policy
Updating
rddpg_torch.py:35: FutureWarning: arrays to stack must be passed as a "sequence" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.
  rdpg.train(params['train_episode_count'], checkpoint_path, True)
/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:130: UserWarning: Error detected in AddmmBackward. Traceback of forward call that caused the error:
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 322, in update_policy
    q_val = self.agent.critic(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 358, in forward
    return self.out_dense_seq(torch.cat([ms, rs, ac], -1))
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/container.py", line 117, in forward
    input = module(input)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 93, in forward
    return F.linear(input, self.weight, self.bias)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/functional.py", line 1690, in linear
    ret = torch.addmm(bias, input, weight.t())
 (Triggered internally at  /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:104.)
  Variable._execution_engine.run_backward(
Traceback (most recent call last):
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 336, in update_policy
    policy_loss.backward(retain_graph=True)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py", line 130, in backward
    Variable._execution_engine.run_backward(
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [72, 1]], which is output 0 of TBackward, is at version 2; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!
2021-05-03 23:47:43.908656: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-03 23:47:43.908699: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1620065870.781476936, 1.257000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1620065870.784302572, 1.258000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1620065870.784607353, 1.258000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1620065872.186987686, 2.415000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1620065873.103531835, 3.201000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1620065874.350844338, 4.201000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1620065875.307808681, 5.001000000]: Ready to take commands for planning group back_left_leg.[0m
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000000: mean_reward:685.6015695441047[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 0: episode_reward:-1.7918713586425636 steps:1[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 1: episode_reward:30565.28189537176 steps:4[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Updating Policy
Updating
rddpg_torch.py:35: FutureWarning: arrays to stack must be passed as a "sequence" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.
  rdpg.train(params['train_episode_count'], checkpoint_path, True)
updating done
Updating
/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:130: UserWarning: Error detected in AddmmBackward. Traceback of forward call that caused the error:
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 317, in update_policy
    action, (robot_enc_state, z) = self.agent.actor(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 287, in forward
    action, robot_enc_state, z = self.cell(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 258, in forward
    rs_r = self.robot_state_enc(self.robot_enc_state)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/container.py", line 117, in forward
    input = module(input)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 93, in forward
    return F.linear(input, self.weight, self.bias)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/functional.py", line 1690, in linear
    ret = torch.addmm(bias, input, weight.t())
 (Triggered internally at  /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:104.)
  Variable._execution_engine.run_backward(
Traceback (most recent call last):
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 334, in update_policy
    policy_loss.backward(retain_graph = True)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py", line 130, in backward
    Variable._execution_engine.run_backward(
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [135, 30]], which is output 0 of TBackward, is at version 2; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!
2021-05-03 23:54:09.229205: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-03 23:54:09.229276: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1620066256.342462002, 1.275000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1620066256.344192285, 1.276000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1620066256.344299525, 1.276000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1620066257.795207302, 2.407000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1620066258.835238388, 3.201000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1620066259.828639210, 4.001000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1620066261.032735124, 4.802000000]: Ready to take commands for planning group back_left_leg.[0m
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[93m [RDDPG] Step_0000000: mean_reward:19779.31898853792[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 0: episode_reward:-1.935493378906248 steps:1[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 1: episode_reward:33.233365161648415 steps:3[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 2: episode_reward:16.515067502055597 steps:5[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000005: mean_reward:33816.81711319146[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
Updating
rddpg_torch.py:35: FutureWarning: arrays to stack must be passed as a "sequence" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.
  rdpg.train(params['train_episode_count'], checkpoint_path, True)
updating done
Updating
/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:130: UserWarning: Error detected in AddmmBackward. Traceback of forward call that caused the error:
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 317, in update_policy
    action, (robot_enc_state, z) = self.agent.actor(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 287, in forward
    action, robot_enc_state, z = self.cell(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 258, in forward
    rs_r = self.robot_state_enc(self.robot_enc_state)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/container.py", line 117, in forward
    input = module(input)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 93, in forward
    return F.linear(input, self.weight, self.bias)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/functional.py", line 1690, in linear
    ret = torch.addmm(bias, input, weight.t())
 (Triggered internally at  /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:104.)
  Variable._execution_engine.run_backward(
Traceback (most recent call last):
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 334, in update_policy
    policy_loss.backward(retain_graph = True)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py", line 130, in backward
    Variable._execution_engine.run_backward(
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [135, 30]], which is output 0 of TBackward, is at version 2; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!
2021-05-04 00:54:10.507416: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-04 00:54:10.507462: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1620069857.152408853, 1.065000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1620069857.156193661, 1.068000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1620069857.156413853, 1.068000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1620069858.751652543, 2.368000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1620069859.778079954, 3.201000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1620069860.718199420, 4.001000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1620069861.682840040, 4.801000000]: Ready to take commands for planning group back_left_leg.[0m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[93m [RDDPG] Step_0000000: mean_reward:16070.467877670515[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 0: episode_reward:73.94608734941855 steps:2[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 1: episode_reward:14.713244959074071 steps:4[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Updating Policy
Updating
rddpg_torch.py:35: FutureWarning: arrays to stack must be passed as a "sequence" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.
  rdpg.train(params['train_episode_count'], checkpoint_path, True)
updating done
Updating
/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:130: UserWarning: Error detected in AddmmBackward. Traceback of forward call that caused the error:
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 317, in update_policy
    action, (robot_enc_state, z) = self.agent.actor(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 287, in forward
    action, robot_enc_state, z = self.cell(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 258, in forward
    rs_r = self.robot_state_enc(self.robot_enc_state)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/container.py", line 117, in forward
    input = module(input)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 93, in forward
    return F.linear(input, self.weight, self.bias)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/functional.py", line 1690, in linear
    ret = torch.addmm(bias, input, weight.t())
 (Triggered internally at  /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:104.)
  Variable._execution_engine.run_backward(
Traceback (most recent call last):
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 333, in update_policy
    policy_loss.backward(retain_graph = True)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py", line 130, in backward
    Variable._execution_engine.run_backward(
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [135, 30]], which is output 0 of TBackward, is at version 2; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!
2021-05-04 01:01:22.941826: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-04 01:01:22.941876: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1620070316.405700540, 23.505000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1620070316.408875709, 23.507000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1620070316.409168484, 23.507000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1620070317.911553888, 24.668000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1620070319.084650476, 25.601000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1620070320.057025478, 26.401000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1620070321.039665308, 27.200000000]: Ready to take commands for planning group back_left_leg.[0m
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000000: mean_reward:99.03951877838165[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 0: episode_reward:9393.607828666863 steps:4[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
Updating
rddpg_torch.py:35: FutureWarning: arrays to stack must be passed as a "sequence" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.
  rdpg.train(params['train_episode_count'], checkpoint_path, True)
updating done
Updating
/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:130: UserWarning: Error detected in AddmmBackward. Traceback of forward call that caused the error:
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 318, in update_policy
    action, (robot_enc_state, z) = self.agent.actor(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 287, in forward
    action, robot_enc_state, z = self.cell(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 258, in forward
    rs_r = self.robot_state_enc(self.robot_enc_state)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/container.py", line 117, in forward
    input = module(input)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 93, in forward
    return F.linear(input, self.weight, self.bias)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/functional.py", line 1690, in linear
    ret = torch.addmm(bias, input, weight.t())
 (Triggered internally at  /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:104.)
  Variable._execution_engine.run_backward(
Traceback (most recent call last):
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 334, in update_policy
    policy_loss.backward(retain_graph = True)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py", line 130, in backward
    Variable._execution_engine.run_backward(
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [135, 30]], which is output 0 of TBackward, is at version 2; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!
2021-05-04 01:06:14.653204: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-04 01:06:14.653363: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
Traceback (most recent call last):
  File "rddpg_torch.py", line 24, in <module>
    env = Env(
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/env.py", line 12, in __init__
    self.quadruped = Quadruped(params, experiment)
  File "/home/shandilya/Desktop/CNS/DDP/src/simulations/ws/src/quadruped/scripts/quadruped.py", line 485, in __init__
    self.all_legs = AllLegs(
  File "/home/shandilya/Desktop/CNS/DDP/src/simulations/ws/src/quadruped/scripts/quadruped.py", line 296, in __init__
    self.front_left = Leg(
  File "/home/shandilya/Desktop/CNS/DDP/src/simulations/ws/src/quadruped/scripts/quadruped.py", line 44, in __init__
    self.jta = actionlib.SimpleActionClient(
  File "/opt/ros/noetic/lib/python3/dist-packages/actionlib/simple_action_client.py", line 55, in __init__
    self.action_client = ActionClient(ns, ActionSpec)
  File "/opt/ros/noetic/lib/python3/dist-packages/actionlib/action_client.py", line 521, in __init__
    self.pub_queue_size = rospy.get_param('actionlib_client_pub_queue_size', 10)
  File "/opt/ros/noetic/lib/python3/dist-packages/rospy/client.py", line 467, in get_param
    return _param_server[param_name] #MasterProxy does all the magic for us
  File "/opt/ros/noetic/lib/python3/dist-packages/rospy/msproxy.py", line 121, in __getitem__
    code, msg, value = self.target.getParam(rospy.names.get_caller_id(), resolved_key)
  File "/usr/lib/python3.8/xmlrpc/client.py", line 1109, in __call__
    return self.__send(self.__name, args)
  File "/opt/ros/noetic/lib/python3/dist-packages/rospy/core.py", line 669, in _ServerProxy__request
    return xmlrpcclient.ServerProxy._ServerProxy__request(
  File "/usr/lib/python3.8/xmlrpc/client.py", line 1450, in __request
    response = self.__transport.request(
  File "/usr/lib/python3.8/xmlrpc/client.py", line 1153, in request
    return self.single_request(host, handler, request_body, verbose)
  File "/usr/lib/python3.8/xmlrpc/client.py", line 1165, in single_request
    http_conn = self.send_request(host, handler, request_body, verbose)
  File "/usr/lib/python3.8/xmlrpc/client.py", line 1278, in send_request
    self.send_content(connection, request_body)
  File "/usr/lib/python3.8/xmlrpc/client.py", line 1308, in send_content
    connection.endheaders(request_body)
  File "/usr/lib/python3.8/http/client.py", line 1250, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.8/http/client.py", line 1010, in _send_output
    self.send(msg)
  File "/usr/lib/python3.8/http/client.py", line 950, in send
    self.connect()
  File "/usr/lib/python3.8/http/client.py", line 921, in connect
    self.sock = self._create_connection(
  File "/usr/lib/python3.8/socket.py", line 808, in create_connection
    raise err
  File "/usr/lib/python3.8/socket.py", line 796, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused
2021-05-04 01:09:36.724446: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-04 01:09:36.724495: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1620070783.900787738, 1.275000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1620070783.902548773, 1.277000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1620070783.902778873, 1.277000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1620070785.434440628, 2.517000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1620070786.590196273, 3.402000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1620070787.549148085, 4.201000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1620070788.880656219, 5.200000000]: Ready to take commands for planning group back_left_leg.[0m
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000000: mean_reward:2787.8994590351367[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 0: episode_reward:78.28762402920412 steps:2[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 1: episode_reward:101.85525546923391 steps:4[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 2: episode_reward:-1.3588967226562485 steps:5[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[93m [RDDPG] Step_0000005: mean_reward:20943.280074953076[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Updating Policy
Updating
rddpg_torch.py:35: FutureWarning: arrays to stack must be passed as a "sequence" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.
  rdpg.train(params['train_episode_count'], checkpoint_path, True)
updating done
Updating
/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:130: UserWarning: Error detected in EluBackward. Traceback of forward call that caused the error:
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 318, in update_policy
    action, (robot_enc_state, z) = self.agent.actor(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 287, in forward
    action, robot_enc_state, z = self.cell(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 258, in forward
    rs_r = self.robot_state_enc(self.robot_enc_state)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/container.py", line 117, in forward
    input = module(input)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 477, in forward
    return F.elu(input, self.alpha, self.inplace)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/functional.py", line 1227, in elu
    result = torch._C._nn.elu(input, alpha)
 (Triggered internally at  /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:104.)
  Variable._execution_engine.run_backward(
Traceback (most recent call last):
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 334, in update_policy
    policy_loss.backward()
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py", line 130, in backward
    Variable._execution_engine.run_backward(
RuntimeError: Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time.
2021-05-04 01:18:41.043609: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-04 01:18:41.043657: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1620071328.723493092, 1.257000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1620071328.728754037, 1.260000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1620071328.729156102, 1.260000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1620071330.342371074, 2.426000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1620071331.378387787, 3.200000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1620071332.212815499, 3.800000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1620071333.317377134, 4.602000000]: Ready to take commands for planning group back_left_leg.[0m
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000000: mean_reward:4401.243456848027[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 0: episode_reward:101131.00703493913 steps:5[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000005: mean_reward:6190.032976451959[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Updating Policy
Updating
rddpg_torch.py:35: FutureWarning: arrays to stack must be passed as a "sequence" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.
  rdpg.train(params['train_episode_count'], checkpoint_path, True)
updating done
Updating
/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:130: UserWarning: Error detected in EluBackward. Traceback of forward call that caused the error:
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 312, in update_policy
    action, (robot_enc_state, z) = self.agent.actor(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 287, in forward
    action, robot_enc_state, z = self.cell(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 258, in forward
    rs_r = self.robot_state_enc(self.robot_enc_state)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/container.py", line 117, in forward
    input = module(input)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 477, in forward
    return F.elu(input, self.alpha, self.inplace)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/functional.py", line 1227, in elu
    result = torch._C._nn.elu(input, alpha)
 (Triggered internally at  /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:104.)
  Variable._execution_engine.run_backward(
Traceback (most recent call last):
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 334, in update_policy
    policy_loss.backward()
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py", line 130, in backward
    Variable._execution_engine.run_backward(
RuntimeError: Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time.
2021-05-04 01:25:47.239453: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-04 01:25:47.239503: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1620071757.204398914, 3.895000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1620071757.207053932, 3.895000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1620071757.207492576, 3.895000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1620071758.819379170, 5.138000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1620071759.916461158, 6.001000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1620071760.895117536, 6.804000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1620071761.859960444, 7.601000000]: Ready to take commands for planning group back_left_leg.[0m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000000: mean_reward:162.4772577839621[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 0: episode_reward:116.29623610537342 steps:2[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 1: episode_reward:-1.1595475228881835 steps:3[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 2: episode_reward:9.739214749988044 steps:5[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[93m [RDDPG] Step_0000005: mean_reward:71765.44775389899[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
Updating
rddpg_torch.py:35: FutureWarning: arrays to stack must be passed as a "sequence" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.
  rdpg.train(params['train_episode_count'], checkpoint_path, True)
updating done
Updating
/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:130: UserWarning: Error detected in EluBackward. Traceback of forward call that caused the error:
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 312, in update_policy
    action, (robot_enc_state, z) = self.agent.actor(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 287, in forward
    action, robot_enc_state, z = self.cell(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 258, in forward
    rs_r = self.robot_state_enc(self.robot_enc_state)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/container.py", line 117, in forward
    input = module(input)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 477, in forward
    return F.elu(input, self.alpha, self.inplace)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/functional.py", line 1227, in elu
    result = torch._C._nn.elu(input, alpha)
 (Triggered internally at  /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:104.)
  Variable._execution_engine.run_backward(
Traceback (most recent call last):
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 334, in update_policy
    policy_loss.backward(retain_graph = True)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py", line 130, in backward
    Variable._execution_engine.run_backward(
RuntimeError: Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time.
2021-05-04 01:30:00.109801: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-04 01:30:00.109849: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1620072007.408122036, 0.916000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1620072007.411416483, 0.917000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1620072007.412283615, 0.917000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1620072009.093335534, 2.006000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1620072010.171161236, 2.801000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1620072011.270790783, 3.600000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1620072012.426978058, 4.401000000]: Ready to take commands for planning group back_left_leg.[0m
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000000: mean_reward:49.285529309244204[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 0: episode_reward:-1.7489388190917938 steps:1[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 1: episode_reward:16035.987588406557 steps:4[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 2: episode_reward:-1.871559084472655 steps:5[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000005: mean_reward:5815.1616008978635[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
Updating
rddpg_torch.py:35: FutureWarning: arrays to stack must be passed as a "sequence" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.
  rdpg.train(params['train_episode_count'], checkpoint_path, True)
updating done
Updating
/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:130: UserWarning: Error detected in AddmmBackward. Traceback of forward call that caused the error:
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 312, in update_policy
    action, (robot_enc_state, z) = self.agent.actor(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 287, in forward
    action, robot_enc_state, z = self.cell(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 258, in forward
    rs_r = self.robot_state_enc(self.robot_enc_state)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/container.py", line 117, in forward
    input = module(input)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 93, in forward
    return F.linear(input, self.weight, self.bias)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/functional.py", line 1690, in linear
    ret = torch.addmm(bias, input, weight.t())
 (Triggered internally at  /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:104.)
  Variable._execution_engine.run_backward(
Traceback (most recent call last):
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 334, in update_policy
    policy_loss.backward(retain_graph = True)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py", line 130, in backward
    Variable._execution_engine.run_backward(
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [135, 30]], which is output 0 of TBackward, is at version 2; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!
2021-05-04 01:41:55.766426: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-04 01:41:55.766476: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1620072722.761680353, 1.221000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1620072722.764426612, 1.223000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1620072722.764576823, 1.223000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1620072724.319456619, 2.482000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1620072725.481177008, 3.400000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1620072726.419274423, 4.200000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1620072727.355275628, 5.002000000]: Ready to take commands for planning group back_left_leg.[0m
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000000: mean_reward:354.4953321769623[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 0: episode_reward:-1.3226425185546826 steps:1[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 1: episode_reward:3.3883183742016953 steps:3[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 2: episode_reward:4.532581881154591 steps:5[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000005: mean_reward:2764.183472739865[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Updating Policy
Updating
rddpg_torch.py:35: FutureWarning: arrays to stack must be passed as a "sequence" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.
  rdpg.train(params['train_episode_count'], checkpoint_path, True)
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Episode Done
[92m [RDDPG] 3: episode_reward:-1.9680018078613348 steps:6[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:130: UserWarning: Error detected in AddmmBackward. Traceback of forward call that caused the error:
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 315, in update_policy
    action, (robot_enc_state, z) = self.agent.actor(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 287, in forward
    action, robot_enc_state, z = self.cell(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 261, in forward
    self.pretrain_cell(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 201, in forward
    actions, Z, _, _ = self.rhythm_gen(
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 187, in forward
    out = self.complex_mlp(z)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/container.py", line 117, in forward
    input = module(input)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 74, in forward
    return apply_complex(self.fc_r, self.fc_i, input)
  File "/home/shandilya/Desktop/CNS/DDP/src/layers/torch_l.py", line 46, in apply_complex
    fr(y) + fi(x)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 93, in forward
    return F.linear(input, self.weight, self.bias)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/nn/functional.py", line 1690, in linear
    ret = torch.addmm(bias, input, weight.t())
 (Triggered internally at  /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:104.)
  Variable._execution_engine.run_backward(
Traceback (most recent call last):
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 156, in train
    self.update_policy()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 347, in update_policy
    policy_loss_total.backward()
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/autograd/__init__.py", line 130, in backward
    Variable._execution_engine.run_backward(
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [100, 12]], which is output 0 of TBackward, is at version 7; expected version 6 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!
2021-05-04 01:50:19.932111: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-04 01:50:19.932148: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1620073229.358150984, 0.938000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1620073229.359643582, 0.939000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1620073229.359766167, 0.939000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1620073230.723412464, 1.733000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1620073232.465702365, 2.634000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1620073234.017329390, 3.435000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1620073235.510699036, 4.233000000]: Ready to take commands for planning group back_left_leg.[0m
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000000: mean_reward:96.06188860478407[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 0: episode_reward:-1.9423287048339721 steps:1[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 1: episode_reward:15.936382013727687 steps:3[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 2: episode_reward:24.86252674402938 steps:5[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000005: mean_reward:7538.915405192367[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Updating Policy
Updating
rddpg_torch.py:35: FutureWarning: arrays to stack must be passed as a "sequence" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.
  rdpg.train(params['train_episode_count'], checkpoint_path, True)
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Episode Done
[92m [RDDPG] 3: episode_reward:-2.0966352770995815 steps:6[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Last Step of episode
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Episode Done
[92m [RDDPG] 4: episode_reward:39769.811681425825 steps:12[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Last Step of episode
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Episode Done
[92m [RDDPG] 5: episode_reward:27194.108027295995 steps:18[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Last Step of episode
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Episode Done
[92m [RDDPG] 6: episode_reward:33170.06662380176 steps:24[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Last Step of episode
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Episode Done
[92m [RDDPG] 7: episode_reward:30052.85901144458 steps:30[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000030: mean_reward:81138.8715624433[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 8: episode_reward:-1.450777116699229 steps:31[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Last Step of episode
[RDDPG] Episode Done
[92m [RDDPG] 9: episode_reward:38699.567390188764 steps:37[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Last Step of episode
[RDDPG] Episode Done
[92m [RDDPG] 10: episode_reward:15091.63557881974 steps:43[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Last Step of episode
[RDDPG] Episode Done
[92m [RDDPG] 11: episode_reward:990.2358875427656 steps:49[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Last Step of episode
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 12: episode_reward:40941.82231978553 steps:55[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000055: mean_reward:4771.37890240026[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 13: episode_reward:4.29817136269605 steps:57[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Last Step of episode
[RDDPG] Episode Done
[92m [RDDPG] 14: episode_reward:61328.47953503886 steps:63[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Last Step of episode
[RDDPG] Episode Done
[92m [RDDPG] 15: episode_reward:17299.121636769472 steps:69[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Last Step of episode
[RDDPG] Episode Done
[92m [RDDPG] 16: episode_reward:582.1247494585903 steps:75[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000075: mean_reward:5186.761178156142[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Episode Done
[92m [RDDPG] 17: episode_reward:-1.4828904299316767 steps:76[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Last Step of episode
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Episode Done
[92m [RDDPG] 18: episode_reward:13343.139018841635 steps:82[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Last Step of episode
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Episode Done
[92m [RDDPG] 19: episode_reward:13692.046240448479 steps:88[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Last Step of episode
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Episode Done
[92m [RDDPG] 20: episode_reward:25221.883913193793 steps:94[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Last Step of episode
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Episode Done
[92m [RDDPG] 21: episode_reward:87287.78016801851 steps:100[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000100: mean_reward:13978.953734992505[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 22: episode_reward:-1.689288602539002 steps:101[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Last Step of episode
[RDDPG] Episode Done
[92m [RDDPG] 23: episode_reward:69819.64017597778 steps:107[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Last Step of episode
[RDDPG] Episode Done
[92m [RDDPG] 24: episode_reward:48653.57996306676 steps:113[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Last Step of episode
[RDDPG] Episode Done
[92m [RDDPG] 25: episode_reward:11804.037649862448 steps:119[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Quadruped Not Upright
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Episode Done
[92m [RDDPG] 26: episode_reward:42096.68377309579 steps:124[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Last Step of episode
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Episode Done
[92m [RDDPG] 27: episode_reward:110118.52399516033 steps:130[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
Traceback (most recent call last):
  File "/opt/ros/noetic/lib/python3/dist-packages/rospy/impl/tcpros_base.py", line 562, in connect
    self.read_header()
  File "/opt/ros/noetic/lib/python3/dist-packages/rospy/impl/tcpros_base.py", line 657, in read_header
    self._validate_header(read_ros_handshake_header(sock, self.read_buff, self.protocol.buff_size))
  File "/opt/ros/noetic/lib/python3/dist-packages/rosgraph/network.py", line 357, in read_ros_handshake_header
    d = sock.recv(buff_size)
ConnectionResetError: [Errno 104] Connection reset by peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/ros/noetic/lib/python3/dist-packages/rospy/impl/tcpros_service.py", line 509, in call
    transport.connect(dest_addr, dest_port, service_uri)
  File "/opt/ros/noetic/lib/python3/dist-packages/rospy/impl/tcpros_base.py", line 589, in connect
    raise TransportInitError(str(e)) #re-raise i/o error
rospy.exceptions.TransportInitError: [Errno 104] Connection reset by peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 192, in train
    validate_reward = self.evaluate(
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/evaluator.py", line 37, in __call__
    observation, reward, done, info = env.step(
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/env.py", line 43, in step
    self.quadruped.set_support_lines()
  File "/home/shandilya/Desktop/CNS/DDP/src/simulations/ws/src/quadruped/scripts/quadruped.py", line 1022, in set_support_lines
    current_pose = self.kinematics.get_end_effector_fk(
  File "/home/shandilya/Desktop/CNS/DDP/src/simulations/ws/src/quadruped/scripts/kinematics.py", line 84, in get_end_effector_fk
    msg = self.compute_fk_proxy(
  File "/opt/ros/noetic/lib/python3/dist-packages/rospy/impl/tcpros_service.py", line 442, in __call__
    return self.call(*args, **kwds)
  File "/opt/ros/noetic/lib/python3/dist-packages/rospy/impl/tcpros_service.py", line 512, in call
    raise ServiceException("unable to connect to service: %s"%e)
rospy.service.ServiceException: unable to connect to service: [Errno 104] Connection reset by peer
2021-05-04 02:09:26.142825: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-04 02:09:26.142862: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1620074384.826950509, 8.435000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1620074384.829574376, 8.436000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1620074384.830091062, 8.437000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1620074386.330981187, 9.386000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1620074387.780875546, 10.294000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1620074388.945397065, 11.094000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1620074390.114629339, 11.894000000]: Ready to take commands for planning group back_left_leg.[0m
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[93m [RDDPG] Step_0000000: mean_reward:17854.705107645983[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 0: episode_reward:-1.76068157812501 steps:1[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 1: episode_reward:5.387875132402882 steps:3[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 2: episode_reward:-1.7723629765625162 steps:4[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
Updating
rddpg_torch.py:35: FutureWarning: arrays to stack must be passed as a "sequence" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.
  rdpg.train(params['train_episode_count'], checkpoint_path, True)
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Last Step of episode
[RDDPG] Updating Policy
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
Updating
updating done
[RDDPG] Episode Done
[92m [RDDPG] 3: episode_reward:32986.42144933438 steps:10[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
Traceback (most recent call last):
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 192, in train
    validate_reward = self.evaluate(
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/evaluator.py", line 37, in __call__
    observation, reward, done, info = env.step(
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/env.py", line 36, in step
    observation =  self.quadruped.step(
  File "/home/shandilya/Desktop/CNS/DDP/src/simulations/ws/src/quadruped/scripts/quadruped.py", line 1216, in step
    self.set_observation(action, desired_motion)
  File "/home/shandilya/Desktop/CNS/DDP/src/simulations/ws/src/quadruped/scripts/quadruped.py", line 1073, in set_observation
    rospy.wait_for_service('/gazebo/get_model_state')
  File "/opt/ros/noetic/lib/python3/dist-packages/rospy/impl/tcpros_service.py", line 166, in wait_for_service
    raise ROSInterruptException("rospy shutdown")
rospy.exceptions.ROSInterruptException: rospy shutdown
2021-05-04 02:13:40.601590: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-04 02:13:40.601637: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1620074627.779526864, 1.288000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1620074627.786464488, 1.289000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1620074627.787753291, 1.290000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1620074629.462452309, 2.401000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1620074630.341358736, 2.966000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1620074631.482900651, 3.765000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1620074632.373980128, 4.365000000]: Ready to take commands for planning group back_left_leg.[0m
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000000: mean_reward:875.4974806661132[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 0: episode_reward:-596.5795077704729 steps:2[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 1: episode_reward:-290.4248115234332 steps:3[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
rddpg_torch.py:35: FutureWarning: arrays to stack must be passed as a "sequence" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.
  rdpg.train(params['train_episode_count'], checkpoint_path, True)
[RDDPG] Updating Policy
[RDDPG] Last Step of episode
[RDDPG] Episode Done
[92m [RDDPG] 2: episode_reward:34296.04619270422 steps:9[00m
[RDDPG] Resetting Environment
[RDDPG] Updating Policy
[RDDPG] Updating Policy
[RDDPG] Updating Policy
[RDDPG] Last Step of episode
[RDDPG] Episode Done
[92m [RDDPG] 3: episode_reward:20614.58666233369 steps:15[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
[RDDPG] Resetting Environment
Traceback (most recent call last):
  File "rddpg_torch.py", line 35, in <module>
    rdpg.train(params['train_episode_count'], checkpoint_path, True)
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/rdpg.py", line 192, in train
    validate_reward = self.evaluate(
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/evaluator.py", line 22, in __call__
    observation = env.reset()
  File "/home/shandilya/Desktop/CNS/DDP/src/rl/torch/env.py", line 16, in reset
    self._state, self._reward = self.quadruped.reset()
  File "/home/shandilya/Desktop/CNS/DDP/src/simulations/ws/src/quadruped/scripts/quadruped.py", line 958, in reset
    rospy.sleep(2.0)
  File "/opt/ros/noetic/lib/python3/dist-packages/rospy/timer.py", line 165, in sleep
    raise rospy.exceptions.ROSInterruptException("ROS shutdown request")
rospy.exceptions.ROSInterruptException: ROS shutdown request
2021-05-04 02:19:08.346186: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
2021-05-04 02:19:08.346233: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1620074955.795017516, 1.478000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1620074955.796776868, 1.479000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1620074955.796904087, 1.479000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1620074957.239381506, 2.413000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1620074957.896874004, 2.866000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1620074958.722451159, 3.466000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1620074959.655460755, 4.065000000]: Ready to take commands for planning group back_left_leg.[0m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[93m [RDDPG] Step_0000000: mean_reward:1532.1245702644865[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 0: episode_reward:-597.794805959667 steps:2[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 1: episode_reward:-261.15305126952876 steps:3[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 2: episode_reward:-45.42006349182143 steps:4[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 3: episode_reward:5174.459528823608 steps:7[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 4: episode_reward:-70.21879174804661 steps:8[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 5: episode_reward:-196.23515954589928 steps:9[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 6: episode_reward:-685.8505503021024 steps:11[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 7: episode_reward:-417.2279057655011 steps:13[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 8: episode_reward:-488.4508845188526 steps:16[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 9: episode_reward:8825.33192418847 steps:19[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 10: episode_reward:-345.46053841293315 steps:21[00m
[RDDPG] Resetting Environment
[RDDPG] Quadruped Not Upright
[RDDPG] Episode Done
[92m [RDDPG] 11: episode_reward:30267.712087416836 steps:25[00m
[RDDPG] Start Evaluation
[RDDPG] Resetting Environment
