2021-04-12 11:01:02.305513: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
Unable to init server: Could not connect: Connection refused
Unable to init server: Could not connect: Connection refused

(learn.py:12733): Gdk-CRITICAL **: 11:01:08.702: gdk_cursor_new_for_display: assertion 'GDK_IS_DISPLAY (display)' failed
2021-04-12 11:01:09.513107: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-04-12 11:01:09.515711: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-04-12 11:01:09.546198: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-12 11:01:09.548494: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:00:1e.0 name: Tesla T4 computeCapability: 7.5
coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s
2021-04-12 11:01:09.548534: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-04-12 11:01:09.557022: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-04-12 11:01:09.557168: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-04-12 11:01:09.561194: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-04-12 11:01:09.561593: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-04-12 11:01:09.571392: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-04-12 11:01:09.573534: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-04-12 11:01:09.574894: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-04-12 11:01:09.575136: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-12 11:01:09.577067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-12 11:01:09.577831: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
[DDPG] Building the actor model
2021-04-12 11:01:09.605444: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-04-12 11:01:09.605695: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-04-12 11:01:09.607052: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-12 11:01:09.608047: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:00:1e.0 name: Tesla T4 computeCapability: 7.5
coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s
2021-04-12 11:01:09.608081: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-04-12 11:01:09.608211: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-04-12 11:01:09.608303: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-04-12 11:01:09.608381: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-04-12 11:01:09.608444: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-04-12 11:01:09.608509: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-04-12 11:01:09.608568: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-04-12 11:01:09.608622: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-04-12 11:01:09.610148: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-12 11:01:09.611219: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-12 11:01:09.611967: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-04-12 11:01:09.612016: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-04-12 11:01:11.516197: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-04-12 11:01:11.516240: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2021-04-12 11:01:11.516255: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2021-04-12 11:01:11.517224: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-12 11:01:11.519363: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-12 11:01:11.521214: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-12 11:01:11.523147: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13164 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5)
[DDPG] Building the actor model
[DDPG] Building the critic model
[DDPG] Building the critic model
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1618225280.900878873, 3.621000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1618225280.903197518, 3.623000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1618225280.904252154, 3.624000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1618225282.390151283, 4.382000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1618225283.654100230, 5.039000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1618225285.290661594, 5.839000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1618225287.745055949, 7.039000000]: Ready to take commands for planning group back_left_leg.[0m
[Actor] GPU>>>>>>>>>>>>
[Actor] [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
[DDPG] Loading Actor Weights
[DDPG] Training Start
[DDPG] Resetting Environment
[DDPG] Starting Pretraining Test
2021-04-12 11:02:11.793682: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-04-12 11:02:13.112701: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
[DDPG] Step 0 Reward 36.27349 Time 13.87592
[DDPG] Step 1 Reward -8.37204 Time 12.29352
[DDPG] Step 2 Reward -4.50459 Time 11.80263
[DDPG] Step 3 Reward 1.91556 Time 12.94310
[DDPG] Quadruped Not Upright
[DDPG] Step 4 Reward -15.25506 Time 10.68537
[DDPG] Resetting Environment
[DDPG] Starting Episode 0
[DDPG] Episode 0 Step 0 Reward -4.92752 Time 11.37349
[DDPG] Quadruped Not Upright
[DDPG] Episode 0 Step 1 Reward -20.71866 Time 10.27140
[DDPG] Starting Next Episode
[DDPG] Critic Loss 214.1677703857422
[DDPG] Total Reward -25.64617 Avg Critic Loss 214.16777 Time 5.45068
[DDPG] Saving Data
[DDPG] Saving Model
[DDPG] Resetting Environment
[DDPG] Starting Episode 1
[DDPG] Quadruped Not Upright
[DDPG] Episode 1 Step 0 Reward -16.19783 Time 11.05610
[DDPG] Starting Next Episode
[DDPG] Critic Loss 226.7623748779297
[DDPG] Total Reward -16.19783 Avg Critic Loss 226.76237 Time 5.36876
[DDPG] Resetting Environment
[DDPG] Starting Episode 2
[DDPG] Episode 2 Step 0 Reward -5.27859 Time 14.95739
[DDPG] Episode 2 Step 1 Reward -6.38187 Time 14.54629
[DDPG] Episode 2 Step 2 Reward -4.62632 Time 15.43243
[DDPG] Episode 2 Step 3 Reward -5.19807 Time 16.14907
[DDPG] Quadruped Not Upright
[DDPG] Episode 2 Step 4 Reward -17.01465 Time 15.95258
[DDPG] Starting Next Episode
[DDPG] Critic Loss 126.59771728515625
[DDPG] Total Reward -38.49949 Avg Critic Loss 126.59772 Time 5.80260
[DDPG] Resetting Environment
[DDPG] Starting Episode 3
[DDPG] Quadruped Not Upright
[DDPG] Episode 3 Step 0 Reward -16.12695 Time 14.51248
[DDPG] Starting Next Episode
[DDPG] Critic Loss 140.72760009765625
[DDPG] Total Reward -16.12695 Avg Critic Loss 140.72760 Time 6.09823
[DDPG] Resetting Environment
[DDPG] Starting Episode 4
[DDPG] Episode 4 Step 0 Reward 20.10146 Time 17.73206
[DDPG] Quadruped Not Upright
[DDPG] Episode 4 Step 1 Reward -21.99979 Time 13.59062
[DDPG] Starting Next Episode
[DDPG] Critic Loss 196.58779907226562
[DDPG] Total Reward -1.89833 Avg Critic Loss 196.58780 Time 4.00057
[DDPG] Resetting Environment
[DDPG] Starting Episode 5
[DDPG] Episode 5 Step 0 Reward -0.86365 Time 16.52010
[DDPG] Quadruped Not Upright
[DDPG] Episode 5 Step 1 Reward -19.43079 Time 13.92148
[DDPG] Starting Next Episode
[DDPG] Critic Loss 192.94190979003906
[DDPG] Total Reward -20.29444 Avg Critic Loss 192.94191 Time 6.57827
[DDPG] Resetting Environment
[DDPG] Starting Episode 6
[DDPG] Episode 6 Step 0 Reward -5.06489 Time 15.82151
[DDPG] Quadruped Not Upright
[DDPG] Episode 6 Step 1 Reward -21.59675 Time 15.47120
[DDPG] Starting Next Episode
[DDPG] Critic Loss 198.03656005859375
[DDPG] Total Reward -26.66164 Avg Critic Loss 198.03656 Time 6.66777
[DDPG] Resetting Environment
[DDPG] Starting Episode 7
[DDPG] Episode 7 Step 0 Reward -4.39260 Time 17.49939
[DDPG] Quadruped Not Upright
[DDPG] Episode 7 Step 1 Reward -17.32840 Time 14.45160
[DDPG] Starting Next Episode
[DDPG] Critic Loss 190.93588256835938
[DDPG] Total Reward -21.72100 Avg Critic Loss 190.93588 Time 6.08435
[DDPG] Resetting Environment
[DDPG] Starting Episode 8
[DDPG] Episode 8 Step 0 Reward -6.10647 Time 16.04926
[DDPG] Quadruped Not Upright
[DDPG] Episode 8 Step 1 Reward -15.29105 Time 13.95131
[DDPG] Starting Next Episode
[DDPG] Critic Loss 183.52230834960938
[DDPG] Total Reward -21.39752 Avg Critic Loss 183.52231 Time 5.61978
[DDPG] Resetting Environment
[DDPG] Starting Episode 9
[DDPG] Quadruped Not Upright
[DDPG] Episode 9 Step 0 Reward -16.30048 Time 16.93022
[DDPG] Starting Next Episode
[DDPG] Critic Loss 187.57516479492188
[DDPG] Total Reward -16.30048 Avg Critic Loss 187.57516 Time 7.02593
[DDPG] Resetting Environment
[DDPG] Starting Episode 10
[DDPG] Episode 10 Step 0 Reward -6.13219 Time 21.52219
[DDPG] Quadruped Not Upright
[DDPG] Episode 10 Step 1 Reward -14.78726 Time 19.02789
[DDPG] Starting Next Episode
[DDPG] Critic Loss 180.572509765625
[DDPG] Total Reward -20.91945 Avg Critic Loss 180.57251 Time 7.00092
[DDPG] Resetting Environment
[DDPG] Starting Episode 11
[DDPG] Episode 11 Step 0 Reward -4.24099 Time 24.16269
[DDPG] Quadruped Not Upright
[DDPG] Episode 11 Step 1 Reward -14.29108 Time 19.18734
[DDPG] Starting Next Episode
[DDPG] Critic Loss 174.3817901611328
[DDPG] Total Reward -18.53207 Avg Critic Loss 174.38179 Time 6.11942
[DDPG] Resetting Environment
[DDPG] Starting Episode 12
[DDPG] Episode 12 Step 0 Reward -4.32425 Time 20.69873
[DDPG] Quadruped Not Upright
[DDPG] Episode 12 Step 1 Reward -17.72599 Time 19.17735
[DDPG] Starting Next Episode
[DDPG] Critic Loss 172.74505615234375
[DDPG] Total Reward -22.05023 Avg Critic Loss 172.74506 Time 7.71122
[DDPG] Resetting Environment
[DDPG] Starting Episode 13
[DDPG] Quadruped Not Upright
[DDPG] Episode 13 Step 0 Reward -12.79409 Time 19.08448
[DDPG] Starting Next Episode
[DDPG] Critic Loss 172.0964813232422
[DDPG] Total Reward -12.79409 Avg Critic Loss 172.09648 Time 5.88090
[DDPG] Resetting Environment
[DDPG] Starting Episode 14
[DDPG] Quadruped Not Upright
[DDPG] Episode 14 Step 0 Reward -14.27027 Time 20.46545
[DDPG] Starting Next Episode
[DDPG] Critic Loss 172.75157165527344
[DDPG] Total Reward -14.27027 Avg Critic Loss 172.75157 Time 8.82861
[DDPG] Resetting Environment
[DDPG] Starting Episode 15
[DDPG] Quadruped Not Upright
[DDPG] Episode 15 Step 0 Reward -12.92588 Time 24.70337
[DDPG] Starting Next Episode
[DDPG] Critic Loss 172.4762420654297
[DDPG] Total Reward -12.92588 Avg Critic Loss 172.47624 Time 9.52685
[DDPG] Resetting Environment
[DDPG] Starting Episode 16
[DDPG] Episode 16 Step 0 Reward -3.56051 Time 28.68527
[DDPG] Episode 16 Step 1 Reward -5.70059 Time 27.16276
[DDPG] Quadruped Not Upright
[DDPG] Episode 16 Step 2 Reward -24.64278 Time 25.66720
[DDPG] Starting Next Episode
[DDPG] Critic Loss 175.7628173828125
[DDPG] Total Reward -33.90387 Avg Critic Loss 175.76282 Time 9.64848
[DDPG] Resetting Environment
[DDPG] Starting Episode 17
[DDPG] Episode 17 Step 0 Reward -4.23325 Time 25.33596
[DDPG] Quadruped Not Upright
[DDPG] Episode 17 Step 1 Reward -23.31362 Time 23.30680
[DDPG] Starting Next Episode
[DDPG] Critic Loss 181.34695434570312
[DDPG] Total Reward -27.54687 Avg Critic Loss 181.34695 Time 9.76486
[DDPG] Resetting Environment
[DDPG] Starting Episode 18
[DDPG] Quadruped Not Upright
[DDPG] Episode 18 Step 0 Reward -15.18167 Time 23.50734
[DDPG] Starting Next Episode
[DDPG] Critic Loss 181.82943725585938
[DDPG] Total Reward -15.18167 Avg Critic Loss 181.82944 Time 9.97639
[DDPG] Resetting Environment
[DDPG] Starting Episode 19
[DDPG] Episode 19 Step 0 Reward -4.95243 Time 24.99678
[DDPG] Episode 19 Step 1 Reward -6.49079 Time 26.96460
[DDPG] Episode 19 Step 2 Reward -5.35923 Time 29.02786
[DDPG] Episode 19 Step 3 Reward -8.51663 Time 27.92514
[DDPG] Quadruped Not Upright
[DDPG] Episode 19 Step 4 Reward -13.27096 Time 29.63499
[DDPG] Starting Next Episode
[DDPG] Critic Loss 167.32826232910156
[DDPG] Total Reward -38.59004 Avg Critic Loss 167.32826 Time 12.18562
[DDPG] Resetting Environment
[DDPG] Starting Episode 20
[DDPG] Quadruped Not Upright
[DDPG] Episode 20 Step 0 Reward -16.17940 Time 30.51104
[DDPG] Starting Next Episode
[DDPG] Critic Loss 167.7338104248047
[DDPG] Total Reward -16.17940 Avg Critic Loss 167.73381 Time 10.80839
[DDPG] Resetting Environment
[DDPG] Starting Episode 21
[DDPG] Episode 21 Step 0 Reward -9.46709 Time 30.82988
[DDPG] Episode 21 Step 1 Reward -3.29902 Time 31.54012
[DDPG] Episode 21 Step 2 Reward -6.35781 Time 35.71252
[DDPG] Episode 21 Step 3 Reward -5.04952 Time 31.81198
[DDPG] Episode 21 Step 4 Reward -6.90120 Time 35.35872
[DDPG] Episode 21 Step 5 Reward -7.97488 Time 29.40609
[DDPG] Episode 21 Step 6 Reward -9.69433 Time 32.53318
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss 149.5659942626953
[DDPG] Total Reward -48.74385 Avg Critic Loss 149.56599 Time 14.22986
[DDPG] Resetting Environment
[DDPG] Starting Episode 22
[DDPG] Episode 22 Step 0 Reward -6.70448 Time 34.69573
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss 146.92987060546875
[DDPG] Total Reward -6.70448 Avg Critic Loss 146.92987 Time 14.69603
[DDPG] Resetting Environment
[DDPG] Starting Episode 23
[DDPG] Quadruped Not Upright
[DDPG] Episode 23 Step 0 Reward -18.49541 Time 36.35237
[DDPG] Starting Next Episode
[DDPG] Critic Loss 147.99664306640625
[DDPG] Total Reward -18.49541 Avg Critic Loss 147.99664 Time 14.23729
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 24
[DDPG] Episode 24 Step 0 Reward -9.18717 Time 33.86680
[DDPG] Episode 24 Step 1 Reward -5.97083 Time 35.85316
[DDPG] Episode 24 Step 2 Reward -5.66859 Time 38.04512
[DDPG] Episode 24 Step 3 Reward -5.75695 Time 37.49391
[DDPG] Episode 24 Step 4 Reward -5.81418 Time 35.01462
[DDPG] Episode 24 Step 5 Reward -5.75448 Time 35.71423
[DDPG] Episode 24 Step 6 Reward -5.81594 Time 38.63524
[DDPG] Quadruped Not Upright
[DDPG] Episode 24 Step 7 Reward -17.20232 Time 39.23426
[DDPG] Starting Next Episode
[DDPG] Critic Loss 138.32818603515625
[DDPG] Total Reward -61.17046 Avg Critic Loss 138.32819 Time 11.03817
[DDPG] Resetting Environment
[DDPG] Starting Episode 25
[DDPG] Episode 25 Step 0 Reward -7.23769 Time 37.77297
[DDPG] Episode 25 Step 1 Reward -6.62097 Time 36.72751
[DDPG] Episode 25 Step 2 Reward -5.99931 Time 38.10074
[DDPG] Episode 25 Step 3 Reward -5.76919 Time 35.05314
[DDPG] Episode 25 Step 4 Reward -5.66057 Time 36.91847
[DDPG] Episode 25 Step 5 Reward -5.81454 Time 37.63969
[DDPG] Episode 25 Step 6 Reward -6.16646 Time 37.36530
[DDPG] Episode 25 Step 7 Reward -5.64181 Time 36.92171
[DDPG] Episode 25 Step 8 Reward -6.41292 Time 38.02781
[DDPG] Episode 25 Step 9 Reward -7.70667 Time 37.64913
[DDPG] Episode 25 Step 10 Reward -8.91625 Time 39.08467
[DDPG] Episode 25 Step 11 Reward -6.65838 Time 34.04497
[DDPG] Quadruped Not Upright
[DDPG] Episode 25 Step 12 Reward -15.70305 Time 32.37512
[DDPG] Starting Next Episode
[DDPG] Critic Loss 123.28042602539062
[DDPG] Total Reward -94.30782 Avg Critic Loss 123.28043 Time 13.68640
[DDPG] Saving Data
[DDPG] Saving Model
[DDPG] Resetting Environment
[DDPG] Starting Episode 26
[DDPG] Episode 26 Step 0 Reward -8.38369 Time 36.09545
[DDPG] Episode 26 Step 1 Reward -5.78689 Time 37.18031
[DDPG] Episode 26 Step 2 Reward -5.91966 Time 35.57758
[DDPG] Episode 26 Step 3 Reward -6.52406 Time 37.68378
[DDPG] Episode 26 Step 4 Reward -7.12294 Time 35.49810
[DDPG] Episode 26 Step 5 Reward -6.05386 Time 39.70801
[DDPG] Episode 26 Step 6 Reward -6.10649 Time 38.62752
[DDPG] Episode 26 Step 7 Reward -6.42502 Time 36.36275
[DDPG] Episode 26 Step 8 Reward -7.48569 Time 35.94350
[DDPG] Episode 26 Step 9 Reward -8.78524 Time 35.44786
[DDPG] Episode 26 Step 10 Reward -7.59209 Time 36.85107
[DDPG] Episode 26 Step 11 Reward -6.97367 Time 36.22908
[DDPG] Episode 26 Step 12 Reward -6.71585 Time 38.13199
[DDPG] Episode 26 Step 13 Reward -5.72450 Time 35.67664
[DDPG] Episode 26 Step 14 Reward -5.96405 Time 37.85454
[DDPG] Episode 26 Step 15 Reward -5.86937 Time 38.58371
[DDPG] Episode 26 Step 16 Reward -5.48574 Time 35.81761
[DDPG] Episode 26 Step 17 Reward -5.48073 Time 35.62821
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss 111.59127807617188
[DDPG] Total Reward -118.39953 Avg Critic Loss 111.59128 Time 15.82829
[DDPG] Resetting Environment
[DDPG] Starting Episode 27
[DDPG] Episode 27 Step 0 Reward -8.43521 Time 35.57831
[DDPG] Episode 27 Step 1 Reward -5.83474 Time 37.15607
[DDPG] Episode 27 Step 2 Reward -5.77507 Time 37.50035
[DDPG] Episode 27 Step 3 Reward -5.77628 Time 38.50708
[DDPG] Episode 27 Step 4 Reward -5.84268 Time 38.80990
[DDPG] Episode 27 Step 5 Reward -8.62694 Time 36.06284
[DDPG] Episode 27 Step 6 Reward -7.08503 Time 38.25968
[DDPG] Episode 27 Step 7 Reward -5.45061 Time 41.40320
[DDPG] Episode 27 Step 8 Reward -5.96595 Time 37.15641
[DDPG] Episode 27 Step 9 Reward -7.55023 Time 37.46548
[DDPG] Episode 27 Step 10 Reward -6.11204 Time 37.23080
[DDPG] Episode 27 Step 11 Reward -5.78155 Time 39.24616
[DDPG] Episode 27 Step 12 Reward -5.48514 Time 35.88917
[DDPG] Episode 27 Step 13 Reward -5.60852 Time 35.98528
[DDPG] Episode 27 Step 14 Reward -5.70930 Time 37.07308
[DDPG] Episode 27 Step 15 Reward -5.74655 Time 38.24241
[DDPG] Episode 27 Step 16 Reward -5.71829 Time 39.66612
[DDPG] Episode 27 Step 17 Reward -5.72961 Time 35.71045
[DDPG] Episode 27 Step 18 Reward -5.27525 Time 37.30773
[DDPG] Quadruped Not Upright
[DDPG] Episode 27 Step 19 Reward -17.56620 Time 36.42713
[DDPG] Starting Next Episode
[DDPG] Critic Loss 96.6520004272461
[DDPG] Total Reward -135.07519 Avg Critic Loss 96.65200 Time 16.83139
[DDPG] Resetting Environment
[DDPG] Starting Episode 28
[DDPG] Episode 28 Step 0 Reward -8.36234 Time 35.20981
[DDPG] Episode 28 Step 1 Reward -5.71689 Time 35.97223
[DDPG] Episode 28 Step 2 Reward -5.77472 Time 40.03825
[DDPG] Episode 28 Step 3 Reward -5.74969 Time 36.52376
[DDPG] Episode 28 Step 4 Reward -6.00800 Time 36.40750
[DDPG] Episode 28 Step 5 Reward -6.13205 Time 37.04946
[DDPG] Episode 28 Step 6 Reward -5.96875 Time 37.44837
[DDPG] Episode 28 Step 7 Reward -5.94603 Time 40.08135
[DDPG] Episode 28 Step 8 Reward -7.01361 Time 35.43559
[DDPG] Episode 28 Step 9 Reward -7.21642 Time 35.54029
[DDPG] Episode 28 Step 10 Reward -7.57857 Time 37.41837
[DDPG] Episode 28 Step 11 Reward -6.43621 Time 38.88828
[DDPG] Episode 28 Step 12 Reward -5.26616 Time 37.86781
[DDPG] Episode 28 Step 13 Reward -5.33477 Time 35.71136
[DDPG] Episode 28 Step 14 Reward -5.45935 Time 41.00068
[DDPG] Episode 28 Step 15 Reward -5.57164 Time 39.79168
[DDPG] Episode 28 Step 16 Reward -5.68760 Time 37.30971
[DDPG] Episode 28 Step 17 Reward -5.95287 Time 40.67333
[DDPG] Quadruped Not Upright
[DDPG] Episode 28 Step 18 Reward -16.08917 Time 33.93992
[DDPG] Starting Next Episode
[DDPG] Critic Loss 94.44242858886719
[DDPG] Total Reward -127.26484 Avg Critic Loss 94.44243 Time 15.96683
[DDPG] Resetting Environment
[DDPG] Starting Episode 29
[DDPG] Episode 29 Step 0 Reward -7.04626 Time 37.33379
[DDPG] Episode 29 Step 1 Reward -5.70183 Time 36.41507
[DDPG] Episode 29 Step 2 Reward -5.81805 Time 36.71812
[DDPG] Episode 29 Step 3 Reward -7.32348 Time 36.15479
[DDPG] Episode 29 Step 4 Reward -6.15131 Time 37.44599
[DDPG] Episode 29 Step 5 Reward -6.03678 Time 36.03496
[DDPG] Episode 29 Step 6 Reward -5.97032 Time 38.30515
[DDPG] Episode 29 Step 7 Reward -6.16173 Time 39.40174
[DDPG] Episode 29 Step 8 Reward -8.06140 Time 34.09339
[DDPG] Episode 29 Step 9 Reward -6.76104 Time 36.81874
[DDPG] Episode 29 Step 10 Reward -7.82723 Time 39.37808
[DDPG] Episode 29 Step 11 Reward -6.31265 Time 34.85108
[DDPG] Episode 29 Step 12 Reward -5.51724 Time 38.37155
[DDPG] Episode 29 Step 13 Reward -4.68603 Time 36.95744
[DDPG] Episode 29 Step 14 Reward -4.68472 Time 36.73205
[DDPG] Episode 29 Step 15 Reward -4.56051 Time 37.17480
[DDPG] Episode 29 Step 16 Reward -4.48561 Time 36.98436
[DDPG] Episode 29 Step 17 Reward -4.36729 Time 37.52047
[DDPG] Episode 29 Step 18 Reward -4.20778 Time 36.96807
[DDPG] Episode 29 Step 19 Reward -4.64091 Time 36.04266
[DDPG] Episode 29 Step 20 Reward -5.12800 Time 37.97821
[DDPG] Quadruped Not Upright
[DDPG] Episode 29 Step 21 Reward -18.39208 Time 33.52720
[DDPG] Starting Next Episode
[DDPG] Critic Loss 79.57408142089844
[DDPG] Total Reward -139.84226 Avg Critic Loss 79.57408 Time 15.48965
[DDPG] Resetting Environment
[DDPG] Starting Episode 30
[DDPG] Episode 30 Step 0 Reward -7.14784 Time 37.50405
[DDPG] Episode 30 Step 1 Reward -5.46099 Time 34.53514
[DDPG] Episode 30 Step 2 Reward -5.65353 Time 36.94612
[DDPG] Episode 30 Step 3 Reward -5.99259 Time 35.27750
[DDPG] Episode 30 Step 4 Reward -5.97423 Time 37.07830
[DDPG] Episode 30 Step 5 Reward -6.23691 Time 36.22615
[DDPG] Episode 30 Step 6 Reward -6.14518 Time 36.50904
[DDPG] Episode 30 Step 7 Reward -7.71192 Time 36.96447
[DDPG] Episode 30 Step 8 Reward -5.58446 Time 37.29857
[DDPG] Episode 30 Step 9 Reward -5.68392 Time 35.14417
[DDPG] Episode 30 Step 10 Reward -5.80590 Time 38.24674
[DDPG] Episode 30 Step 11 Reward -5.82245 Time 37.54300
[DDPG] Episode 30 Step 12 Reward -5.80972 Time 38.02910
[DDPG] Episode 30 Step 13 Reward -5.76266 Time 36.55361
[DDPG] Episode 30 Step 14 Reward -5.78067 Time 35.75649
[DDPG] Episode 30 Step 15 Reward -5.75508 Time 37.54285
[DDPG] Episode 30 Step 16 Reward -5.80345 Time 37.07779
[DDPG] Episode 30 Step 17 Reward -5.80511 Time 35.21708
[DDPG] Episode 30 Step 18 Reward -5.80765 Time 40.07952
[DDPG] Quadruped Not Upright
[DDPG] Episode 30 Step 19 Reward -15.73272 Time 35.83565
[DDPG] Starting Next Episode
[DDPG] Critic Loss 83.79167175292969
[DDPG] Total Reward -129.47699 Avg Critic Loss 83.79167 Time 18.60463
[DDPG] Resetting Environment
[DDPG] Starting Episode 31
[DDPG] Quadruped Not Upright
[DDPG] Episode 31 Step 0 Reward -19.78605 Time 32.85047
[DDPG] Starting Next Episode
[DDPG] Critic Loss 78.64385223388672
[DDPG] Total Reward -19.78605 Avg Critic Loss 78.64385 Time 14.11171
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 32
[DDPG] Quadruped Not Upright
[DDPG] Episode 32 Step 0 Reward -6.44414 Time 35.39511
[DDPG] Starting Next Episode
[DDPG] Critic Loss 79.87118530273438
[DDPG] Total Reward -6.44414 Avg Critic Loss 79.87119 Time 16.11521
[DDPG] Resetting Environment
[DDPG] Starting Episode 33
[DDPG] Episode 33 Step 0 Reward -12.26524 Time 35.25771
[DDPG] Episode 33 Step 1 Reward -8.13838 Time 34.56282
[DDPG] Episode 33 Step 2 Reward -6.31639 Time 35.35787
[DDPG] Episode 33 Step 3 Reward -6.75238 Time 38.44549
[DDPG] Episode 33 Step 4 Reward -7.28578 Time 35.92201
[DDPG] Episode 33 Step 5 Reward -7.74260 Time 36.02000
[DDPG] Quadruped Not Upright
[DDPG] Episode 33 Step 6 Reward -20.53081 Time 32.98681
[DDPG] Starting Next Episode
[DDPG] Critic Loss 80.27642822265625
[DDPG] Total Reward -69.03157 Avg Critic Loss 80.27643 Time 14.82785
[DDPG] Resetting Environment
[DDPG] Starting Episode 34
[DDPG] Episode 34 Step 0 Reward -10.52525 Time 32.73901
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss 78.91667938232422
[DDPG] Total Reward -10.52525 Avg Critic Loss 78.91668 Time 14.21498
[DDPG] Resetting Environment
[DDPG] Starting Episode 35
[DDPG] Episode 35 Step 0 Reward -7.73075 Time 37.41943
[DDPG] Episode 35 Step 1 Reward -6.03823 Time 35.98796
[DDPG] Episode 35 Step 2 Reward -6.47635 Time 36.50929
[DDPG] Episode 35 Step 3 Reward -6.92564 Time 38.20318
[DDPG] Episode 35 Step 4 Reward -7.47724 Time 39.11294
[DDPG] Episode 35 Step 5 Reward -7.63530 Time 35.91397
[DDPG] Episode 35 Step 6 Reward -7.94632 Time 35.16002
[DDPG] Episode 35 Step 7 Reward -5.73544 Time 34.00462
[DDPG] Episode 35 Step 8 Reward -5.73630 Time 37.48635
[DDPG] Episode 35 Step 9 Reward -5.77209 Time 38.20538
[DDPG] Episode 35 Step 10 Reward -5.84225 Time 35.31210
[DDPG] Episode 35 Step 11 Reward -5.94020 Time 38.54314
[DDPG] Episode 35 Step 12 Reward -5.99472 Time 37.15716
[DDPG] Episode 35 Step 13 Reward -6.02708 Time 35.17531
[DDPG] Episode 35 Step 14 Reward -6.00982 Time 38.40448
[DDPG] Episode 35 Step 15 Reward -6.10062 Time 35.68861
[DDPG] Quadruped Not Upright
[DDPG] Episode 35 Step 16 Reward -16.16892 Time 38.83270
[DDPG] Starting Next Episode
[DDPG] Critic Loss 76.95548248291016
[DDPG] Total Reward -119.55727 Avg Critic Loss 76.95548 Time 15.42609
[DDPG] Resetting Environment
[DDPG] Starting Episode 36
[DDPG] Quadruped Not Upright
[DDPG] Episode 36 Step 0 Reward -15.17116 Time 34.22401
[DDPG] Starting Next Episode
[DDPG] Critic Loss 75.47014617919922
[DDPG] Total Reward -15.17116 Avg Critic Loss 75.47015 Time 13.35369
[DDPG] Resetting Environment
[DDPG] Starting Episode 37
[DDPG] Quadruped Not Upright
[DDPG] Episode 37 Step 0 Reward -16.66450 Time 34.78397
[DDPG] Starting Next Episode
[DDPG] Critic Loss 75.20854949951172
[DDPG] Total Reward -16.66450 Avg Critic Loss 75.20855 Time 16.03501
[DDPG] Resetting Environment
[DDPG] Starting Episode 38
[DDPG] Quadruped Not Upright
[DDPG] Episode 38 Step 0 Reward -16.60817 Time 33.54040
[DDPG] Starting Next Episode
[DDPG] Critic Loss 75.31016540527344
[DDPG] Total Reward -16.60817 Avg Critic Loss 75.31017 Time 15.93179
[DDPG] Resetting Environment
[DDPG] Starting Episode 39
[DDPG] Quadruped Not Upright
[DDPG] Episode 39 Step 0 Reward -16.52186 Time 35.95761
[DDPG] Starting Next Episode
[DDPG] Critic Loss 76.30732727050781
[DDPG] Total Reward -16.52186 Avg Critic Loss 76.30733 Time 16.31036
[DDPG] Resetting Environment
[DDPG] Starting Episode 40
[DDPG] Episode 40 Step 0 Reward -6.55418 Time 38.07952
[DDPG] Episode 40 Step 1 Reward -5.59161 Time 40.69750
[DDPG] Episode 40 Step 2 Reward -5.69529 Time 39.94473
[DDPG] Episode 40 Step 3 Reward -5.92705 Time 40.52886
[DDPG] Episode 40 Step 4 Reward -5.87158 Time 41.08874
[DDPG] Episode 40 Step 5 Reward -5.88292 Time 41.66282
[DDPG] Episode 40 Step 6 Reward -5.89887 Time 41.15664
[DDPG] Episode 40 Step 7 Reward -5.92725 Time 39.42644
[DDPG] Episode 40 Step 8 Reward -6.04945 Time 36.42098
[DDPG] Quadruped Not Upright
[DDPG] Episode 40 Step 9 Reward -17.09043 Time 35.35551
[DDPG] Starting Next Episode
[DDPG] Critic Loss 78.3730239868164
[DDPG] Total Reward -70.48863 Avg Critic Loss 78.37302 Time 15.64705
[DDPG] Resetting Environment
[DDPG] Starting Episode 41
[DDPG] Quadruped Not Upright
[DDPG] Episode 41 Step 0 Reward -19.38548 Time 35.64168
[DDPG] Starting Next Episode
[DDPG] Critic Loss 78.13937377929688
[DDPG] Total Reward -19.38548 Avg Critic Loss 78.13937 Time 17.15300
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 42
[DDPG] Episode 42 Step 0 Reward 3.91172 Time 35.76003
[DDPG] Episode 42 Step 1 Reward -5.42765 Time 37.76276
[DDPG] Episode 42 Step 2 Reward -5.40247 Time 36.34310
[DDPG] Episode 42 Step 3 Reward -5.36696 Time 38.04782
[DDPG] Episode 42 Step 4 Reward -5.25613 Time 36.58711
[DDPG] Episode 42 Step 5 Reward -5.45462 Time 40.73375
[DDPG] Episode 42 Step 6 Reward -5.29543 Time 38.15618
[DDPG] Episode 42 Step 7 Reward -5.64208 Time 38.51020
[DDPG] Episode 42 Step 8 Reward -5.82750 Time 34.69761
[DDPG] Episode 42 Step 9 Reward -5.91980 Time 36.77837
[DDPG] Episode 42 Step 10 Reward -6.07549 Time 38.13896
[DDPG] Episode 42 Step 11 Reward -6.16080 Time 37.07330
[DDPG] Episode 42 Step 12 Reward -6.36343 Time 38.03337
[DDPG] Episode 42 Step 13 Reward -6.49922 Time 38.34565
[DDPG] Episode 42 Step 14 Reward -6.30024 Time 37.75849
[DDPG] Episode 42 Step 15 Reward -6.48230 Time 36.46858
[DDPG] Episode 42 Step 16 Reward -6.34624 Time 38.26815
[DDPG] Episode 42 Step 17 Reward -6.04057 Time 37.52328
[DDPG] Episode 42 Step 18 Reward -5.88327 Time 38.09877
[DDPG] Episode 42 Step 19 Reward -5.70126 Time 36.94959
[DDPG] Episode 42 Step 20 Reward -5.72486 Time 38.29905
[DDPG] Episode 42 Step 21 Reward -5.60499 Time 36.14912
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss 78.23992919921875
[DDPG] Total Reward -118.86360 Avg Critic Loss 78.23993 Time 15.35695
[DDPG] Resetting Environment
[DDPG] Starting Episode 43
[DDPG] Quadruped Not Upright
[DDPG] Episode 43 Step 0 Reward -16.26172 Time 34.02644
[DDPG] Starting Next Episode
[DDPG] Critic Loss 76.56591796875
[DDPG] Total Reward -16.26172 Avg Critic Loss 76.56592 Time 14.92115
[DDPG] Resetting Environment
[DDPG] Starting Episode 44
[DDPG] Episode 44 Step 0 Reward -7.09315 Time 41.07484
[DDPG] Episode 44 Step 1 Reward -5.05549 Time 40.76196
[DDPG] Episode 44 Step 2 Reward -5.04935 Time 39.08511
[DDPG] Episode 44 Step 3 Reward -5.04707 Time 39.13492
[DDPG] Episode 44 Step 4 Reward -5.05234 Time 39.70381
[DDPG] Episode 44 Step 5 Reward -5.05558 Time 42.21518
[DDPG] Episode 44 Step 6 Reward -5.04344 Time 41.74285
[DDPG] Episode 44 Step 7 Reward -5.05194 Time 39.26086
[DDPG] Episode 44 Step 8 Reward -5.04977 Time 39.40420
[DDPG] Episode 44 Step 9 Reward -5.97677 Time 39.78022
[DDPG] Episode 44 Step 10 Reward -6.16328 Time 43.22252
[DDPG] Episode 44 Step 11 Reward -6.12674 Time 44.61539
[DDPG] Episode 44 Step 12 Reward -6.00886 Time 40.04754
[DDPG] Episode 44 Step 13 Reward -5.85549 Time 38.69734
[DDPG] Episode 44 Step 14 Reward -8.56614 Time 36.16793
[DDPG] Episode 44 Step 15 Reward -5.93563 Time 38.02809
[DDPG] Episode 44 Step 16 Reward -5.95495 Time 35.91351
[DDPG] Episode 44 Step 17 Reward -5.90846 Time 39.10293
[DDPG] Episode 44 Step 18 Reward -5.92344 Time 38.96309
[DDPG] Episode 44 Step 19 Reward -5.86945 Time 37.72741
[DDPG] Episode 44 Step 20 Reward -5.83937 Time 38.23282
[DDPG] Episode 44 Step 21 Reward -5.84576 Time 36.55015
[DDPG] Episode 44 Step 22 Reward -5.79627 Time 39.62697
[DDPG] Episode 44 Step 23 Reward -5.69287 Time 35.61173
[DDPG] Episode 44 Step 24 Reward -5.71145 Time 37.25546
[DDPG] Quadruped Not Upright
[DDPG] Episode 44 Step 25 Reward -16.69912 Time 36.04802
[DDPG] Starting Next Episode
[DDPG] Critic Loss 68.70523071289062
[DDPG] Total Reward -161.37221 Avg Critic Loss 68.70523 Time 15.65423
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 45
[DDPG] Quadruped Not Upright
[DDPG] Episode 45 Step 0 Reward -10.19588 Time 33.06544
[DDPG] Starting Next Episode
[DDPG] Critic Loss 65.83784484863281
[DDPG] Total Reward -10.19588 Avg Critic Loss 65.83784 Time 15.48992
[DDPG] Resetting Environment
[DDPG] Starting Episode 46
[DDPG] Episode 46 Step 0 Reward -3.94258 Time 37.19150
[DDPG] Episode 46 Step 1 Reward -5.04626 Time 38.37487
[DDPG] Episode 46 Step 2 Reward -5.05278 Time 38.35337
[DDPG] Episode 46 Step 3 Reward -5.06109 Time 38.03300
[DDPG] Episode 46 Step 4 Reward -5.06483 Time 39.59118
[DDPG] Episode 46 Step 5 Reward -5.07252 Time 38.21605
[DDPG] Episode 46 Step 6 Reward -5.07395 Time 36.90080
[DDPG] Episode 46 Step 7 Reward -5.07631 Time 38.98286
[DDPG] Episode 46 Step 8 Reward -5.08261 Time 36.78634
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss 65.3633804321289
[DDPG] Total Reward -44.47291 Avg Critic Loss 65.36338 Time 13.81640
[DDPG] Resetting Environment
[DDPG] Starting Episode 47
[DDPG] Episode 47 Step 0 Reward -3.09446 Time 36.40384
[DDPG] Episode 47 Step 1 Reward -4.95522 Time 39.42096
[DDPG] Episode 47 Step 2 Reward -4.95661 Time 37.65645
[DDPG] Episode 47 Step 3 Reward -4.95991 Time 37.02290
[DDPG] Episode 47 Step 4 Reward -4.96322 Time 39.51447
[DDPG] Episode 47 Step 5 Reward -4.96638 Time 36.96382
[DDPG] Episode 47 Step 6 Reward -4.96848 Time 37.29932
[DDPG] Episode 47 Step 7 Reward -5.92662 Time 35.86088
[DDPG] Episode 47 Step 8 Reward -4.97568 Time 41.06440
[DDPG] Episode 47 Step 9 Reward -4.98749 Time 37.92908
[DDPG] Episode 47 Step 10 Reward -4.99117 Time 38.63993
[DDPG] Episode 47 Step 11 Reward -4.99490 Time 37.19211
[DDPG] Episode 47 Step 12 Reward -5.00233 Time 37.42133
[DDPG] Episode 47 Step 13 Reward -5.00887 Time 39.05807
[DDPG] Episode 47 Step 14 Reward -5.01386 Time 39.93940
[DDPG] Episode 47 Step 15 Reward -5.02056 Time 38.31136
[DDPG] Quadruped Not Upright
[DDPG] Episode 47 Step 16 Reward -15.15430 Time 32.72004
[DDPG] Starting Next Episode
[DDPG] Critic Loss 61.71315383911133
[DDPG] Total Reward -93.94006 Avg Critic Loss 61.71315 Time 14.06638
[DDPG] Resetting Environment
[DDPG] Starting Episode 48
[DDPG] Episode 48 Step 0 Reward -2.86311 Time 35.91092
[DDPG] Episode 48 Step 1 Reward -5.04305 Time 36.52067
[DDPG] Episode 48 Step 2 Reward -5.05164 Time 38.24384
[DDPG] Episode 48 Step 3 Reward -5.05943 Time 38.30289
[DDPG] Episode 48 Step 4 Reward -5.06429 Time 38.18763
[DDPG] Episode 48 Step 5 Reward -5.06313 Time 35.79749
[DDPG] Episode 48 Step 6 Reward -5.07144 Time 38.36015
[DDPG] Quadruped Not Upright
[DDPG] Episode 48 Step 7 Reward -16.42414 Time 35.50214
[DDPG] Starting Next Episode
[DDPG] Critic Loss 65.23957824707031
[DDPG] Total Reward -49.64023 Avg Critic Loss 65.23958 Time 15.64411
[DDPG] Resetting Environment
[DDPG] Starting Episode 49
[DDPG] Quadruped Not Upright
[DDPG] Episode 49 Step 0 Reward -11.16961 Time 34.53313
[DDPG] Starting Next Episode
[DDPG] Critic Loss 64.8635025024414
[DDPG] Total Reward -11.16961 Avg Critic Loss 64.86350 Time 15.81248
[DDPG] Resetting Environment
[DDPG] Starting Episode 50
[DDPG] Quadruped Not Upright
[DDPG] Episode 50 Step 0 Reward -12.96954 Time 35.45400
[DDPG] Starting Next Episode
[DDPG] Critic Loss 64.48527526855469
[DDPG] Total Reward -12.96954 Avg Critic Loss 64.48528 Time 13.37449
[DDPG] Saving Data
[DDPG] Saving Model
[DDPG] Resetting Environment
[DDPG] Starting Episode 51
[DDPG] Episode 51 Step 0 Reward -3.04686 Time 35.65809
[DDPG] Episode 51 Step 1 Reward -5.07014 Time 36.92780
[DDPG] Episode 51 Step 2 Reward -5.07800 Time 37.46449
[DDPG] Episode 51 Step 3 Reward -5.08287 Time 34.33932
[DDPG] Episode 51 Step 4 Reward -5.08806 Time 39.21262
[DDPG] Episode 51 Step 5 Reward -5.08708 Time 38.38835
[DDPG] Episode 51 Step 6 Reward -5.08962 Time 35.55631
[DDPG] Episode 51 Step 7 Reward -5.09528 Time 38.98637
[DDPG] Quadruped Not Upright
[DDPG] Episode 51 Step 8 Reward -16.68292 Time 33.99572
[DDPG] Starting Next Episode
[DDPG] Critic Loss 66.9375
[DDPG] Total Reward -55.32084 Avg Critic Loss 66.93750 Time 17.39250
[DDPG] Resetting Environment
[DDPG] Starting Episode 52
[DDPG] Episode 52 Step 0 Reward -1.17476 Time 37.68983
[DDPG] Episode 52 Step 1 Reward -4.96081 Time 38.13718
[DDPG] Episode 52 Step 2 Reward -4.95760 Time 39.85125
[DDPG] Episode 52 Step 3 Reward -4.96319 Time 37.87468
[DDPG] Episode 52 Step 4 Reward -4.96834 Time 38.71694
[DDPG] Episode 52 Step 5 Reward -4.96622 Time 36.36576
[DDPG] Episode 52 Step 6 Reward -4.97102 Time 39.48431
[DDPG] Quadruped Not Upright
[DDPG] Episode 52 Step 7 Reward -16.23440 Time 38.05524
[DDPG] Starting Next Episode
[DDPG] Critic Loss 56.19562530517578
[DDPG] Total Reward -47.19634 Avg Critic Loss 56.19563 Time 14.54373
[DDPG] Resetting Environment
[DDPG] Starting Episode 53
[DDPG] Quadruped Not Upright
[DDPG] Episode 53 Step 0 Reward -11.95825 Time 33.55145
[DDPG] Starting Next Episode
[DDPG] Critic Loss 68.19917297363281
[DDPG] Total Reward -11.95825 Avg Critic Loss 68.19917 Time 14.95645
[DDPG] Resetting Environment
[DDPG] Starting Episode 54
[DDPG] Episode 54 Step 0 Reward -3.58552 Time 38.54754
[DDPG] Episode 54 Step 1 Reward -5.06484 Time 39.44619
[DDPG] Episode 54 Step 2 Reward -5.07838 Time 40.57533
[DDPG] Episode 54 Step 3 Reward -5.07855 Time 39.43664
[DDPG] Episode 54 Step 4 Reward -5.07850 Time 39.95765
[DDPG] Episode 54 Step 5 Reward -5.08380 Time 39.33385
[DDPG] Episode 54 Step 6 Reward -5.08372 Time 37.01199
[DDPG] Quadruped Not Upright
[DDPG] Episode 54 Step 7 Reward -16.64097 Time 35.73531
[DDPG] Starting Next Episode
[DDPG] Critic Loss 64.81228637695312
[DDPG] Total Reward -50.69428 Avg Critic Loss 64.81229 Time 17.09559
[DDPG] Resetting Environment
[DDPG] Starting Episode 55
[DDPG] Quadruped Not Upright
[DDPG] Episode 55 Step 0 Reward -11.91575 Time 37.55251
[DDPG] Starting Next Episode
[DDPG] Critic Loss 63.19001388549805
[DDPG] Total Reward -11.91575 Avg Critic Loss 63.19001 Time 18.20725
[DDPG] Resetting Environment
[DDPG] Starting Episode 56
[DDPG] Episode 56 Step 0 Reward -3.33263 Time 33.39808
[DDPG] Episode 56 Step 1 Reward -5.07383 Time 36.34553
[DDPG] Episode 56 Step 2 Reward -5.07622 Time 37.95419
[DDPG] Episode 56 Step 3 Reward -5.07613 Time 37.48907
[DDPG] Episode 56 Step 4 Reward -5.08500 Time 41.06419
[DDPG] Episode 56 Step 5 Reward -5.08624 Time 39.15530
[DDPG] Episode 56 Step 6 Reward -5.09145 Time 39.91586
[DDPG] Quadruped Not Upright
[DDPG] Episode 56 Step 7 Reward -17.45575 Time 39.67167
[DDPG] Starting Next Episode
[DDPG] Critic Loss 57.128326416015625
[DDPG] Total Reward -51.27725 Avg Critic Loss 57.12833 Time 14.67992
[DDPG] Resetting Environment
[DDPG] Starting Episode 57
[DDPG] Episode 57 Step 0 Reward -1.36496 Time 38.89118
[DDPG] Episode 57 Step 1 Reward -4.94931 Time 38.49621
[DDPG] Episode 57 Step 2 Reward -4.95358 Time 38.63025
[DDPG] Episode 57 Step 3 Reward -4.95581 Time 36.48607
[DDPG] Episode 57 Step 4 Reward -4.95663 Time 42.70493
[DDPG] Episode 57 Step 5 Reward -4.95834 Time 40.58446
[DDPG] Quadruped Not Upright
[DDPG] Episode 57 Step 6 Reward -15.05088 Time 38.71355
[DDPG] Starting Next Episode
[DDPG] Critic Loss 56.827423095703125
[DDPG] Total Reward -41.18952 Avg Critic Loss 56.82742 Time 16.05396
[DDPG] Resetting Environment
[DDPG] Starting Episode 58
[DDPG] Episode 58 Step 0 Reward -2.04385 Time 35.49322
[DDPG] Episode 58 Step 1 Reward -5.08388 Time 38.92419
[DDPG] Episode 58 Step 2 Reward -5.10994 Time 37.10333
[DDPG] Episode 58 Step 3 Reward -5.12066 Time 41.79560
[DDPG] Episode 58 Step 4 Reward -5.14075 Time 42.22301
[DDPG] Episode 58 Step 5 Reward -5.14328 Time 38.65994
[DDPG] Episode 58 Step 6 Reward -5.14955 Time 38.14307
[DDPG] Quadruped Not Upright
[DDPG] Episode 58 Step 7 Reward -16.05240 Time 37.02723
[DDPG] Starting Next Episode
[DDPG] Critic Loss 53.813072204589844
[DDPG] Total Reward -48.84430 Avg Critic Loss 53.81307 Time 16.56023
[DDPG] Resetting Environment
[DDPG] Starting Episode 59
[DDPG] Episode 59 Step 0 Reward -1.76084 Time 38.44842
[DDPG] Episode 59 Step 1 Reward -4.94654 Time 41.17518
[DDPG] Episode 59 Step 2 Reward -5.39403 Time 38.83914
[DDPG] Episode 59 Step 3 Reward -4.95048 Time 41.98742
[DDPG] Episode 59 Step 4 Reward -4.95812 Time 37.31490
[DDPG] Episode 59 Step 5 Reward -4.96168 Time 38.92182
[DDPG] Episode 59 Step 6 Reward -4.96313 Time 39.41292
[DDPG] Episode 59 Step 7 Reward -4.96611 Time 37.07620
[DDPG] Episode 59 Step 8 Reward -4.97430 Time 38.22038
[DDPG] Episode 59 Step 9 Reward -4.97165 Time 40.67799
[DDPG] Episode 59 Step 10 Reward -4.97102 Time 39.29808
[DDPG] Episode 59 Step 11 Reward -4.97540 Time 40.20906
[DDPG] Episode 59 Step 12 Reward -4.98350 Time 39.64288
[DDPG] Episode 59 Step 13 Reward -4.98835 Time 39.99932
[DDPG] Episode 59 Step 14 Reward -4.98673 Time 40.61360
[DDPG] Episode 59 Step 15 Reward -4.99625 Time 41.36325
[DDPG] Episode 59 Step 16 Reward -5.00256 Time 37.96670
[DDPG] Episode 59 Step 17 Reward -5.00744 Time 38.03244
[DDPG] Episode 59 Step 18 Reward -5.01625 Time 40.40449
[DDPG] Episode 59 Step 19 Reward -5.02634 Time 36.31820
[DDPG] Episode 59 Step 20 Reward -5.03302 Time 40.29312
[DDPG] Episode 59 Step 21 Reward -5.04038 Time 45.47846
[DDPG] Episode 59 Step 22 Reward -5.04572 Time 41.42586
[DDPG] Quadruped Not Upright
[DDPG] Episode 59 Step 23 Reward -15.03728 Time 36.68918
[DDPG] Starting Next Episode
[DDPG] Critic Loss 53.10986328125
[DDPG] Total Reward -126.95713 Avg Critic Loss 53.10986 Time 15.17680
[DDPG] Resetting Environment
[DDPG] Starting Episode 60
[DDPG] Quadruped Not Upright
[DDPG] Episode 60 Step 0 Reward -16.71928 Time 35.44990
[DDPG] Starting Next Episode
[DDPG] Critic Loss 56.83842468261719
[DDPG] Total Reward -16.71928 Avg Critic Loss 56.83842 Time 16.03807
[DDPG] Resetting Environment
[DDPG] Starting Episode 61
[DDPG] Quadruped Not Upright
[DDPG] Episode 61 Step 0 Reward -17.08094 Time 34.76603
[DDPG] Starting Next Episode
[DDPG] Critic Loss 59.338932037353516
[DDPG] Total Reward -17.08094 Avg Critic Loss 59.33893 Time 15.70558
[DDPG] Resetting Environment
[DDPG] Starting Episode 62
[DDPG] Quadruped Not Upright
[DDPG] Episode 62 Step 0 Reward -18.10679 Time 35.49618
[DDPG] Starting Next Episode
[DDPG] Critic Loss 53.38645553588867
[DDPG] Total Reward -18.10679 Avg Critic Loss 53.38646 Time 16.76426
[DDPG] Resetting Environment
[DDPG] Starting Episode 63
[DDPG] Quadruped Not Upright
[DDPG] Episode 63 Step 0 Reward -17.37686 Time 35.36094
[DDPG] Starting Next Episode
[DDPG] Critic Loss 57.30620574951172
[DDPG] Total Reward -17.37686 Avg Critic Loss 57.30621 Time 15.84719
[DDPG] Resetting Environment
[DDPG] Starting Episode 64
[DDPG] Quadruped Not Upright
[DDPG] Episode 64 Step 0 Reward -18.03041 Time 34.75302
[DDPG] Starting Next Episode
[DDPG] Critic Loss 55.891990661621094
[DDPG] Total Reward -18.03041 Avg Critic Loss 55.89199 Time 16.32896
[DDPG] Resetting Environment
[DDPG] Starting Episode 65
[DDPG] Quadruped Not Upright
[DDPG] Episode 65 Step 0 Reward -17.41744 Time 34.63164
[DDPG] Starting Next Episode
[DDPG] Critic Loss 56.5534553527832
[DDPG] Total Reward -17.41744 Avg Critic Loss 56.55346 Time 12.41638
[DDPG] Resetting Environment
[DDPG] Starting Episode 66
[DDPG] Quadruped Not Upright
[DDPG] Episode 66 Step 0 Reward -17.91203 Time 34.51414
[DDPG] Starting Next Episode
[DDPG] Critic Loss 59.692901611328125
[DDPG] Total Reward -17.91203 Avg Critic Loss 59.69290 Time 13.11581
[DDPG] Resetting Environment
[DDPG] Starting Episode 67
[DDPG] Quadruped Not Upright
[DDPG] Episode 67 Step 0 Reward -17.55999 Time 33.09629
[DDPG] Starting Next Episode
[DDPG] Critic Loss 61.89167022705078
[DDPG] Total Reward -17.55999 Avg Critic Loss 61.89167 Time 12.70751
[DDPG] Resetting Environment
[DDPG] Starting Episode 68
[DDPG] Quadruped Not Upright
[DDPG] Episode 68 Step 0 Reward -17.70342 Time 34.16719
[DDPG] Starting Next Episode
[DDPG] Critic Loss 55.30339431762695
[DDPG] Total Reward -17.70342 Avg Critic Loss 55.30339 Time 16.63958
[DDPG] Resetting Environment
[DDPG] Starting Episode 69
[DDPG] Quadruped Not Upright
[DDPG] Episode 69 Step 0 Reward -17.01531 Time 33.11392
[DDPG] Starting Next Episode
[DDPG] Critic Loss 59.493499755859375
[DDPG] Total Reward -17.01531 Avg Critic Loss 59.49350 Time 16.59371
[DDPG] Resetting Environment
[DDPG] Starting Episode 70
[DDPG] Quadruped Not Upright
[DDPG] Episode 70 Step 0 Reward -17.85846 Time 35.21239
[DDPG] Starting Next Episode
[DDPG] Critic Loss 55.06145477294922
[DDPG] Total Reward -17.85846 Avg Critic Loss 55.06145 Time 15.97156
[DDPG] Resetting Environment
[DDPG] Starting Episode 71
[DDPG] Quadruped Not Upright
[DDPG] Episode 71 Step 0 Reward -17.48439 Time 32.68053
[DDPG] Starting Next Episode
[DDPG] Critic Loss 59.18561935424805
[DDPG] Total Reward -17.48439 Avg Critic Loss 59.18562 Time 15.06963
[DDPG] Resetting Environment
[DDPG] Starting Episode 72
[DDPG] Quadruped Not Upright
[DDPG] Episode 72 Step 0 Reward -17.88739 Time 34.62979
[DDPG] Starting Next Episode
[DDPG] Critic Loss 70.06622314453125
[DDPG] Total Reward -17.88739 Avg Critic Loss 70.06622 Time 15.14975
[DDPG] Resetting Environment
[DDPG] Starting Episode 73
[DDPG] Quadruped Not Upright
[DDPG] Episode 73 Step 0 Reward -18.00035 Time 36.88781
[DDPG] Starting Next Episode
[DDPG] Critic Loss 72.31706237792969
[DDPG] Total Reward -18.00035 Avg Critic Loss 72.31706 Time 16.46073
[DDPG] Resetting Environment
[DDPG] Starting Episode 74
[DDPG] Quadruped Not Upright
[DDPG] Episode 74 Step 0 Reward -19.05923 Time 34.78748
[DDPG] Starting Next Episode
[DDPG] Critic Loss 68.59819030761719
[DDPG] Total Reward -19.05923 Avg Critic Loss 68.59819 Time 17.24324
[DDPG] Resetting Environment
[DDPG] Starting Episode 75
[DDPG] Quadruped Not Upright
[DDPG] Episode 75 Step 0 Reward -18.00915 Time 35.11009
[DDPG] Starting Next Episode
[DDPG] Critic Loss 71.69213104248047
[DDPG] Total Reward -18.00915 Avg Critic Loss 71.69213 Time 15.91343
[DDPG] Saving Data
[DDPG] Saving Model
[DDPG] Resetting Environment
[DDPG] Starting Episode 76
[DDPG] Quadruped Not Upright
[DDPG] Episode 76 Step 0 Reward -17.90581 Time 34.85641
[DDPG] Starting Next Episode
[DDPG] Critic Loss 68.57257080078125
[DDPG] Total Reward -17.90581 Avg Critic Loss 68.57257 Time 15.54648
[DDPG] Resetting Environment
[DDPG] Starting Episode 77
[DDPG] Quadruped Not Upright
[DDPG] Episode 77 Step 0 Reward -17.67150 Time 33.86546
[DDPG] Starting Next Episode
[DDPG] Critic Loss 64.79199981689453
[DDPG] Total Reward -17.67150 Avg Critic Loss 64.79200 Time 15.07645
[DDPG] Resetting Environment
[DDPG] Starting Episode 78
[DDPG] Quadruped Not Upright
[DDPG] Episode 78 Step 0 Reward -17.68381 Time 34.44177
[DDPG] Starting Next Episode
[DDPG] Critic Loss 61.84305191040039
[DDPG] Total Reward -17.68381 Avg Critic Loss 61.84305 Time 11.18832
[DDPG] Resetting Environment
[DDPG] Starting Episode 79
[DDPG] Quadruped Not Upright
[DDPG] Episode 79 Step 0 Reward -17.51628 Time 34.14309
[DDPG] Starting Next Episode
[DDPG] Critic Loss 58.789005279541016
[DDPG] Total Reward -17.51628 Avg Critic Loss 58.78901 Time 15.08602
[DDPG] Resetting Environment
[DDPG] Starting Episode 80
[DDPG] Quadruped Not Upright
[DDPG] Episode 80 Step 0 Reward -18.42113 Time 31.97960
[DDPG] Starting Next Episode
[DDPG] Critic Loss 56.512393951416016
[DDPG] Total Reward -18.42113 Avg Critic Loss 56.51239 Time 17.27570
[DDPG] Resetting Environment
[DDPG] Starting Episode 81
[DDPG] Quadruped Not Upright
[DDPG] Episode 81 Step 0 Reward -17.60647 Time 35.40396
[DDPG] Starting Next Episode
[DDPG] Critic Loss 67.6036605834961
[DDPG] Total Reward -17.60647 Avg Critic Loss 67.60366 Time 16.64192
[DDPG] Resetting Environment
[DDPG] Starting Episode 82
[DDPG] Quadruped Not Upright
[DDPG] Episode 82 Step 0 Reward -17.29401 Time 35.02332
[DDPG] Starting Next Episode
[DDPG] Critic Loss 70.53192901611328
[DDPG] Total Reward -17.29401 Avg Critic Loss 70.53193 Time 14.26286
[DDPG] Resetting Environment
[DDPG] Starting Episode 83
[DDPG] Quadruped Not Upright
[DDPG] Episode 83 Step 0 Reward -18.43533 Time 35.41140
[DDPG] Starting Next Episode
[DDPG] Critic Loss 69.92459869384766
[DDPG] Total Reward -18.43533 Avg Critic Loss 69.92460 Time 15.96715
[DDPG] Resetting Environment
[DDPG] Starting Episode 84
[DDPG] Quadruped Not Upright
[DDPG] Episode 84 Step 0 Reward -18.42685 Time 33.64968
[DDPG] Starting Next Episode
[DDPG] Critic Loss 68.78763580322266
[DDPG] Total Reward -18.42685 Avg Critic Loss 68.78764 Time 13.78604
[DDPG] Resetting Environment
[DDPG] Starting Episode 85
[DDPG] Quadruped Not Upright
[DDPG] Episode 85 Step 0 Reward -17.99132 Time 35.67612
[DDPG] Starting Next Episode
[DDPG] Critic Loss 70.43022155761719
[DDPG] Total Reward -17.99132 Avg Critic Loss 70.43022 Time 16.26595
[DDPG] Resetting Environment
[DDPG] Starting Episode 86
[DDPG] Quadruped Not Upright
[DDPG] Episode 86 Step 0 Reward -17.99615 Time 33.51668
[DDPG] Starting Next Episode
[DDPG] Critic Loss 70.4480972290039
[DDPG] Total Reward -17.99615 Avg Critic Loss 70.44810 Time 17.65458
[DDPG] Resetting Environment
[DDPG] Starting Episode 87
[DDPG] Quadruped Not Upright
[DDPG] Episode 87 Step 0 Reward -17.50726 Time 34.78776
[DDPG] Starting Next Episode
[DDPG] Critic Loss 69.51165771484375
[DDPG] Total Reward -17.50726 Avg Critic Loss 69.51166 Time 15.84394
[DDPG] Resetting Environment
[DDPG] Starting Episode 88
[DDPG] Quadruped Not Upright
[DDPG] Episode 88 Step 0 Reward -17.51662 Time 34.67338
[DDPG] Starting Next Episode
[DDPG] Critic Loss 71.16169738769531
[DDPG] Total Reward -17.51662 Avg Critic Loss 71.16170 Time 15.10139
[DDPG] Resetting Environment
[DDPG] Starting Episode 89
[DDPG] Quadruped Not Upright
[DDPG] Episode 89 Step 0 Reward -16.81540 Time 37.56852
[DDPG] Starting Next Episode
[DDPG] Critic Loss 76.5107192993164
[DDPG] Total Reward -16.81540 Avg Critic Loss 76.51072 Time 14.57113
[DDPG] Resetting Environment
[DDPG] Starting Episode 90
[DDPG] Quadruped Not Upright
[DDPG] Episode 90 Step 0 Reward -18.17389 Time 34.79952
[DDPG] Starting Next Episode
[DDPG] Critic Loss 77.82904815673828
[DDPG] Total Reward -18.17389 Avg Critic Loss 77.82905 Time 15.68285
[DDPG] Resetting Environment
[DDPG] Starting Episode 91
[DDPG] Quadruped Not Upright
[DDPG] Episode 91 Step 0 Reward -17.36642 Time 33.17503
[DDPG] Starting Next Episode
[DDPG] Critic Loss 74.9749984741211
[DDPG] Total Reward -17.36642 Avg Critic Loss 74.97500 Time 15.93322
[DDPG] Resetting Environment
[DDPG] Starting Episode 92
[DDPG] Quadruped Not Upright
[DDPG] Episode 92 Step 0 Reward -17.07946 Time 31.97499
[DDPG] Starting Next Episode
[DDPG] Critic Loss 82.6282730102539
[DDPG] Total Reward -17.07946 Avg Critic Loss 82.62827 Time 12.84660
[DDPG] Resetting Environment
[DDPG] Starting Episode 93
[DDPG] Quadruped Not Upright
[DDPG] Episode 93 Step 0 Reward -17.47403 Time 33.42781
[DDPG] Starting Next Episode
[DDPG] Critic Loss 60.3532600402832
[DDPG] Total Reward -17.47403 Avg Critic Loss 60.35326 Time 17.65505
[DDPG] Resetting Environment
[DDPG] Starting Episode 94
[DDPG] Quadruped Not Upright
[DDPG] Episode 94 Step 0 Reward -17.46993 Time 31.28331
[DDPG] Starting Next Episode
[DDPG] Critic Loss 73.22032165527344
[DDPG] Total Reward -17.46993 Avg Critic Loss 73.22032 Time 16.75582
[DDPG] Resetting Environment
[DDPG] Starting Episode 95
[DDPG] Quadruped Not Upright
[DDPG] Episode 95 Step 0 Reward -17.19430 Time 35.50464
[DDPG] Starting Next Episode
[DDPG] Critic Loss 75.84089660644531
[DDPG] Total Reward -17.19430 Avg Critic Loss 75.84090 Time 14.79910
[DDPG] Resetting Environment
[DDPG] Starting Episode 96
[DDPG] Quadruped Not Upright
[DDPG] Episode 96 Step 0 Reward -18.67701 Time 35.39768
[DDPG] Starting Next Episode
[DDPG] Critic Loss 70.84417724609375
[DDPG] Total Reward -18.67701 Avg Critic Loss 70.84418 Time 16.79128
[DDPG] Resetting Environment
[DDPG] Starting Episode 97
[DDPG] Quadruped Not Upright
[DDPG] Episode 97 Step 0 Reward -17.27034 Time 35.38531
[DDPG] Starting Next Episode
[DDPG] Critic Loss 71.90680694580078
[DDPG] Total Reward -17.27034 Avg Critic Loss 71.90681 Time 17.96782
[DDPG] Resetting Environment
[DDPG] Starting Episode 98
[DDPG] Quadruped Not Upright
[DDPG] Episode 98 Step 0 Reward -18.10230 Time 35.06762
[DDPG] Starting Next Episode
[DDPG] Critic Loss 76.85609436035156
[DDPG] Total Reward -18.10230 Avg Critic Loss 76.85609 Time 14.02040
[DDPG] Resetting Environment
[DDPG] Starting Episode 99
[DDPG] Quadruped Not Upright
[DDPG] Episode 99 Step 0 Reward -17.03971 Time 36.22781
[DDPG] Starting Next Episode
[DDPG] Critic Loss 75.9984359741211
[DDPG] Total Reward -17.03971 Avg Critic Loss 75.99844 Time 18.09291
[DDPG] Resetting Environment
[DDPG] Starting Episode 100
[DDPG] Quadruped Not Upright
[DDPG] Episode 100 Step 0 Reward -17.99953 Time 34.58225
[DDPG] Starting Next Episode
[DDPG] Critic Loss 79.75456237792969
[DDPG] Total Reward -17.99953 Avg Critic Loss 79.75456 Time 15.81618
[DDPG] Saving Data
[DDPG] Saving Model
[DDPG] Resetting Environment
[DDPG] Starting Episode 101
[DDPG] Quadruped Not Upright
[DDPG] Episode 101 Step 0 Reward -17.60250 Time 35.12367
[DDPG] Starting Next Episode
[DDPG] Critic Loss 71.22594451904297
[DDPG] Total Reward -17.60250 Avg Critic Loss 71.22594 Time 16.10366
[DDPG] Resetting Environment
[DDPG] Starting Episode 102
[DDPG] Quadruped Not Upright
[DDPG] Episode 102 Step 0 Reward -17.87536 Time 35.88678
[DDPG] Starting Next Episode
[DDPG] Critic Loss 70.9228744506836
[DDPG] Total Reward -17.87536 Avg Critic Loss 70.92287 Time 16.32827
[DDPG] Resetting Environment
[DDPG] Starting Episode 103
[DDPG] Quadruped Not Upright
[DDPG] Episode 103 Step 0 Reward -17.66049 Time 35.94196
[DDPG] Starting Next Episode
[DDPG] Critic Loss 84.49225616455078
[DDPG] Total Reward -17.66049 Avg Critic Loss 84.49226 Time 16.84193
[DDPG] Resetting Environment
[DDPG] Starting Episode 104
[DDPG] Quadruped Not Upright
[DDPG] Episode 104 Step 0 Reward -18.11604 Time 33.50241
[DDPG] Starting Next Episode
[DDPG] Critic Loss 83.56002044677734
[DDPG] Total Reward -18.11604 Avg Critic Loss 83.56002 Time 16.47160
[DDPG] Resetting Environment
[DDPG] Starting Episode 105
[DDPG] Quadruped Not Upright
[DDPG] Episode 105 Step 0 Reward -18.29761 Time 37.40852
[DDPG] Starting Next Episode
[DDPG] Critic Loss 75.6651382446289
[DDPG] Total Reward -18.29761 Avg Critic Loss 75.66514 Time 18.36615
[DDPG] Resetting Environment
[DDPG] Starting Episode 106
[DDPG] Quadruped Not Upright
[DDPG] Episode 106 Step 0 Reward -17.91797 Time 36.36777
[DDPG] Starting Next Episode
[DDPG] Critic Loss 77.46946716308594
[DDPG] Total Reward -17.91797 Avg Critic Loss 77.46947 Time 16.65521
[DDPG] Resetting Environment
[DDPG] Starting Episode 107
[DDPG] Quadruped Not Upright
[DDPG] Episode 107 Step 0 Reward -17.56317 Time 33.83943
[DDPG] Starting Next Episode
[DDPG] Critic Loss 79.47760009765625
[DDPG] Total Reward -17.56317 Avg Critic Loss 79.47760 Time 15.81638
[DDPG] Resetting Environment
[DDPG] Starting Episode 108
[DDPG] Quadruped Not Upright
[DDPG] Episode 108 Step 0 Reward -17.60152 Time 37.21971
[DDPG] Starting Next Episode
[DDPG] Critic Loss 82.23497772216797
[DDPG] Total Reward -17.60152 Avg Critic Loss 82.23498 Time 16.66610
[DDPG] Resetting Environment
[DDPG] Starting Episode 109
[DDPG] Quadruped Not Upright
[DDPG] Episode 109 Step 0 Reward -17.34192 Time 36.22739
[DDPG] Starting Next Episode
[DDPG] Critic Loss 82.09370422363281
[DDPG] Total Reward -17.34192 Avg Critic Loss 82.09370 Time 18.00034
[DDPG] Resetting Environment
[DDPG] Starting Episode 110
[DDPG] Quadruped Not Upright
[DDPG] Episode 110 Step 0 Reward -17.90053 Time 33.85625
[DDPG] Starting Next Episode
[DDPG] Critic Loss 81.38207244873047
[DDPG] Total Reward -17.90053 Avg Critic Loss 81.38207 Time 14.22068
[DDPG] Resetting Environment
[DDPG] Starting Episode 111
[DDPG] Quadruped Not Upright
[DDPG] Episode 111 Step 0 Reward -18.53138 Time 35.21273
[DDPG] Starting Next Episode
[DDPG] Critic Loss 79.7471923828125
[DDPG] Total Reward -18.53138 Avg Critic Loss 79.74719 Time 15.98669
[DDPG] Resetting Environment
[DDPG] Starting Episode 112
[DDPG] Quadruped Not Upright
[DDPG] Episode 112 Step 0 Reward -18.30133 Time 37.34881
[DDPG] Starting Next Episode
[DDPG] Critic Loss 85.25505828857422
[DDPG] Total Reward -18.30133 Avg Critic Loss 85.25506 Time 15.01217
[DDPG] Resetting Environment
[DDPG] Starting Episode 113
[DDPG] Quadruped Not Upright
[DDPG] Episode 113 Step 0 Reward -18.06371 Time 33.81487
[DDPG] Starting Next Episode
[DDPG] Critic Loss 82.9421615600586
[DDPG] Total Reward -18.06371 Avg Critic Loss 82.94216 Time 14.47218
[DDPG] Resetting Environment
[DDPG] Starting Episode 114
[DDPG] Quadruped Not Upright
[DDPG] Episode 114 Step 0 Reward -17.84983 Time 35.68880
[DDPG] Starting Next Episode
[DDPG] Critic Loss 88.00204467773438
[DDPG] Total Reward -17.84983 Avg Critic Loss 88.00204 Time 14.26750
[DDPG] Resetting Environment
[DDPG] Starting Episode 115
[DDPG] Quadruped Not Upright
[DDPG] Episode 115 Step 0 Reward -17.62258 Time 35.31778
[DDPG] Starting Next Episode
[DDPG] Critic Loss 78.80663299560547
[DDPG] Total Reward -17.62258 Avg Critic Loss 78.80663 Time 14.55122
[DDPG] Resetting Environment
[DDPG] Starting Episode 116
[DDPG] Quadruped Not Upright
[DDPG] Episode 116 Step 0 Reward -18.43381 Time 36.38891
[DDPG] Starting Next Episode
[DDPG] Critic Loss 83.26750183105469
[DDPG] Total Reward -18.43381 Avg Critic Loss 83.26750 Time 17.80459
[DDPG] Resetting Environment
[DDPG] Starting Episode 117
[DDPG] Quadruped Not Upright
[DDPG] Episode 117 Step 0 Reward -17.68671 Time 36.17322
[DDPG] Starting Next Episode
[DDPG] Critic Loss 79.90936279296875
[DDPG] Total Reward -17.68671 Avg Critic Loss 79.90936 Time 11.52975
[DDPG] Resetting Environment
[DDPG] Starting Episode 118
[DDPG] Quadruped Not Upright
[DDPG] Episode 118 Step 0 Reward -18.42824 Time 36.29571
[DDPG] Starting Next Episode
[DDPG] Critic Loss 97.37841796875
[DDPG] Total Reward -18.42824 Avg Critic Loss 97.37842 Time 15.99200
[DDPG] Resetting Environment
[DDPG] Starting Episode 119
[DDPG] Quadruped Not Upright
[DDPG] Episode 119 Step 0 Reward -17.93644 Time 38.81655
[DDPG] Starting Next Episode
[DDPG] Critic Loss 89.27278137207031
[DDPG] Total Reward -17.93644 Avg Critic Loss 89.27278 Time 17.61065
[DDPG] Resetting Environment
[DDPG] Starting Episode 120
[DDPG] Quadruped Not Upright
[DDPG] Episode 120 Step 0 Reward -17.70911 Time 36.28358
[DDPG] Starting Next Episode
[DDPG] Critic Loss 87.9831771850586
[DDPG] Total Reward -17.70911 Avg Critic Loss 87.98318 Time 16.83896
[DDPG] Resetting Environment
[DDPG] Starting Episode 121
Traceback (most recent call last):
  File "learn.py", line 1381, in <module>
    learner.learn(
  File "learn.py", line 927, in learn
    self.current_time_step = self.env.step(
  File "/home/ubuntu/CPGController/src/rl/env.py", line 72, in step
    return self._step(action, desired_motion, last_step)
  File "/home/ubuntu/CPGController/src/rl/env.py", line 88, in _step
    observation =  self.quadruped.step(
  File "/home/ubuntu/CPGController/src/simulations/ws/src/quadruped/scripts/quadruped.py", line 1167, in step
    rospy.sleep(15.0/60.0)
  File "/opt/ros/noetic/lib/python3/dist-packages/rospy/timer.py", line 165, in sleep
    raise rospy.exceptions.ROSInterruptException("ROS shutdown request")
rospy.exceptions.ROSInterruptException: ROS shutdown request
2021-04-12 17:42:47.342041: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
Unable to init server: Could not connect: Connection refused
Unable to init server: Could not connect: Connection refused

(learn.py:4786): Gdk-CRITICAL **: 17:42:54.740: gdk_cursor_new_for_display: assertion 'GDK_IS_DISPLAY (display)' failed
2021-04-12 17:42:55.392783: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-04-12 17:42:55.396244: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-04-12 17:42:55.446052: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-12 17:42:55.447565: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:00:1e.0 name: Tesla T4 computeCapability: 7.5
coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s
2021-04-12 17:42:55.447711: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-04-12 17:42:55.459993: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-04-12 17:42:55.461804: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-04-12 17:42:55.465101: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-04-12 17:42:55.466256: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-04-12 17:42:55.476124: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-04-12 17:42:55.478445: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-04-12 17:42:55.478944: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-04-12 17:42:55.479548: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-12 17:42:55.481341: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-12 17:42:55.483448: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
[DDPG] Building the actor model
2021-04-12 17:42:55.514025: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-04-12 17:42:55.514496: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-04-12 17:42:55.514852: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-12 17:42:55.516033: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:00:1e.0 name: Tesla T4 computeCapability: 7.5
coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s
2021-04-12 17:42:55.516141: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-04-12 17:42:55.516427: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-04-12 17:42:55.517335: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-04-12 17:42:55.517396: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-04-12 17:42:55.517442: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-04-12 17:42:55.517488: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-04-12 17:42:55.517872: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-04-12 17:42:55.518006: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-04-12 17:42:55.518365: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-12 17:42:55.521787: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-12 17:42:55.523336: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-04-12 17:42:55.523385: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-04-12 17:42:57.185109: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-04-12 17:42:57.185593: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2021-04-12 17:42:57.185615: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2021-04-12 17:42:57.186716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-12 17:42:57.189990: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-12 17:42:57.192339: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-12 17:42:57.195266: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13217 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5)
[DDPG] Building the actor model
[DDPG] Building the critic model
[DDPG] Building the critic model
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1618249385.211585046, 4.206000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1618249385.214371514, 4.208000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1618249385.214680879, 4.208000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1618249386.794870424, 4.968000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1618249388.102716239, 5.600000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1618249389.754797681, 6.400000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1618249391.393533241, 7.200000000]: Ready to take commands for planning group back_left_leg.[0m
[Actor] GPU>>>>>>>>>>>>
[Actor] [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
[DDPG] Loading Actor Weights
[DDPG] Training Start
[DDPG] Resetting Environment
[DDPG] Starting Pretraining Test
2021-04-12 17:43:57.155937: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-04-12 17:43:58.381634: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
[DDPG] Step 0 Reward 48.57864 Time 18.63935
[DDPG] Step 1 Reward 42.09331 Time 17.13675
[DDPG] Step 2 Reward 46.97230 Time 18.45257
[DDPG] Step 3 Reward 49.95964 Time 21.24487
[DDPG] Step 4 Reward -7.32743 Time 20.67581
[DDPG] Resetting Environment
[DDPG] Starting Episode 0
[DDPG] Quadruped Not Upright
[DDPG] Episode 0 Step 0 Reward -15.66623 Time 19.44652
[DDPG] Starting Next Episode
[DDPG] Critic Loss 300.6062316894531
[DDPG] Total Reward -15.66623 Avg Critic Loss 300.60623 Time 9.62282
[DDPG] Saving Data
[DDPG] Saving Model
[DDPG] Resetting Environment
[DDPG] Starting Episode 1
[DDPG] Episode 1 Step 0 Reward -4.72538 Time 28.02785
[DDPG] Quadruped Not Upright
[DDPG] Episode 1 Step 1 Reward -21.01569 Time 28.23286
[DDPG] Starting Next Episode
[DDPG] Critic Loss 261.677490234375
[DDPG] Total Reward -25.74108 Avg Critic Loss 261.67749 Time 12.02362
[DDPG] Resetting Environment
[DDPG] Starting Episode 2
[DDPG] Episode 2 Step 0 Reward 0.12445 Time 32.38901
[DDPG] Episode 2 Step 1 Reward -8.44471 Time 31.95623
[DDPG] Quadruped Not Upright
[DDPG] Episode 2 Step 2 Reward -10.67075 Time 26.36804
[DDPG] Starting Next Episode
[DDPG] Critic Loss 156.73716735839844
[DDPG] Total Reward -18.99101 Avg Critic Loss 156.73717 Time 12.77007
[DDPG] Resetting Environment
[DDPG] Starting Episode 3
[DDPG] Episode 3 Step 0 Reward -4.58832 Time 29.73053
[DDPG] Episode 3 Step 1 Reward -7.60855 Time 32.13490
[DDPG] Episode 3 Step 2 Reward -4.31717 Time 25.01150
[DDPG] Episode 3 Step 3 Reward -3.99294 Time 27.99956
[DDPG] Episode 3 Step 4 Reward -5.41710 Time 24.69496
[DDPG] Episode 3 Step 5 Reward -4.35332 Time 25.62620
[DDPG] Episode 3 Step 6 Reward -3.98995 Time 28.41998
[DDPG] Episode 3 Step 7 Reward -5.27335 Time 30.62339
[DDPG] Episode 3 Step 8 Reward -5.42515 Time 32.48783
[DDPG] Episode 3 Step 9 Reward -6.27139 Time 35.75912
[DDPG] Episode 3 Step 10 Reward -5.86395 Time 37.71508
[DDPG] Episode 3 Step 11 Reward -5.93141 Time 37.53936
[DDPG] Episode 3 Step 12 Reward -6.11894 Time 36.30508
[DDPG] Quadruped Not Upright
[DDPG] Episode 3 Step 13 Reward -22.10790 Time 29.44057
[DDPG] Starting Next Episode
[DDPG] Critic Loss 90.38664245605469
[DDPG] Total Reward -91.25944 Avg Critic Loss 90.38664 Time 12.86345
[DDPG] Resetting Environment
[DDPG] Starting Episode 4
[DDPG] Episode 4 Step 0 Reward -6.83983 Time 37.05252
[DDPG] Episode 4 Step 1 Reward -6.62402 Time 32.82703
[DDPG] Quadruped Not Upright
[DDPG] Episode 4 Step 2 Reward -9.66712 Time 27.24366
[DDPG] Starting Next Episode
[DDPG] Critic Loss 92.85330200195312
[DDPG] Total Reward -23.13097 Avg Critic Loss 92.85330 Time 9.47923
[DDPG] Resetting Environment
[DDPG] Starting Episode 5
[DDPG] Episode 5 Step 0 Reward -6.45640 Time 18.70650
[DDPG] Episode 5 Step 1 Reward -8.24536 Time 17.45999
[DDPG] Quadruped Not Upright
[DDPG] Episode 5 Step 2 Reward -6.98186 Time 13.46585
[DDPG] Starting Next Episode
[DDPG] Critic Loss 82.04710388183594
[DDPG] Total Reward -21.68362 Avg Critic Loss 82.04710 Time 7.03894
[DDPG] Resetting Environment
[DDPG] Starting Episode 6
[DDPG] Episode 6 Step 0 Reward -5.42475 Time 15.51702
[DDPG] Episode 6 Step 1 Reward -8.19171 Time 12.38038
[DDPG] Episode 6 Step 2 Reward -5.89803 Time 11.60072
[DDPG] Episode 6 Step 3 Reward -6.00732 Time 12.19320
[DDPG] Episode 6 Step 4 Reward -5.73840 Time 12.53547
[DDPG] Episode 6 Step 5 Reward -5.04993 Time 15.43429
[DDPG] Episode 6 Step 6 Reward -6.26059 Time 17.26216
[DDPG] Episode 6 Step 7 Reward -5.79356 Time 16.60166
[DDPG] Episode 6 Step 8 Reward -6.52667 Time 16.06728
[DDPG] Quadruped Not Upright
[DDPG] Episode 6 Step 9 Reward -15.01554 Time 16.76378
[DDPG] Starting Next Episode
[DDPG] Critic Loss 80.712158203125
[DDPG] Total Reward -69.90650 Avg Critic Loss 80.71216 Time 6.52480
[DDPG] Resetting Environment
[DDPG] Starting Episode 7
[DDPG] Quadruped Not Upright
[DDPG] Episode 7 Step 0 Reward -17.72242 Time 15.86935
[DDPG] Starting Next Episode
[DDPG] Critic Loss 87.78742980957031
[DDPG] Total Reward -17.72242 Avg Critic Loss 87.78743 Time 6.12255
[DDPG] Resetting Environment
[DDPG] Starting Episode 8
[DDPG] Episode 8 Step 0 Reward -7.89233 Time 16.91935
[DDPG] Quadruped Not Upright
[DDPG] Episode 8 Step 1 Reward -15.41106 Time 15.46588
[DDPG] Starting Next Episode
[DDPG] Critic Loss 91.49095153808594
[DDPG] Total Reward -23.30339 Avg Critic Loss 91.49095 Time 6.21171
[DDPG] Resetting Environment
[DDPG] Starting Episode 9
[DDPG] Episode 9 Step 0 Reward -6.46346 Time 16.23515
[DDPG] Episode 9 Step 1 Reward -6.38123 Time 16.82742
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss 88.24225616455078
[DDPG] Total Reward -12.84469 Avg Critic Loss 88.24226 Time 6.73147
[DDPG] Resetting Environment
[DDPG] Starting Episode 10
[DDPG] Episode 10 Step 0 Reward -7.84850 Time 18.33771
[DDPG] Episode 10 Step 1 Reward -7.25827 Time 16.91990
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss 85.48515319824219
[DDPG] Total Reward -15.10677 Avg Critic Loss 85.48515 Time 5.66624
[DDPG] Resetting Environment
[DDPG] Starting Episode 11
[DDPG] Quadruped Not Upright
[DDPG] Episode 11 Step 0 Reward -12.80088 Time 15.16072
[DDPG] Starting Next Episode
[DDPG] Critic Loss 86.51840209960938
[DDPG] Total Reward -12.80088 Avg Critic Loss 86.51840 Time 7.57666
[DDPG] Resetting Environment
[DDPG] Starting Episode 12
[DDPG] Quadruped Not Upright
[DDPG] Episode 12 Step 0 Reward -19.00083 Time 16.26973
[DDPG] Starting Next Episode
[DDPG] Critic Loss 91.4258041381836
[DDPG] Total Reward -19.00083 Avg Critic Loss 91.42580 Time 6.10924
[DDPG] Resetting Environment
[DDPG] Starting Episode 13
[DDPG] Episode 13 Step 0 Reward -9.25404 Time 25.45285
[DDPG] Quadruped Not Upright
[DDPG] Episode 13 Step 1 Reward -14.66037 Time 21.33544
[DDPG] Starting Next Episode
[DDPG] Critic Loss 94.0245361328125
[DDPG] Total Reward -23.91441 Avg Critic Loss 94.02454 Time 8.00294
[DDPG] Resetting Environment
[DDPG] Starting Episode 14
[DDPG] Episode 14 Step 0 Reward -8.12219 Time 21.99722
[DDPG] Quadruped Not Upright
[DDPG] Episode 14 Step 1 Reward -14.78164 Time 21.66947
[DDPG] Starting Next Episode
[DDPG] Critic Loss 95.24227905273438
[DDPG] Total Reward -22.90383 Avg Critic Loss 95.24228 Time 9.12531
[DDPG] Resetting Environment
[DDPG] Starting Episode 15
[DDPG] Episode 15 Step 0 Reward -9.68206 Time 22.54843
[DDPG] Episode 15 Step 1 Reward -5.00513 Time 22.13889
[DDPG] Episode 15 Step 2 Reward -4.94850 Time 22.33221
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss 92.30805206298828
[DDPG] Total Reward -19.63568 Avg Critic Loss 92.30805 Time 9.37810
[DDPG] Resetting Environment
[DDPG] Starting Episode 16
[DDPG] Quadruped Not Upright
[DDPG] Episode 16 Step 0 Reward -19.29005 Time 32.57331
[DDPG] Starting Next Episode
[DDPG] Critic Loss 96.29828643798828
[DDPG] Total Reward -19.29005 Avg Critic Loss 96.29829 Time 13.49831
[DDPG] Resetting Environment
[DDPG] Starting Episode 17
[DDPG] Episode 17 Step 0 Reward -8.65605 Time 30.18889
[DDPG] Episode 17 Step 1 Reward -4.99717 Time 24.23309
[DDPG] Episode 17 Step 2 Reward -4.98094 Time 29.31200
[DDPG] Episode 17 Step 3 Reward -5.05750 Time 28.27269
[DDPG] Episode 17 Step 4 Reward -4.95727 Time 23.03298
[DDPG] Quadruped Not Upright
[DDPG] Episode 17 Step 5 Reward -15.04057 Time 22.27664
[DDPG] Starting Next Episode
[DDPG] Critic Loss 93.76485443115234
[DDPG] Total Reward -43.68950 Avg Critic Loss 93.76485 Time 8.62607
[DDPG] Resetting Environment
[DDPG] Starting Episode 18
[DDPG] Quadruped Not Upright
[DDPG] Episode 18 Step 0 Reward -22.80776 Time 22.21683
[DDPG] Starting Next Episode
[DDPG] Critic Loss 98.70817565917969
[DDPG] Total Reward -22.80776 Avg Critic Loss 98.70818 Time 9.33309
[DDPG] Resetting Environment
[DDPG] Starting Episode 19
[DDPG] Quadruped Not Upright
[DDPG] Episode 19 Step 0 Reward -22.73849 Time 22.70535
[DDPG] Starting Next Episode
[DDPG] Critic Loss 102.00296020507812
[DDPG] Total Reward -22.73849 Avg Critic Loss 102.00296 Time 8.37736
[DDPG] Resetting Environment
[DDPG] Starting Episode 20
[DDPG] Quadruped Not Upright
[DDPG] Episode 20 Step 0 Reward -21.12184 Time 21.03013
[DDPG] Starting Next Episode
[DDPG] Critic Loss 106.78314208984375
[DDPG] Total Reward -21.12184 Avg Critic Loss 106.78314 Time 7.72807
[DDPG] Resetting Environment
[DDPG] Starting Episode 21
[DDPG] Quadruped Not Upright
[DDPG] Episode 21 Step 0 Reward -21.67824 Time 21.39808
[DDPG] Starting Next Episode
[DDPG] Critic Loss 112.5346450805664
[DDPG] Total Reward -21.67824 Avg Critic Loss 112.53465 Time 8.86863
[DDPG] Resetting Environment
[DDPG] Starting Episode 22
[DDPG] Quadruped Not Upright
[DDPG] Episode 22 Step 0 Reward -22.11962 Time 22.66988
[DDPG] Starting Next Episode
[DDPG] Critic Loss 118.64279174804688
[DDPG] Total Reward -22.11962 Avg Critic Loss 118.64279 Time 10.88657
[DDPG] Resetting Environment
[DDPG] Starting Episode 23
[DDPG] Quadruped Not Upright
[DDPG] Episode 23 Step 0 Reward -21.80507 Time 22.62115
[DDPG] Starting Next Episode
[DDPG] Critic Loss 124.61264038085938
[DDPG] Total Reward -21.80507 Avg Critic Loss 124.61264 Time 9.64086
[DDPG] Resetting Environment
[DDPG] Starting Episode 24
[DDPG] Quadruped Not Upright
[DDPG] Episode 24 Step 0 Reward -21.99659 Time 23.19699
[DDPG] Starting Next Episode
[DDPG] Critic Loss 130.5211181640625
[DDPG] Total Reward -21.99659 Avg Critic Loss 130.52112 Time 9.77549
[DDPG] Resetting Environment
[DDPG] Starting Episode 25
[DDPG] Quadruped Not Upright
[DDPG] Episode 25 Step 0 Reward -22.25983 Time 20.79844
[DDPG] Starting Next Episode
[DDPG] Critic Loss 136.36451721191406
[DDPG] Total Reward -22.25983 Avg Critic Loss 136.36452 Time 7.92019
[DDPG] Saving Data
[DDPG] Saving Model
[DDPG] Resetting Environment
[DDPG] Starting Episode 26
[DDPG] Quadruped Not Upright
[DDPG] Episode 26 Step 0 Reward -22.17249 Time 22.83765
[DDPG] Starting Next Episode
[DDPG] Critic Loss 141.99078369140625
[DDPG] Total Reward -22.17249 Avg Critic Loss 141.99078 Time 9.21499
[DDPG] Resetting Environment
[DDPG] Starting Episode 27
[DDPG] Quadruped Not Upright
[DDPG] Episode 27 Step 0 Reward -22.53872 Time 21.97485
[DDPG] Starting Next Episode
[DDPG] Critic Loss 147.63619995117188
[DDPG] Total Reward -22.53872 Avg Critic Loss 147.63620 Time 8.55379
[DDPG] Resetting Environment
[DDPG] Starting Episode 28
[DDPG] Quadruped Not Upright
[DDPG] Episode 28 Step 0 Reward -21.37047 Time 22.64092
[DDPG] Starting Next Episode
[DDPG] Critic Loss 152.2928924560547
[DDPG] Total Reward -21.37047 Avg Critic Loss 152.29289 Time 8.87287
[DDPG] Resetting Environment
[DDPG] Starting Episode 29
[DDPG] Quadruped Not Upright
[DDPG] Episode 29 Step 0 Reward -21.17220 Time 21.89760
[DDPG] Starting Next Episode
[DDPG] Critic Loss 156.71005249023438
[DDPG] Total Reward -21.17220 Avg Critic Loss 156.71005 Time 9.46908
[DDPG] Resetting Environment
[DDPG] Starting Episode 30
[DDPG] Quadruped Not Upright
[DDPG] Episode 30 Step 0 Reward -22.14561 Time 21.08801
[DDPG] Starting Next Episode
[DDPG] Critic Loss 161.54165649414062
[DDPG] Total Reward -22.14561 Avg Critic Loss 161.54166 Time 5.73770
[DDPG] Resetting Environment
[DDPG] Starting Episode 31
[DDPG] Quadruped Not Upright
[DDPG] Episode 31 Step 0 Reward -21.66224 Time 21.89455
[DDPG] Starting Next Episode
[DDPG] Critic Loss 165.87521362304688
[DDPG] Total Reward -21.66224 Avg Critic Loss 165.87521 Time 8.82850
[DDPG] Resetting Environment
[DDPG] Starting Episode 32
[DDPG] Quadruped Not Upright
[DDPG] Episode 32 Step 0 Reward -21.74062 Time 21.05404
[DDPG] Starting Next Episode
[DDPG] Critic Loss 170.06785583496094
[DDPG] Total Reward -21.74062 Avg Critic Loss 170.06786 Time 7.68709
[DDPG] Resetting Environment
[DDPG] Starting Episode 33
[DDPG] Quadruped Not Upright
[DDPG] Episode 33 Step 0 Reward -22.48951 Time 20.99861
[DDPG] Starting Next Episode
[DDPG] Critic Loss 174.0672149658203
[DDPG] Total Reward -22.48951 Avg Critic Loss 174.06721 Time 7.98335
[DDPG] Resetting Environment
[DDPG] Starting Episode 34
[DDPG] Episode 34 Step 0 Reward -7.94754 Time 22.99288
[DDPG] Episode 34 Step 1 Reward -6.57013 Time 23.87110
[DDPG] Quadruped Not Upright
[DDPG] Episode 34 Step 2 Reward -15.36212 Time 21.80274
[DDPG] Starting Next Episode
[DDPG] Critic Loss 169.56588745117188
[DDPG] Total Reward -29.87978 Avg Critic Loss 169.56589 Time 9.03362
[DDPG] Resetting Environment
[DDPG] Starting Episode 35
[DDPG] Quadruped Not Upright
[DDPG] Episode 35 Step 0 Reward -23.74181 Time 22.04859
[DDPG] Starting Next Episode
[DDPG] Critic Loss 173.62535095214844
[DDPG] Total Reward -23.74181 Avg Critic Loss 173.62535 Time 8.27897
[DDPG] Resetting Environment
[DDPG] Starting Episode 36
[DDPG] Quadruped Not Upright
[DDPG] Episode 36 Step 0 Reward -22.70123 Time 19.66173
[DDPG] Starting Next Episode
[DDPG] Critic Loss 176.08041381835938
[DDPG] Total Reward -22.70123 Avg Critic Loss 176.08041 Time 10.61739
[DDPG] Resetting Environment
[DDPG] Starting Episode 37
[DDPG] Quadruped Not Upright
[DDPG] Episode 37 Step 0 Reward -20.28986 Time 20.62426
[DDPG] Starting Next Episode
[DDPG] Critic Loss 175.97352600097656
[DDPG] Total Reward -20.28986 Avg Critic Loss 175.97353 Time 8.88059
[DDPG] Resetting Environment
[DDPG] Starting Episode 38
[DDPG] Quadruped Not Upright
[DDPG] Episode 38 Step 0 Reward -19.16582 Time 21.03829
[DDPG] Starting Next Episode
[DDPG] Critic Loss 177.236328125
[DDPG] Total Reward -19.16582 Avg Critic Loss 177.23633 Time 9.56359
[DDPG] Resetting Environment
[DDPG] Starting Episode 39
[DDPG] Episode 39 Step 0 Reward -9.77530 Time 24.83596
[DDPG] Episode 39 Step 1 Reward -3.66202 Time 22.67276
[DDPG] Episode 39 Step 2 Reward -4.47805 Time 25.22891
[DDPG] Episode 39 Step 3 Reward -4.34574 Time 22.90857
[DDPG] Episode 39 Step 4 Reward -4.35679 Time 24.90446
[DDPG] Episode 39 Step 5 Reward -4.37240 Time 23.04984
[DDPG] Episode 39 Step 6 Reward -4.29910 Time 25.24412
[DDPG] Episode 39 Step 7 Reward -4.27145 Time 24.66717
[DDPG] Episode 39 Step 8 Reward -4.31391 Time 24.50746
[DDPG] Episode 39 Step 9 Reward -4.24005 Time 24.44079
[DDPG] Episode 39 Step 10 Reward -4.37561 Time 21.68986
[DDPG] Episode 39 Step 11 Reward -4.32671 Time 24.26263
[DDPG] Episode 39 Step 12 Reward -4.30315 Time 25.74018
[DDPG] Episode 39 Step 13 Reward -4.30499 Time 28.93517
[DDPG] Episode 39 Step 14 Reward -4.29586 Time 25.36277
[DDPG] Episode 39 Step 15 Reward -4.72822 Time 25.61335
[DDPG] Quadruped Not Upright
[DDPG] Episode 39 Step 16 Reward -16.33286 Time 22.22961
[DDPG] Starting Next Episode
[DDPG] Critic Loss 154.8454132080078
[DDPG] Total Reward -90.78221 Avg Critic Loss 154.84541 Time 7.93716
[DDPG] Resetting Environment
[DDPG] Starting Episode 40
[DDPG] Quadruped Not Upright
[DDPG] Episode 40 Step 0 Reward -19.47773 Time 19.19108
[DDPG] Starting Next Episode
[DDPG] Critic Loss 153.68344116210938
[DDPG] Total Reward -19.47773 Avg Critic Loss 153.68344 Time 8.12363
[DDPG] Resetting Environment
[DDPG] Starting Episode 41
[DDPG] Episode 41 Step 0 Reward -9.01125 Time 21.32047
[DDPG] Episode 41 Step 1 Reward -3.76158 Time 24.43382
[DDPG] Episode 41 Step 2 Reward -3.46383 Time 24.31336
[DDPG] Episode 41 Step 3 Reward -3.54444 Time 24.51967
[DDPG] Quadruped Not Upright
[DDPG] Episode 41 Step 4 Reward -14.00699 Time 26.87136
[DDPG] Starting Next Episode
[DDPG] Critic Loss 149.54653930664062
[DDPG] Total Reward -33.78809 Avg Critic Loss 149.54654 Time 10.86018
[DDPG] Resetting Environment
[DDPG] Starting Episode 42
[DDPG] Quadruped Not Upright
[DDPG] Episode 42 Step 0 Reward -19.91828 Time 23.73369
[DDPG] Starting Next Episode
[DDPG] Critic Loss 150.91525268554688
[DDPG] Total Reward -19.91828 Avg Critic Loss 150.91525 Time 9.46769
[DDPG] Resetting Environment
[DDPG] Starting Episode 43
[DDPG] Episode 43 Step 0 Reward -6.64946 Time 24.91964
[DDPG] Episode 43 Step 1 Reward -5.53043 Time 25.81587
[DDPG] Episode 43 Step 2 Reward -5.48229 Time 21.86464
[DDPG] Episode 43 Step 3 Reward -6.61855 Time 23.45109
[DDPG] Episode 43 Step 4 Reward -4.49850 Time 22.86600
[DDPG] Episode 43 Step 5 Reward -5.48832 Time 22.88981
[DDPG] Episode 43 Step 6 Reward -5.46623 Time 23.79196
[DDPG] Episode 43 Step 7 Reward -5.61704 Time 26.16168
[DDPG] Episode 43 Step 8 Reward -5.53042 Time 23.58643
[DDPG] Episode 43 Step 9 Reward -5.50062 Time 28.79357
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss 142.6011962890625
[DDPG] Total Reward -56.38187 Avg Critic Loss 142.60120 Time 13.02767
[DDPG] Resetting Environment
[DDPG] Starting Episode 44
[DDPG] Episode 44 Step 0 Reward -9.50310 Time 24.97065
[DDPG] Episode 44 Step 1 Reward -8.89503 Time 26.52324
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss 139.75962829589844
[DDPG] Total Reward -18.39812 Avg Critic Loss 139.75963 Time 8.98786
[DDPG] Resetting Environment
[DDPG] Starting Episode 45
[DDPG] Quadruped Not Upright
[DDPG] Episode 45 Step 0 Reward -22.86239 Time 21.79519
[DDPG] Starting Next Episode
[DDPG] Critic Loss 141.2745361328125
[DDPG] Total Reward -22.86239 Avg Critic Loss 141.27454 Time 8.17042
[DDPG] Resetting Environment
[DDPG] Starting Episode 46
[DDPG] Episode 46 Step 0 Reward -9.66705 Time 22.09257
[DDPG] Episode 46 Step 1 Reward -5.33515 Time 24.72968
[DDPG] Quadruped Not Upright
[DDPG] Episode 46 Step 2 Reward -25.19272 Time 19.90612
[DDPG] Starting Next Episode
[DDPG] Critic Loss 145.2751007080078
[DDPG] Total Reward -40.19491 Avg Critic Loss 145.27510 Time 7.12944
[DDPG] Resetting Environment
[DDPG] Starting Episode 47
[DDPG] Quadruped Not Upright
[DDPG] Episode 47 Step 0 Reward -27.75453 Time 23.33457
[DDPG] Starting Next Episode
[DDPG] Critic Loss 147.57177734375
[DDPG] Total Reward -27.75453 Avg Critic Loss 147.57178 Time 10.63427
[DDPG] Resetting Environment
[DDPG] Starting Episode 48
[DDPG] Episode 48 Step 0 Reward -13.85656 Time 21.78816
[DDPG] Quadruped Not Upright
[DDPG] Episode 48 Step 1 Reward -35.30692 Time 23.81625
[DDPG] Starting Next Episode
[DDPG] Critic Loss 157.77444458007812
[DDPG] Total Reward -49.16348 Avg Critic Loss 157.77444 Time 8.26545
[DDPG] Resetting Environment
[DDPG] Starting Episode 49
[DDPG] Quadruped Not Upright
[DDPG] Episode 49 Step 0 Reward -21.43422 Time 23.93913
[DDPG] Starting Next Episode
[DDPG] Critic Loss 159.67112731933594
[DDPG] Total Reward -21.43422 Avg Critic Loss 159.67113 Time 8.73641
[DDPG] Resetting Environment
[DDPG] Starting Episode 50
[DDPG] Quadruped Not Upright
[DDPG] Episode 50 Step 0 Reward -11.61790 Time 21.79740
[DDPG] Starting Next Episode
[DDPG] Critic Loss 159.08827209472656
[DDPG] Total Reward -11.61790 Avg Critic Loss 159.08827 Time 10.29884
[DDPG] Saving Data
[DDPG] Saving Model
[DDPG] Resetting Environment
[DDPG] Starting Episode 51
[DDPG] Quadruped Not Upright
[DDPG] Episode 51 Step 0 Reward -21.22219 Time 22.62723
[DDPG] Starting Next Episode
[DDPG] Critic Loss 161.0101776123047
[DDPG] Total Reward -21.22219 Avg Critic Loss 161.01018 Time 7.22838
[DDPG] Resetting Environment
[DDPG] Starting Episode 52
[DDPG] Episode 52 Step 0 Reward -15.30546 Time 20.24081
[DDPG] Quadruped Not Upright
[DDPG] Episode 52 Step 1 Reward -23.45731 Time 20.28850
[DDPG] Starting Next Episode
[DDPG] Critic Loss 164.46493530273438
[DDPG] Total Reward -38.76277 Avg Critic Loss 164.46494 Time 9.07502
[DDPG] Resetting Environment
[DDPG] Starting Episode 53
[DDPG] Quadruped Not Upright
[DDPG] Episode 53 Step 0 Reward -24.79039 Time 23.26004
[DDPG] Starting Next Episode
[DDPG] Critic Loss 166.60227966308594
[DDPG] Total Reward -24.79039 Avg Critic Loss 166.60228 Time 10.87554
[DDPG] Resetting Environment
[DDPG] Starting Episode 54
[DDPG] Quadruped Not Upright
[DDPG] Episode 54 Step 0 Reward -26.68241 Time 24.27168
[DDPG] Starting Next Episode
[DDPG] Critic Loss 172.2770233154297
[DDPG] Total Reward -26.68241 Avg Critic Loss 172.27702 Time 10.84773
[DDPG] Resetting Environment
[DDPG] Starting Episode 55
[DDPG] Episode 55 Step 0 Reward -5.98494 Time 23.80702
[DDPG] Episode 55 Step 1 Reward -30.30927 Time 21.97009
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward -36.29421 Avg Critic Loss nan Time 10.65795
[DDPG] Resetting Environment
[DDPG] Starting Episode 56
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.23079
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 57
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.93736
[DDPG] Resetting Environment
[DDPG] Starting Episode 58
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 9.24987
[DDPG] Resetting Environment
[DDPG] Starting Episode 59
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.68009
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 60
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 10.77587
[DDPG] Resetting Environment
[DDPG] Starting Episode 61
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 9.41371
[DDPG] Resetting Environment
[DDPG] Starting Episode 62
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 9.10495
[DDPG] Resetting Environment
[DDPG] Starting Episode 63
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.17304
[DDPG] Resetting Environment
[DDPG] Starting Episode 64
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 10.74428
[DDPG] Resetting Environment
[DDPG] Starting Episode 65
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.17001
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 66
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.79794
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 67
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 9.75817
[DDPG] Resetting Environment
[DDPG] Starting Episode 68
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.71261
[DDPG] Resetting Environment
[DDPG] Starting Episode 69
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.63485
[DDPG] Resetting Environment
[DDPG] Starting Episode 70
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.16261
[DDPG] Resetting Environment
[DDPG] Starting Episode 71
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 9.25705
[DDPG] Resetting Environment
[DDPG] Starting Episode 72
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 9.06031
[DDPG] Resetting Environment
[DDPG] Starting Episode 73
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.64551
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 74
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 10.09919
[DDPG] Resetting Environment
[DDPG] Starting Episode 75
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.69073
[DDPG] Saving Data
[DDPG] Saving Model
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 76
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 10.11589
[DDPG] Resetting Environment
[DDPG] Starting Episode 77
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.36897
[DDPG] Resetting Environment
[DDPG] Starting Episode 78
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.70715
[DDPG] Resetting Environment
[DDPG] Starting Episode 79
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 11.79472
[DDPG] Resetting Environment
[DDPG] Starting Episode 80
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.53834
[DDPG] Resetting Environment
[DDPG] Starting Episode 81
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.53824
[DDPG] Resetting Environment
[DDPG] Starting Episode 82
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 9.99891
[DDPG] Resetting Environment
[DDPG] Starting Episode 83
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.97264
[DDPG] Resetting Environment
[DDPG] Starting Episode 84
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.97621
[DDPG] Resetting Environment
[DDPG] Starting Episode 85
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 9.45845
[DDPG] Resetting Environment
[DDPG] Starting Episode 86
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.64517
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 87
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.23650
[DDPG] Resetting Environment
[DDPG] Starting Episode 88
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 9.54940
[DDPG] Resetting Environment
[DDPG] Starting Episode 89
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.54125
[DDPG] Resetting Environment
[DDPG] Starting Episode 90
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.82205
[DDPG] Resetting Environment
[DDPG] Starting Episode 91
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.77387
[DDPG] Resetting Environment
[DDPG] Starting Episode 92
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.61683
[DDPG] Resetting Environment
[DDPG] Starting Episode 93
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 9.77250
[DDPG] Resetting Environment
[DDPG] Starting Episode 94
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.25108
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 95
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.72024
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 96
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 9.53959
[DDPG] Resetting Environment
[DDPG] Starting Episode 97
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 9.05814
[DDPG] Resetting Environment
[DDPG] Starting Episode 98
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 9.25272
[DDPG] Resetting Environment
[DDPG] Starting Episode 99
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 9.64405
[DDPG] Resetting Environment
[DDPG] Starting Episode 100
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 9.72653
[DDPG] Saving Data
[DDPG] Saving Model
[DDPG] Resetting Environment
[DDPG] Starting Episode 101
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.88349
[DDPG] Resetting Environment
[DDPG] Starting Episode 102
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 9.72557
[DDPG] Resetting Environment
[DDPG] Starting Episode 103
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.31106
[DDPG] Resetting Environment
[DDPG] Starting Episode 104
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 9.83149
[DDPG] Resetting Environment
[DDPG] Starting Episode 105
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.17835
[DDPG] Resetting Environment
[DDPG] Starting Episode 106
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 9.39939
[DDPG] Resetting Environment
[DDPG] Starting Episode 107
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 9.47133
[DDPG] Resetting Environment
[DDPG] Starting Episode 108
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 12.36345
[DDPG] Resetting Environment
[DDPG] Starting Episode 109
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 9.89950
[DDPG] Resetting Environment
[DDPG] Starting Episode 110
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 10.80313
[DDPG] Resetting Environment
[DDPG] Starting Episode 111
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 10.97928
[DDPG] Resetting Environment
[DDPG] Starting Episode 112
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.75388
[DDPG] Resetting Environment
[DDPG] Starting Episode 113
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.04563
[DDPG] Resetting Environment
[DDPG] Starting Episode 114
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.96995
[DDPG] Resetting Environment
[DDPG] Starting Episode 115
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.62693
[DDPG] Resetting Environment
[DDPG] Starting Episode 116
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.38355
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 117
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.30932
[DDPG] Resetting Environment
[DDPG] Starting Episode 118
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.77792
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 119
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 9.48105
[DDPG] Resetting Environment
[DDPG] Starting Episode 120
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.02802
[DDPG] Resetting Environment
[DDPG] Starting Episode 121
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 9.35849
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 122
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.49573
[DDPG] Resetting Environment
[DDPG] Starting Episode 123
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.99122
[DDPG] Resetting Environment
[DDPG] Starting Episode 124
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.76679
[DDPG] Resetting Environment
[DDPG] Starting Episode 125
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.93679
[DDPG] Saving Data
[DDPG] Saving Model
[DDPG] Resetting Environment
[DDPG] Starting Episode 126
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.88511
[DDPG] Resetting Environment
[DDPG] Starting Episode 127
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 9.69692
[DDPG] Resetting Environment
[DDPG] Starting Episode 128
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.79469
[DDPG] Resetting Environment
[DDPG] Starting Episode 129
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.75366
[DDPG] Resetting Environment
[DDPG] Starting Episode 130
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 9.27851
[DDPG] Resetting Environment
[DDPG] Starting Episode 131
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.55590
[DDPG] Resetting Environment
[DDPG] Starting Episode 132
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.65803
[DDPG] Resetting Environment
[DDPG] Starting Episode 133
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.00432
[DDPG] Resetting Environment
[DDPG] Starting Episode 134
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.68807
[DDPG] Resetting Environment
[DDPG] Starting Episode 135
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.99626
[DDPG] Resetting Environment
[DDPG] Starting Episode 136
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.01631
[DDPG] Resetting Environment
[DDPG] Starting Episode 137
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 9.19859
[DDPG] Resetting Environment
[DDPG] Starting Episode 138
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.12015
[DDPG] Resetting Environment
[DDPG] Starting Episode 139
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.89944
[DDPG] Resetting Environment
[DDPG] Starting Episode 140
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.73351
[DDPG] Resetting Environment
[DDPG] Starting Episode 141
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.35986
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 142
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.94794
[DDPG] Resetting Environment
[DDPG] Starting Episode 143
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.55634
[DDPG] Resetting Environment
[DDPG] Starting Episode 144
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.69214
[DDPG] Resetting Environment
[DDPG] Starting Episode 145
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.99460
[DDPG] Resetting Environment
[DDPG] Starting Episode 146
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.39591
[DDPG] Resetting Environment
[DDPG] Starting Episode 147
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.33763
[DDPG] Resetting Environment
[DDPG] Starting Episode 148
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.39154
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 149
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.99041
[DDPG] Resetting Environment
[DDPG] Starting Episode 150
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.03937
[DDPG] Saving Data
[DDPG] Saving Model
[DDPG] Resetting Environment
[DDPG] Starting Episode 151
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.32851
[DDPG] Resetting Environment
[DDPG] Starting Episode 152
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.36767
[DDPG] Resetting Environment
[DDPG] Starting Episode 153
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.74119
[DDPG] Resetting Environment
[DDPG] Starting Episode 154
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.98425
[DDPG] Resetting Environment
[DDPG] Starting Episode 155
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.68313
[DDPG] Resetting Environment
[DDPG] Starting Episode 156
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.02312
[DDPG] Resetting Environment
[DDPG] Starting Episode 157
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.56138
[DDPG] Resetting Environment
[DDPG] Starting Episode 158
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.03344
[DDPG] Resetting Environment
[DDPG] Starting Episode 159
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.48905
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 160
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.88808
[DDPG] Resetting Environment
[DDPG] Starting Episode 161
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.55716
[DDPG] Resetting Environment
[DDPG] Starting Episode 162
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.53801
[DDPG] Resetting Environment
[DDPG] Starting Episode 163
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.93384
[DDPG] Resetting Environment
[DDPG] Starting Episode 164
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.05019
[DDPG] Resetting Environment
[DDPG] Starting Episode 165
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.13463
[DDPG] Resetting Environment
[DDPG] Starting Episode 166
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.65658
[DDPG] Resetting Environment
[DDPG] Starting Episode 167
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.68380
[DDPG] Resetting Environment
[DDPG] Starting Episode 168
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.81922
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 169
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.59999
[DDPG] Resetting Environment
[DDPG] Starting Episode 170
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.41846
[DDPG] Resetting Environment
[DDPG] Starting Episode 171
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.96695
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 172
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.86415
[DDPG] Resetting Environment
[DDPG] Starting Episode 173
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.37471
[DDPG] Resetting Environment
[DDPG] Starting Episode 174
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.29143
[DDPG] Resetting Environment
[DDPG] Starting Episode 175
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.91032
[DDPG] Saving Data
[DDPG] Saving Model
[DDPG] Resetting Environment
[DDPG] Starting Episode 176
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.95819
[DDPG] Resetting Environment
[DDPG] Starting Episode 177
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.67175
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 178
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.75033
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 179
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.43212
[DDPG] Resetting Environment
[DDPG] Starting Episode 180
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.85978
[DDPG] Resetting Environment
[DDPG] Starting Episode 181
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.80461
[DDPG] Resetting Environment
[DDPG] Starting Episode 182
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 9.14536
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 183
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.87681
[DDPG] Resetting Environment
[DDPG] Starting Episode 184
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.81813
[DDPG] Resetting Environment
[DDPG] Starting Episode 185
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 9.04768
[DDPG] Resetting Environment
[DDPG] Starting Episode 186
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 9.28131
[DDPG] Resetting Environment
[DDPG] Starting Episode 187
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.51216
[DDPG] Resetting Environment
[DDPG] Starting Episode 188
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.23330
[DDPG] Resetting Environment
[DDPG] Starting Episode 189
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.14975
[DDPG] Resetting Environment
[DDPG] Starting Episode 190
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.08462
[DDPG] Resetting Environment
[DDPG] Starting Episode 191
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 9.05681
[DDPG] Resetting Environment
[DDPG] Starting Episode 192
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.34069
[DDPG] Resetting Environment
[DDPG] Starting Episode 193
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.63649
[DDPG] Resetting Environment
[DDPG] Starting Episode 194
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.32166
[DDPG] Resetting Environment
[DDPG] Starting Episode 195
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.14568
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 196
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 9.62579
[DDPG] Resetting Environment
[DDPG] Starting Episode 197
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.75639
[DDPG] Resetting Environment
[DDPG] Starting Episode 198
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.29852
[DDPG] Resetting Environment
[DDPG] Starting Episode 199
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.59909
[DDPG] Resetting Environment
[DDPG] Starting Episode 200
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.88711
[DDPG] Saving Data
[DDPG] Saving Model
[DDPG] Resetting Environment
[DDPG] Starting Episode 201
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.21568
[DDPG] Resetting Environment
[DDPG] Starting Episode 202
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.70798
[DDPG] Resetting Environment
[DDPG] Starting Episode 203
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.67053
[DDPG] Resetting Environment
[DDPG] Starting Episode 204
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.52484
[DDPG] Resetting Environment
[DDPG] Starting Episode 205
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.96476
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 206
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.00846
[DDPG] Resetting Environment
[DDPG] Starting Episode 207
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.35651
[DDPG] Resetting Environment
[DDPG] Starting Episode 208
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.49287
[DDPG] Resetting Environment
[DDPG] Starting Episode 209
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.78028
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 210
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.53573
[DDPG] Resetting Environment
[DDPG] Starting Episode 211
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.92716
[DDPG] Resetting Environment
[DDPG] Starting Episode 212
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 5.65710
[DDPG] Resetting Environment
[DDPG] Starting Episode 213
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.77716
[DDPG] Resetting Environment
[DDPG] Starting Episode 214
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 9.30636
[DDPG] Resetting Environment
[DDPG] Starting Episode 215
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.72475
[DDPG] Resetting Environment
[DDPG] Starting Episode 216
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.33545
[DDPG] Resetting Environment
[DDPG] Starting Episode 217
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.14701
[DDPG] Resetting Environment
[DDPG] Starting Episode 218
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.77056
[DDPG] Resetting Environment
[DDPG] Starting Episode 219
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.13727
[DDPG] Resetting Environment
[DDPG] Starting Episode 220
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.53883
[DDPG] Resetting Environment
[DDPG] Starting Episode 221
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.30881
[DDPG] Resetting Environment
[DDPG] Starting Episode 222
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.43280
[DDPG] Resetting Environment
[DDPG] Starting Episode 223
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 5.99394
[DDPG] Resetting Environment
[DDPG] Starting Episode 224
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.76928
[DDPG] Resetting Environment
[DDPG] Starting Episode 225
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 9.04462
[DDPG] Saving Data
[DDPG] Saving Model
[DDPG] Resetting Environment
[DDPG] Starting Episode 226
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.25241
[DDPG] Resetting Environment
[DDPG] Starting Episode 227
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.11449
[DDPG] Resetting Environment
[DDPG] Starting Episode 228
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.31847
[DDPG] Resetting Environment
[DDPG] Starting Episode 229
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.44836
[DDPG] Resetting Environment
[DDPG] Starting Episode 230
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.57254
[DDPG] Resetting Environment
[DDPG] Starting Episode 231
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.86995
[DDPG] Resetting Environment
[DDPG] Starting Episode 232
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 5.43297
[DDPG] Resetting Environment
[DDPG] Starting Episode 233
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.73555
[DDPG] Resetting Environment
[DDPG] Starting Episode 234
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.01191
[DDPG] Resetting Environment
[DDPG] Starting Episode 235
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.09101
[DDPG] Resetting Environment
[DDPG] Starting Episode 236
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.26578
[DDPG] Resetting Environment
[DDPG] Starting Episode 237
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.79675
[DDPG] Resetting Environment
[DDPG] Starting Episode 238
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.73591
[DDPG] Resetting Environment
[DDPG] Starting Episode 239
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.58788
[DDPG] Resetting Environment
[DDPG] Starting Episode 240
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.63990
[DDPG] Resetting Environment
[DDPG] Starting Episode 241
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.76656
[DDPG] Resetting Environment
[DDPG] Starting Episode 242
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.79199
[DDPG] Resetting Environment
[DDPG] Starting Episode 243
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.08292
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 244
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.45282
[DDPG] Resetting Environment
[DDPG] Starting Episode 245
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.85134
[DDPG] Resetting Environment
[DDPG] Starting Episode 246
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.13979
[DDPG] Resetting Environment
[DDPG] Starting Episode 247
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.66159
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 248
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.22060
[DDPG] Resetting Environment
[DDPG] Starting Episode 249
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.23285
[DDPG] Resetting Environment
[DDPG] Starting Episode 250
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.67536
[DDPG] Saving Data
[DDPG] Saving Model
[DDPG] Resetting Environment
[DDPG] Starting Episode 251
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.21521
[DDPG] Resetting Environment
[DDPG] Starting Episode 252
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.85320
[DDPG] Resetting Environment
[DDPG] Starting Episode 253
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.38891
[DDPG] Resetting Environment
[DDPG] Starting Episode 254
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.59504
[DDPG] Resetting Environment
[DDPG] Starting Episode 255
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.55458
[DDPG] Resetting Environment
[DDPG] Starting Episode 256
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.57265
[DDPG] Resetting Environment
[DDPG] Starting Episode 257
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.24998
[DDPG] Resetting Environment
[DDPG] Starting Episode 258
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 5.81112
[DDPG] Resetting Environment
[DDPG] Starting Episode 259
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.03872
[DDPG] Resetting Environment
[DDPG] Starting Episode 260
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.54188
[DDPG] Resetting Environment
[DDPG] Starting Episode 261
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.46664
[DDPG] Resetting Environment
[DDPG] Starting Episode 262
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.95086
[DDPG] Resetting Environment
[DDPG] Starting Episode 263
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.71265
[DDPG] Resetting Environment
[DDPG] Starting Episode 264
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.13874
[DDPG] Resetting Environment
[DDPG] Starting Episode 265
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.48703
[DDPG] Resetting Environment
[DDPG] Starting Episode 266
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.67301
[DDPG] Resetting Environment
[DDPG] Starting Episode 267
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.54250
[DDPG] Resetting Environment
[DDPG] Starting Episode 268
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.54346
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 269
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.86045
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 270
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.54728
[DDPG] Resetting Environment
[DDPG] Starting Episode 271
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.71094
[DDPG] Resetting Environment
[DDPG] Starting Episode 272
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.42971
[DDPG] Resetting Environment
[DDPG] Starting Episode 273
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 5.09567
[DDPG] Resetting Environment
[DDPG] Starting Episode 274
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.65584
[DDPG] Resetting Environment
[DDPG] Starting Episode 275
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.68331
[DDPG] Saving Data
[DDPG] Saving Model
[DDPG] Resetting Environment
[DDPG] Starting Episode 276
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.79310
[DDPG] Resetting Environment
[DDPG] Starting Episode 277
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.60888
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 278
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.12505
[DDPG] Resetting Environment
[DDPG] Starting Episode 279
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.35100
[DDPG] Resetting Environment
[DDPG] Starting Episode 280
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 9.24543
[DDPG] Resetting Environment
[DDPG] Starting Episode 281
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.63174
[DDPG] Resetting Environment
[DDPG] Starting Episode 282
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 5.64071
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 283
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.76756
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 284
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.51835
[DDPG] Resetting Environment
[DDPG] Starting Episode 285
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.88808
[DDPG] Resetting Environment
[DDPG] Starting Episode 286
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.00844
[DDPG] Resetting Environment
[DDPG] Starting Episode 287
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.15633
[DDPG] Resetting Environment
[DDPG] Starting Episode 288
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.10009
[DDPG] Resetting Environment
[DDPG] Starting Episode 289
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.88975
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 290
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.40837
[DDPG] Resetting Environment
[DDPG] Starting Episode 291
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.12049
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 292
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.18124
[DDPG] Resetting Environment
[DDPG] Starting Episode 293
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.15570
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 294
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.78675
[DDPG] Resetting Environment
[DDPG] Starting Episode 295
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.69113
[DDPG] Resetting Environment
[DDPG] Starting Episode 296
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.94965
[DDPG] Resetting Environment
[DDPG] Starting Episode 297
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 5.82669
[DDPG] Resetting Environment
[DDPG] Starting Episode 298
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.27481
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 299
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.73806
[DDPG] Resetting Environment
[DDPG] Starting Episode 300
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.41756
[DDPG] Saving Data
[DDPG] Saving Model
[DDPG] Resetting Environment
[DDPG] Starting Episode 301
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.99906
[DDPG] Resetting Environment
[DDPG] Starting Episode 302
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.74593
[DDPG] Resetting Environment
[DDPG] Starting Episode 303
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.07722
[DDPG] Resetting Environment
[DDPG] Starting Episode 304
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.50424
[DDPG] Resetting Environment
[DDPG] Starting Episode 305
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.21354
[DDPG] Resetting Environment
[DDPG] Starting Episode 306
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.15052
[DDPG] Resetting Environment
[DDPG] Starting Episode 307
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.55413
[DDPG] Resetting Environment
[DDPG] Starting Episode 308
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.61981
[DDPG] Resetting Environment
[DDPG] Starting Episode 309
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.18487
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 310
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.86325
[DDPG] Resetting Environment
[DDPG] Starting Episode 311
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 5.78578
[DDPG] Resetting Environment
[DDPG] Starting Episode 312
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.31678
[DDPG] Resetting Environment
[DDPG] Starting Episode 313
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.64021
[DDPG] Resetting Environment
[DDPG] Starting Episode 314
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.88581
[DDPG] Resetting Environment
[DDPG] Starting Episode 315
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 5.52113
[DDPG] Resetting Environment
[DDPG] Starting Episode 316
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 5.99994
[DDPG] Resetting Environment
[DDPG] Starting Episode 317
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.31214
[DDPG] Resetting Environment
[DDPG] Starting Episode 318
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.94469
[DDPG] Resetting Environment
[DDPG] Starting Episode 319
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.22603
[DDPG] Resetting Environment
[DDPG] Starting Episode 320
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.00504
[DDPG] Resetting Environment
[DDPG] Starting Episode 321
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 5.95457
[DDPG] Resetting Environment
[DDPG] Starting Episode 322
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.94104
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 323
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.66234
[DDPG] Resetting Environment
[DDPG] Starting Episode 324
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 10.18656
[DDPG] Resetting Environment
[DDPG] Starting Episode 325
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.06443
[DDPG] Saving Data
[DDPG] Saving Model
[DDPG] Resetting Environment
[DDPG] Starting Episode 326
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.66429
[DDPG] Resetting Environment
[DDPG] Starting Episode 327
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.17808
[DDPG] Resetting Environment
[DDPG] Starting Episode 328
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.55336
[DDPG] Resetting Environment
[DDPG] Starting Episode 329
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.42375
[DDPG] Resetting Environment
[DDPG] Starting Episode 330
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.23911
[DDPG] Resetting Environment
[DDPG] Starting Episode 331
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.05835
[DDPG] Resetting Environment
[DDPG] Starting Episode 332
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.65572
[DDPG] Resetting Environment
[DDPG] Starting Episode 333
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.20240
[DDPG] Resetting Environment
[DDPG] Starting Episode 334
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.78082
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 335
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.57401
[DDPG] Resetting Environment
[DDPG] Starting Episode 336
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.56363
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 337
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.11782
[DDPG] Resetting Environment
[DDPG] Starting Episode 338
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.90550
[DDPG] Resetting Environment
[DDPG] Starting Episode 339
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.28929
[DDPG] Resetting Environment
[DDPG] Starting Episode 340
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.21970
[DDPG] Resetting Environment
[DDPG] Starting Episode 341
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.15276
[DDPG] Resetting Environment
[DDPG] Starting Episode 342
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.35485
[DDPG] Resetting Environment
[DDPG] Starting Episode 343
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 9.01092
[DDPG] Resetting Environment
[DDPG] Starting Episode 344
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.49801
[DDPG] Resetting Environment
[DDPG] Starting Episode 345
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.94008
[DDPG] Resetting Environment
[DDPG] Starting Episode 346
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.64021
[DDPG] Resetting Environment
[DDPG] Starting Episode 347
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.52649
[DDPG] Resetting Environment
[DDPG] Starting Episode 348
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.53574
[DDPG] Resetting Environment
[DDPG] Starting Episode 349
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.35826
[DDPG] Resetting Environment
[DDPG] Starting Episode 350
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.45092
[DDPG] Saving Data
[DDPG] Saving Model
[DDPG] Resetting Environment
[DDPG] Starting Episode 351
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.66197
[DDPG] Resetting Environment
[DDPG] Starting Episode 352
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.24066
[DDPG] Resetting Environment
[DDPG] Starting Episode 353
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.39842
[DDPG] Resetting Environment
[DDPG] Starting Episode 354
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.65158
[DDPG] Resetting Environment
[DDPG] Starting Episode 355
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.23560
[DDPG] Resetting Environment
[DDPG] Starting Episode 356
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.83892
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 357
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.46264
[DDPG] Resetting Environment
[DDPG] Starting Episode 358
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.78436
[DDPG] Resetting Environment
[DDPG] Starting Episode 359
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.84846
[DDPG] Resetting Environment
[DDPG] Starting Episode 360
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.96492
[DDPG] Resetting Environment
[DDPG] Starting Episode 361
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 5.92667
[DDPG] Resetting Environment
[DDPG] Starting Episode 362
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.47801
[DDPG] Resetting Environment
[DDPG] Starting Episode 363
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.73465
[DDPG] Resetting Environment
[DDPG] Starting Episode 364
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.59228
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 365
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.93883
[DDPG] Resetting Environment
[DDPG] Starting Episode 366
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.89479
[DDPG] Resetting Environment
[DDPG] Starting Episode 367
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.97413
[DDPG] Resetting Environment
[DDPG] Starting Episode 368
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.33176
[DDPG] Resetting Environment
[DDPG] Starting Episode 369
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.31604
[DDPG] Resetting Environment
[DDPG] Starting Episode 370
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.04791
[DDPG] Resetting Environment
[DDPG] Starting Episode 371
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.87762
[DDPG] Resetting Environment
[DDPG] Starting Episode 372
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.95161
[DDPG] Resetting Environment
[DDPG] Starting Episode 373
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.86287
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 374
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.23815
[DDPG] Resetting Environment
[DDPG] Starting Episode 375
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.84129
[DDPG] Saving Data
[DDPG] Saving Model
[DDPG] Resetting Environment
[DDPG] Starting Episode 376
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.14974
[DDPG] Resetting Environment
[DDPG] Starting Episode 377
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.36878
[DDPG] Resetting Environment
[DDPG] Starting Episode 378
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.68044
[DDPG] Resetting Environment
[DDPG] Starting Episode 379
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.33253
[DDPG] Resetting Environment
[DDPG] Starting Episode 380
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.05569
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 381
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.33211
[DDPG] Resetting Environment
[DDPG] Starting Episode 382
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.35064
[DDPG] Resetting Environment
[DDPG] Starting Episode 383
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.48583
[DDPG] Resetting Environment
[DDPG] Starting Episode 384
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.11491
[DDPG] Resetting Environment
[DDPG] Starting Episode 385
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.46578
[DDPG] Resetting Environment
[DDPG] Starting Episode 386
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.30695
[DDPG] Resetting Environment
[DDPG] Starting Episode 387
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.46083
[DDPG] Resetting Environment
[DDPG] Starting Episode 388
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.15632
[DDPG] Resetting Environment
[DDPG] Starting Episode 389
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.51977
[DDPG] Resetting Environment
[DDPG] Starting Episode 390
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.47387
[DDPG] Resetting Environment
[DDPG] Starting Episode 391
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.75807
[DDPG] Resetting Environment
[DDPG] Starting Episode 392
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.62756
[DDPG] Resetting Environment
[DDPG] Starting Episode 393
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 9.58347
[DDPG] Resetting Environment
[DDPG] Starting Episode 394
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.41166
[DDPG] Resetting Environment
[DDPG] Starting Episode 395
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 5.59686
[DDPG] Resetting Environment
[DDPG] Starting Episode 396
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.05283
[DDPG] Resetting Environment
[DDPG] Starting Episode 397
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.66748
[DDPG] Resetting Environment
[DDPG] Starting Episode 398
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.71243
[DDPG] Resetting Environment
[DDPG] Starting Episode 399
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.91123
[DDPG] Resetting Environment
[DDPG] Starting Episode 400
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.68130
[DDPG] Saving Data
[DDPG] Saving Model
[DDPG] Resetting Environment
[DDPG] Starting Episode 401
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.80336
[DDPG] Resetting Environment
[DDPG] Starting Episode 402
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 4.71315
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 403
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.06883
[DDPG] Resetting Environment
[DDPG] Starting Episode 404
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.95249
[DDPG] Resetting Environment
[DDPG] Starting Episode 405
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.18424
[DDPG] Resetting Environment
[DDPG] Starting Episode 406
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.39886
[DDPG] Resetting Environment
[DDPG] Starting Episode 407
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.65187
[DDPG] Resetting Environment
[DDPG] Starting Episode 408
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 5.65465
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 409
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.82148
[DDPG] Resetting Environment
[DDPG] Starting Episode 410
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.37726
[DDPG] Resetting Environment
[DDPG] Starting Episode 411
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.58349
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 412
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 9.10906
[DDPG] Resetting Environment
[DDPG] Starting Episode 413
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.08140
[DDPG] Resetting Environment
[DDPG] Starting Episode 414
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.92567
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 415
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.30912
[DDPG] Resetting Environment
[DDPG] Starting Episode 416
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.85376
[DDPG] Resetting Environment
[DDPG] Starting Episode 417
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.41898
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 418
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.57502
[DDPG] Resetting Environment
[DDPG] Starting Episode 419
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.75483
[DDPG] Resetting Environment
[DDPG] Starting Episode 420
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.04297
[DDPG] Resetting Environment
[DDPG] Starting Episode 421
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.03215
[DDPG] Resetting Environment
[DDPG] Starting Episode 422
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.93529
[DDPG] Resetting Environment
[DDPG] Starting Episode 423
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.68808
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 424
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.69269
[DDPG] Resetting Environment
[DDPG] Starting Episode 425
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.11900
[DDPG] Saving Data
[DDPG] Saving Model
[DDPG] Resetting Environment
[DDPG] Starting Episode 426
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.00485
[DDPG] Resetting Environment
[DDPG] Starting Episode 427
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.65121
[DDPG] Resetting Environment
[DDPG] Starting Episode 428
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 5.86575
[DDPG] Resetting Environment
[DDPG] Starting Episode 429
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 5.93120
[DDPG] Resetting Environment
[DDPG] Starting Episode 430
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 4.98979
[DDPG] Resetting Environment
[DDPG] Starting Episode 431
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 5.78770
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 432
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.24782
[DDPG] Resetting Environment
[DDPG] Starting Episode 433
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.78956
[DDPG] Resetting Environment
[DDPG] Starting Episode 434
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 5.33741
[DDPG] Resetting Environment
[DDPG] Starting Episode 435
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 5.16770
[DDPG] Resetting Environment
[DDPG] Starting Episode 436
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.03341
[DDPG] Resetting Environment
[DDPG] Starting Episode 437
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.68552
[DDPG] Resetting Environment
[DDPG] Starting Episode 438
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.01260
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 439
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.60766
[DDPG] Resetting Environment
[DDPG] Starting Episode 440
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.56983
[DDPG] Resetting Environment
[DDPG] Starting Episode 441
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.13523
[DDPG] Resetting Environment
[DDPG] Starting Episode 442
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.09112
[DDPG] Resetting Environment
[DDPG] Starting Episode 443
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.86025
[DDPG] Resetting Environment
[DDPG] Starting Episode 444
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.17753
[DDPG] Resetting Environment
[DDPG] Starting Episode 445
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.19655
[DDPG] Resetting Environment
[DDPG] Starting Episode 446
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.75183
[DDPG] Resetting Environment
[DDPG] Starting Episode 447
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.43623
[DDPG] Resetting Environment
[DDPG] Starting Episode 448
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 5.93822
[DDPG] Resetting Environment
[DDPG] Starting Episode 449
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.51684
[DDPG] Resetting Environment
[DDPG] Starting Episode 450
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.70993
[DDPG] Saving Data
[DDPG] Saving Model
[DDPG] Resetting Environment
[DDPG] Starting Episode 451
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.13597
[DDPG] Resetting Environment
[DDPG] Starting Episode 452
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.10305
[DDPG] Resetting Environment
[DDPG] Starting Episode 453
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.90784
[DDPG] Resetting Environment
[DDPG] Starting Episode 454
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.33412
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 455
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.50400
[DDPG] Resetting Environment
[DDPG] Starting Episode 456
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.19090
[DDPG] Resetting Environment
[DDPG] Starting Episode 457
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.11356
[DDPG] Resetting Environment
[DDPG] Starting Episode 458
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.21491
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 459
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.06575
[DDPG] Resetting Environment
[DDPG] Starting Episode 460
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.26090
[DDPG] Resetting Environment
[DDPG] Starting Episode 461
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.65009
[DDPG] Resetting Environment
[DDPG] Starting Episode 462
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.17938
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 463
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.79504
[DDPG] Resetting Environment
[DDPG] Starting Episode 464
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.07519
[DDPG] Resetting Environment
[DDPG] Starting Episode 465
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.65158
[DDPG] Resetting Environment
[DDPG] Starting Episode 466
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.24988
[DDPG] Resetting Environment
[DDPG] Starting Episode 467
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.21681
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 468
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.91253
[DDPG] Resetting Environment
[DDPG] Starting Episode 469
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.52828
[DDPG] Resetting Environment
[DDPG] Starting Episode 470
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.22212
[DDPG] Resetting Environment
[DDPG] Starting Episode 471
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.77052
[DDPG] Resetting Environment
[DDPG] Starting Episode 472
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 5.72474
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 473
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.49126
[DDPG] Resetting Environment
[DDPG] Starting Episode 474
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.22448
[DDPG] Resetting Environment
[DDPG] Starting Episode 475
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.74065
[DDPG] Saving Data
[DDPG] Saving Model
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 476
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.39358
[DDPG] Resetting Environment
[DDPG] Starting Episode 477
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.56930
[DDPG] Resetting Environment
[DDPG] Starting Episode 478
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.77035
[DDPG] Resetting Environment
[DDPG] Starting Episode 479
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.73507
[DDPG] Resetting Environment
[DDPG] Starting Episode 480
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 5.26658
[DDPG] Resetting Environment
[DDPG] Starting Episode 481
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.09988
[DDPG] Resetting Environment
[DDPG] Starting Episode 482
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.16950
[DDPG] Resetting Environment
[DDPG] Starting Episode 483
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.81628
[DDPG] Resetting Environment
[DDPG] Starting Episode 484
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.46073
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 485
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.77061
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 486
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.59552
[DDPG] Resetting Environment
[DDPG] Starting Episode 487
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.59709
[DDPG] Resetting Environment
[DDPG] Starting Episode 488
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.45815
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 489
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.33921
[DDPG] Resetting Environment
[DDPG] Starting Episode 490
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.77169
[DDPG] Resetting Environment
[DDPG] Starting Episode 491
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.00713
[DDPG] Resetting Environment
[DDPG] Starting Episode 492
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.29203
[DDPG] Resetting Environment
[DDPG] Starting Episode 493
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.48782
[DDPG] Resetting Environment
[DDPG] Starting Episode 494
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.30652
[DDPG] Resetting Environment
[DDPG] Starting Episode 495
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.97941
[DDPG] Resetting Environment
[DDPG] Starting Episode 496
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 5.88134
[DDPG] Resetting Environment
[DDPG] Starting Episode 497
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.57107
[DDPG] Resetting Environment
[DDPG] Starting Episode 498
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.35293
[DDPG] Resetting Environment
[DDPG] Starting Episode 499
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.06767
[DDPG] Resetting Environment
[DDPG] Starting Episode 500
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.53836
[DDPG] Saving Data
[DDPG] Saving Model
[DDPG] Resetting Environment
[DDPG] Starting Episode 501
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.12720
[DDPG] Resetting Environment
[DDPG] Starting Episode 502
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.79694
[DDPG] Resetting Environment
[DDPG] Starting Episode 503
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.08052
[DDPG] Resetting Environment
[DDPG] Starting Episode 504
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.05261
[DDPG] Resetting Environment
[DDPG] Starting Episode 505
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.97425
[DDPG] Resetting Environment
[DDPG] Starting Episode 506
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.53492
[DDPG] Resetting Environment
[DDPG] Starting Episode 507
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.87850
[DDPG] Resetting Environment
[DDPG] Starting Episode 508
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.57439
[DDPG] Resetting Environment
[DDPG] Starting Episode 509
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.50621
[DDPG] Resetting Environment
[DDPG] Starting Episode 510
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 5.85217
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 511
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.30699
[DDPG] Resetting Environment
[DDPG] Starting Episode 512
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.29302
[DDPG] Resetting Environment
[DDPG] Starting Episode 513
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.48925
[DDPG] Resetting Environment
[DDPG] Starting Episode 514
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.40128
[DDPG] Resetting Environment
[DDPG] Starting Episode 515
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.00772
[DDPG] Resetting Environment
[DDPG] Starting Episode 516
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.23608
[DDPG] Resetting Environment
[DDPG] Starting Episode 517
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.16720
[DDPG] Resetting Environment
[DDPG] Starting Episode 518
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 5.83749
[DDPG] Resetting Environment
[DDPG] Starting Episode 519
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.57654
[DDPG] Resetting Environment
[DDPG] Starting Episode 520
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.35246
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 521
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.52027
[DDPG] Resetting Environment
[DDPG] Starting Episode 522
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.19737
[DDPG] Resetting Environment
[DDPG] Starting Episode 523
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 5.99913
[DDPG] Resetting Environment
[DDPG] Starting Episode 524
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 5.11258
[DDPG] Resetting Environment
[DDPG] Starting Episode 525
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.51853
[DDPG] Saving Data
[DDPG] Saving Model
[DDPG] Resetting Environment
[DDPG] Starting Episode 526
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.14957
[DDPG] Resetting Environment
[DDPG] Starting Episode 527
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.65250
[DDPG] Resetting Environment
[DDPG] Starting Episode 528
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.38276
[DDPG] Resetting Environment
[DDPG] Starting Episode 529
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.05082
[DDPG] Resetting Environment
[DDPG] Starting Episode 530
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.99221
[DDPG] Resetting Environment
[DDPG] Starting Episode 531
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.48440
[DDPG] Resetting Environment
[DDPG] Starting Episode 532
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.00194
[DDPG] Resetting Environment
[DDPG] Starting Episode 533
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.73858
[DDPG] Resetting Environment
[DDPG] Starting Episode 534
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.29563
[DDPG] Resetting Environment
[DDPG] Starting Episode 535
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.48876
[DDPG] Resetting Environment
[DDPG] Starting Episode 536
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.17330
[DDPG] Resetting Environment
[DDPG] Starting Episode 537
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.27165
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 538
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.77080
[DDPG] Resetting Environment
[DDPG] Starting Episode 539
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.54932
[DDPG] Resetting Environment
[DDPG] Starting Episode 540
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.62830
[DDPG] Resetting Environment
[DDPG] Starting Episode 541
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.18638
[DDPG] Resetting Environment
[DDPG] Starting Episode 542
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.77224
[DDPG] Resetting Environment
[DDPG] Starting Episode 543
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.22695
[DDPG] Resetting Environment
[DDPG] Starting Episode 544
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.25087
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 545
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.83158
[DDPG] Resetting Environment
[DDPG] Starting Episode 546
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.33032
[DDPG] Resetting Environment
[DDPG] Starting Episode 547
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.61394
[DDPG] Resetting Environment
[DDPG] Starting Episode 548
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.42921
[DDPG] Resetting Environment
[DDPG] Starting Episode 549
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.95658
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 550
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.02422
[DDPG] Saving Data
[DDPG] Saving Model
[DDPG] Resetting Environment
[DDPG] Starting Episode 551
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.56176
[DDPG] Resetting Environment
[DDPG] Starting Episode 552
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 5.68113
[DDPG] Resetting Environment
[DDPG] Starting Episode 553
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.46966
[DDPG] Resetting Environment
[DDPG] Starting Episode 554
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.46074
[DDPG] Resetting Environment
[DDPG] Starting Episode 555
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 4.88631
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 556
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.49694
[DDPG] Resetting Environment
[DDPG] Starting Episode 557
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.97145
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 558
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.28422
[DDPG] Resetting Environment
[DDPG] Starting Episode 559
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.57354
[DDPG] Resetting Environment
[DDPG] Starting Episode 560
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.35287
[DDPG] Resetting Environment
[DDPG] Starting Episode 561
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.26078
[DDPG] Resetting Environment
[DDPG] Starting Episode 562
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.50523
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 563
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.78747
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 564
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.08138
[DDPG] Resetting Environment
[DDPG] Starting Episode 565
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.27003
[DDPG] Resetting Environment
[DDPG] Starting Episode 566
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.68210
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 567
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.59402
[DDPG] Resetting Environment
[DDPG] Starting Episode 568
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.34487
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 569
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.05079
[DDPG] Resetting Environment
[DDPG] Starting Episode 570
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.30844
[DDPG] Resetting Environment
[DDPG] Starting Episode 571
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.89097
[DDPG] Resetting Environment
[DDPG] Starting Episode 572
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.28812
[DDPG] Resetting Environment
[DDPG] Starting Episode 573
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.16547
[DDPG] Resetting Environment
[DDPG] Starting Episode 574
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.07680
[DDPG] Resetting Environment
[DDPG] Starting Episode 575
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.21553
[DDPG] Saving Data
[DDPG] Saving Model
[DDPG] Resetting Environment
[DDPG] Starting Episode 576
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.01144
[DDPG] Resetting Environment
[DDPG] Starting Episode 577
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 5.47900
[DDPG] Resetting Environment
[DDPG] Starting Episode 578
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.26683
[DDPG] Resetting Environment
[DDPG] Starting Episode 579
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 5.68060
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 580
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.58518
[DDPG] Resetting Environment
[DDPG] Starting Episode 581
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.52105
[DDPG] Resetting Environment
[DDPG] Starting Episode 582
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.75371
[DDPG] Resetting Environment
[DDPG] Starting Episode 583
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.65996
[DDPG] Resetting Environment
[DDPG] Starting Episode 584
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.20432
[DDPG] Resetting Environment
[]
[DDPG] Error in Resetting End Training
[DDPG] Starting Episode 585
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.82365
[DDPG] Resetting Environment
[DDPG] Starting Episode 586
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.26872
[DDPG] Resetting Environment
[DDPG] Starting Episode 587
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.02169
[DDPG] Resetting Environment
[DDPG] Starting Episode 588
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.81909
[DDPG] Resetting Environment
[DDPG] Starting Episode 589
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.55469
[DDPG] Resetting Environment
[DDPG] Starting Episode 590
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.18783
[DDPG] Resetting Environment
[DDPG] Starting Episode 591
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.16965
[DDPG] Resetting Environment
[DDPG] Starting Episode 592
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.83479
[DDPG] Resetting Environment
[DDPG] Starting Episode 593
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.03865
[DDPG] Resetting Environment
[DDPG] Starting Episode 594
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.83346
[DDPG] Resetting Environment
[DDPG] Starting Episode 595
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.16053
[DDPG] Resetting Environment
[DDPG] Starting Episode 596
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.45883
[DDPG] Resetting Environment
[DDPG] Starting Episode 597
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.37533
[DDPG] Resetting Environment
[DDPG] Starting Episode 598
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.94333
[DDPG] Resetting Environment
[DDPG] Starting Episode 599
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 8.15850
[DDPG] Resetting Environment
[DDPG] Starting Episode 600
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 6.59654
[DDPG] Saving Data
[DDPG] Saving Model
[DDPG] Resetting Environment
[DDPG] Starting Episode 601
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
[DDPG] Critic Loss nan
[DDPG] Total Reward 0.00000 Avg Critic Loss nan Time 7.66156
[DDPG] Resetting Environment
Traceback (most recent call last):
  File "learn.py", line 1406, in <module>
    learner.learn(
  File "learn.py", line 905, in learn
    self.current_time_step = self.env.reset()
  File "/home/ubuntu/.local/lib/python3.8/site-packages/tf_agents/environments/tf_environment.py", line 209, in reset
    return self._reset()
  File "/home/ubuntu/CPGController/src/rl/env.py", line 55, in _reset
    self._state, self._reward = self.quadruped.reset()
  File "/home/ubuntu/CPGController/src/simulations/ws/src/quadruped/scripts/quadruped.py", line 919, in reset
    AB = self.all_legs.get_AB()
  File "/home/ubuntu/CPGController/src/simulations/ws/src/quadruped/scripts/quadruped.py", line 300, in get_AB
    fr_contact, fl_contact, br_contact, bl_contact = self.get_all_contacts()
  File "/home/ubuntu/CPGController/src/simulations/ws/src/quadruped/scripts/quadruped.py", line 296, in get_all_contacts
    self.bl_contact = self.back_left.get_processed_contact_state()
  File "/home/ubuntu/CPGController/src/simulations/ws/src/quadruped/scripts/quadruped.py", line 161, in get_processed_contact_state
    force = np.mean(
  File "<__array_function__ internals>", line 5, in mean
  File "/usr/lib/python3/dist-packages/numpy/core/fromnumeric.py", line 3256, in mean
    return _methods._mean(a, axis=axis, dtype=dtype,
  File "/usr/lib/python3/dist-packages/numpy/core/_methods.py", line 153, in _mean
    ret = um.true_divide(
FloatingPointError: underflow encountered in true_divide
2021-04-13 04:54:23.844754: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-04-13 04:55:49.865246: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
Unable to init server: Could not connect: Connection refused
Unable to init server: Could not connect: Connection refused

(learn.py:159320): Gdk-CRITICAL **: 04:56:08.448: gdk_cursor_new_for_display: assertion 'GDK_IS_DISPLAY (display)' failed
2021-04-13 04:56:10.136117: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-04-13 04:56:10.144156: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-04-13 04:56:10.216996: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-13 04:56:10.221490: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:00:1e.0 name: Tesla T4 computeCapability: 7.5
coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s
2021-04-13 04:56:10.221538: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-04-13 04:56:10.248575: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-04-13 04:56:10.249264: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-04-13 04:56:10.265783: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-04-13 04:56:10.269677: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-04-13 04:56:10.295266: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-04-13 04:56:10.301805: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-04-13 04:56:10.303594: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-04-13 04:56:10.304066: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-13 04:56:10.311919: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-13 04:56:10.314472: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
[DDPG] Building the actor model
2021-04-13 04:56:10.410449: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-04-13 04:56:10.412422: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-04-13 04:56:10.414222: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-13 04:56:10.422113: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:00:1e.0 name: Tesla T4 computeCapability: 7.5
coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s
2021-04-13 04:56:10.422928: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-04-13 04:56:10.424939: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-04-13 04:56:10.425470: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-04-13 04:56:10.426944: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-04-13 04:56:10.427615: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-04-13 04:56:10.429070: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-04-13 04:56:10.429139: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-04-13 04:56:10.429180: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-04-13 04:56:10.431620: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-13 04:56:10.437532: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-13 04:56:10.446682: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-04-13 04:56:10.447034: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-04-13 04:56:15.850368: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-04-13 04:56:15.851331: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2021-04-13 04:56:15.851464: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2021-04-13 04:56:15.853169: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-13 04:56:15.855590: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-13 04:56:15.859760: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-13 04:56:15.861843: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 12795 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5)
[DDPG] Building the actor model
[DDPG] Building the critic model
[DDPG] Building the critic model
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1618289795.939959066, 7.412000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1618289795.943093401, 7.413000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1618289795.943450984, 7.413000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1618289797.740039735, 7.868000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1618289800.605931660, 8.594000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1618289803.561983987, 9.394000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1618289806.599693407, 10.194000000]: Ready to take commands for planning group back_left_leg.[0m
[Actor] GPU>>>>>>>>>>>>
[Actor] [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
[DDPG] Loading Actor Weights
[DDPG] Training Start
[DDPG] Resetting Environment
[DDPG] Starting Pretraining Test
2021-04-13 04:58:17.453646: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-04-13 04:58:22.109502: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
[DDPG] Step 0 Reward 51.74420 Time 30.22058
[DDPG] Step 1 Reward -8.23735 Time 22.38856
[DDPG] Step 2 Reward -3.13059 Time 25.54814
[DDPG] Step 3 Reward 1.46780 Time 26.36620
[DDPG] Step 4 Reward -6.20018 Time 24.72995
[DDPG] Resetting Environment
[DDPG] Starting Episode 500
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
Traceback (most recent call last):
  File "learn.py", line 1406, in <module>
    learner.learn(
  File "learn.py", line 1012, in learn
    states = [tf.concat(state, 0) for state in states]
  File "learn.py", line 1012, in <listcomp>
    states = [tf.concat(state, 0) for state in states]
  File "/home/ubuntu/.local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py", line 201, in wrapper
    return target(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py", line 1677, in concat
    return gen_array_ops.concat_v2(values=values, axis=axis, name=name)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py", line 1193, in concat_v2
    _ops.raise_from_not_ok_status(e, name)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py", line 6862, in raise_from_not_ok_status
    six.raise_from(core._status_to_exception(e.code, message), None)
  File "<string>", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError: OpKernel 'ConcatV2' has constraint on attr 'T' not in NodeDef '[N=0, Tidx=DT_INT32]', KernelDef: 'op: "ConcatV2" device_type: "GPU" constraint { name: "T" allowed_values { list { type: DT_INT32 } } } host_memory_arg: "values" host_memory_arg: "axis" host_memory_arg: "output"' [Op:ConcatV2] name: concat
2021-04-13 05:01:49.459353: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
Unable to init server: Could not connect: Connection refused
Unable to init server: Could not connect: Connection refused

(learn.py:161055): Gdk-CRITICAL **: 05:02:06.684: gdk_cursor_new_for_display: assertion 'GDK_IS_DISPLAY (display)' failed
2021-04-13 05:02:08.364132: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-04-13 05:02:08.370896: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-04-13 05:02:08.458012: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-13 05:02:08.465623: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:00:1e.0 name: Tesla T4 computeCapability: 7.5
coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s
2021-04-13 05:02:08.465832: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-04-13 05:02:08.496276: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-04-13 05:02:08.496425: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-04-13 05:02:08.510397: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-04-13 05:02:08.513868: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-04-13 05:02:08.544731: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-04-13 05:02:08.551574: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-04-13 05:02:08.554328: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-04-13 05:02:08.556712: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-13 05:02:08.562120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-13 05:02:08.567223: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
[DDPG] Building the actor model
2021-04-13 05:02:08.657952: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-04-13 05:02:08.660724: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-04-13 05:02:08.662049: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-13 05:02:08.668448: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:00:1e.0 name: Tesla T4 computeCapability: 7.5
coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s
2021-04-13 05:02:08.668486: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-04-13 05:02:08.668891: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-04-13 05:02:08.669027: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-04-13 05:02:08.669120: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-04-13 05:02:08.671823: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-04-13 05:02:08.671920: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-04-13 05:02:08.672011: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-04-13 05:02:08.673076: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-04-13 05:02:08.674706: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-13 05:02:08.680760: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-13 05:02:08.687743: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-04-13 05:02:08.687802: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-04-13 05:02:13.859630: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-04-13 05:02:13.861308: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2021-04-13 05:02:13.861380: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2021-04-13 05:02:13.861848: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-13 05:02:13.868142: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-13 05:02:13.872415: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-13 05:02:13.876471: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 12795 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5)
[DDPG] Building the actor model
[DDPG] Building the critic model
[DDPG] Building the critic model
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1618290154.984172462, 6.055000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1618290154.985320308, 6.055000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1618290154.988102925, 6.056000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1618290156.687799781, 6.569000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1618290159.156179669, 7.200000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1618290161.865769991, 8.000000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1618290164.025941865, 8.800000000]: Ready to take commands for planning group back_left_leg.[0m
[Actor] GPU>>>>>>>>>>>>
[Actor] [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
[DDPG] Loading Actor Weights
[DDPG] Training Start
[DDPG] Resetting Environment
[DDPG] Starting Pretraining Test
2021-04-13 05:04:16.647555: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-04-13 05:04:20.497898: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
[DDPG] Step 0 Reward 62.17607 Time 29.34454
[DDPG] Step 1 Reward -7.43806 Time 22.46049
[DDPG] Quadruped Not Upright
[DDPG] Step 2 Reward -18.95792 Time 20.33558
[DDPG] Quadruped Not Upright
[DDPG] Step 3 Reward -15.45215 Time 19.54192
[DDPG] Quadruped Not Upright
[DDPG] Step 4 Reward -13.25015 Time 21.51603
[DDPG] Resetting Environment
[DDPG] Starting Episode 400
[DDPG] Action value NaN. Ending Episode
[DDPG] Floating Point Error in reward computation
Traceback (most recent call last):
  File "learn.py", line 1406, in <module>
    
  File "learn.py", line 1012, in learn
    next_states[i].append(s)
  File "learn.py", line 1012, in <listcomp>
    next_states[i].append(s)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py", line 201, in wrapper
    return target(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py", line 1677, in concat
    return gen_array_ops.concat_v2(values=values, axis=axis, name=name)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py", line 1193, in concat_v2
    _ops.raise_from_not_ok_status(e, name)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py", line 6862, in raise_from_not_ok_status
    six.raise_from(core._status_to_exception(e.code, message), None)
  File "<string>", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError: OpKernel 'ConcatV2' has constraint on attr 'T' not in NodeDef '[N=0, Tidx=DT_INT32]', KernelDef: 'op: "ConcatV2" device_type: "GPU" constraint { name: "T" allowed_values { list { type: DT_INT32 } } } host_memory_arg: "values" host_memory_arg: "axis" host_memory_arg: "output"' [Op:ConcatV2] name: concat
2021-04-13 05:11:43.153476: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
Unable to init server: Could not connect: Connection refused
Unable to init server: Could not connect: Connection refused

(learn.py:164050): Gdk-CRITICAL **: 05:12:07.070: gdk_cursor_new_for_display: assertion 'GDK_IS_DISPLAY (display)' failed
2021-04-13 05:12:08.573887: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-04-13 05:12:08.586648: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-04-13 05:12:08.675699: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-13 05:12:08.680224: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:00:1e.0 name: Tesla T4 computeCapability: 7.5
coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s
2021-04-13 05:12:08.682433: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-04-13 05:12:08.712094: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-04-13 05:12:08.713779: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-04-13 05:12:08.728171: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-04-13 05:12:08.729460: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-04-13 05:12:08.767973: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-04-13 05:12:08.778163: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-04-13 05:12:08.780978: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-04-13 05:12:08.783438: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-13 05:12:08.791331: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-13 05:12:08.798376: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
[DDPG] Building the actor model
2021-04-13 05:12:08.916888: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-04-13 05:12:08.920173: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-04-13 05:12:08.922646: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-13 05:12:08.931634: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:00:1e.0 name: Tesla T4 computeCapability: 7.5
coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s
2021-04-13 05:12:08.932211: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-04-13 05:12:08.934117: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-04-13 05:12:08.937627: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-04-13 05:12:08.937818: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-04-13 05:12:08.938073: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-04-13 05:12:08.938858: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-04-13 05:12:08.939678: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-04-13 05:12:08.939882: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-04-13 05:12:08.940917: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-13 05:12:08.946654: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-13 05:12:08.953612: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-04-13 05:12:08.954042: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-04-13 05:12:14.709010: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-04-13 05:12:14.709067: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2021-04-13 05:12:14.709079: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2021-04-13 05:12:14.714359: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-13 05:12:14.723613: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-13 05:12:14.728924: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-13 05:12:14.735651: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 12393 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5)
[DDPG] Building the actor model
[DDPG] Building the critic model
[DDPG] Building the critic model
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1618290762.638720512, 6.524000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1618290762.639762362, 6.524000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1618290762.640016526, 6.524000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1618290764.490760260, 6.964000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1618290767.528288623, 7.593000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1618290772.020899731, 8.393000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1618290775.992659957, 9.193000000]: Ready to take commands for planning group back_left_leg.[0m
[Actor] GPU>>>>>>>>>>>>
[Actor] [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
[DDPG] Loading Actor Weights
[DDPG] Training Start
[DDPG] Resetting Environment
[DDPG] Starting Pretraining Test
2021-04-13 05:14:52.922205: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-04-13 05:14:58.538535: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
[DDPG] Step 0 Reward 44.54643 Time 39.30314
[DDPG] Step 1 Reward 39.17766 Time 28.16538
[DDPG] Step 2 Reward 43.36988 Time 31.15847
[DDPG] Step 3 Reward 47.67365 Time 33.48104
[DDPG] Quadruped Not Upright
[DDPG] Step 4 Reward -19.72891 Time 26.21380
[DDPG] Resetting Environment
[DDPG] Starting Episode 400
[DDPG] Action value NaN. Ending Episode
Traceback (most recent call last):
  File "learn.py", line 1407, in <module>
    learner.learn(
  File "learn.py", line 1013, in learn
    states = [tf.concat(state, 0) for state in states]
  File "learn.py", line 1013, in <listcomp>
    states = [tf.concat(state, 0) for state in states]
  File "/home/ubuntu/.local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py", line 201, in wrapper
    return target(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py", line 1677, in concat
    return gen_array_ops.concat_v2(values=values, axis=axis, name=name)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py", line 1193, in concat_v2
    _ops.raise_from_not_ok_status(e, name)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py", line 6862, in raise_from_not_ok_status
    six.raise_from(core._status_to_exception(e.code, message), None)
  File "<string>", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError: OpKernel 'ConcatV2' has constraint on attr 'T' not in NodeDef '[N=0, Tidx=DT_INT32]', KernelDef: 'op: "ConcatV2" device_type: "GPU" constraint { name: "T" allowed_values { list { type: DT_INT32 } } } host_memory_arg: "values" host_memory_arg: "axis" host_memory_arg: "output"' [Op:ConcatV2] name: concat
2021-04-13 06:35:08.958623: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
Unable to init server: Could not connect: Connection refused
Unable to init server: Could not connect: Connection refused

(learn.py:182796): Gdk-CRITICAL **: 06:35:37.726: gdk_cursor_new_for_display: assertion 'GDK_IS_DISPLAY (display)' failed
2021-04-13 06:35:39.489762: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-04-13 06:35:39.497898: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-04-13 06:35:39.584199: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-13 06:35:39.590418: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:00:1e.0 name: Tesla T4 computeCapability: 7.5
coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s
2021-04-13 06:35:39.590478: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-04-13 06:35:39.624689: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-04-13 06:35:39.629745: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-04-13 06:35:39.653269: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-04-13 06:35:39.659083: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-04-13 06:35:39.714089: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-04-13 06:35:39.728073: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-04-13 06:35:39.734385: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-04-13 06:35:39.737476: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-13 06:35:39.748063: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-13 06:35:39.754573: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
[DDPG] Building the actor model
2021-04-13 06:35:39.902986: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-04-13 06:35:39.908636: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-04-13 06:35:39.910301: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-13 06:35:39.918183: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:00:1e.0 name: Tesla T4 computeCapability: 7.5
coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s
2021-04-13 06:35:39.918226: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-04-13 06:35:39.918388: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-04-13 06:35:39.921573: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-04-13 06:35:39.922918: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-04-13 06:35:39.923002: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-04-13 06:35:39.923053: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-04-13 06:35:39.923102: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-04-13 06:35:39.923154: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-04-13 06:35:39.923314: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-13 06:35:39.935670: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-13 06:35:39.942016: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-04-13 06:35:39.942080: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-04-13 06:35:46.331187: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-04-13 06:35:46.331723: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2021-04-13 06:35:46.331826: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2021-04-13 06:35:46.334139: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-13 06:35:46.339191: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-13 06:35:46.343258: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-13 06:35:46.345965: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 12765 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5)
[DDPG] Building the actor model
[DDPG] Building the critic model
[DDPG] Building the critic model
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1618295942.679696610, 1421.699000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1618295942.682051044, 1421.700000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1618295942.682685797, 1421.700000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
Traceback (most recent call last):
  File "learn.py", line 1375, in <module>
    learner = Learner(params, args.experiment, False)
  File "learn.py", line 124, in __init__
    self.env = Env(
  File "/home/ubuntu/CPGController/src/rl/env.py", line 43, in __init__
    self.quadruped = Quadruped(params, experiment)
  File "/home/ubuntu/CPGController/src/simulations/ws/src/quadruped/scripts/quadruped.py", line 457, in __init__
    self.kinematics = Kinematics(
  File "/home/ubuntu/CPGController/src/simulations/ws/src/quadruped/scripts/kinematics.py", line 21, in __init__
    self.front_right_leg = moveit_commander.MoveGroupCommander(
  File "/opt/ros/noetic/lib/python3/dist-packages/moveit_commander/move_group.py", line 53, in __init__
    self._g = _moveit_move_group_interface.MoveGroupInterface(name, robot_description, ns, wait_for_servers)
RuntimeError: Unable to connect to move_group action server 'move_group' within allotted time (5s)
2021-04-13 06:42:01.498242: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
Unable to init server: Could not connect: Connection refused
Unable to init server: Could not connect: Connection refused

(learn.py:185449): Gdk-CRITICAL **: 06:42:17.552: gdk_cursor_new_for_display: assertion 'GDK_IS_DISPLAY (display)' failed
2021-04-13 06:42:18.850157: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-04-13 06:42:18.853474: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-04-13 06:42:18.892055: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-13 06:42:18.894178: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:00:1e.0 name: Tesla T4 computeCapability: 7.5
coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s
2021-04-13 06:42:18.894224: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-04-13 06:42:18.905734: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-04-13 06:42:18.905868: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-04-13 06:42:18.911313: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-04-13 06:42:18.912026: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-04-13 06:42:18.925742: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-04-13 06:42:18.928494: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-04-13 06:42:18.929547: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-04-13 06:42:18.930071: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-13 06:42:18.932180: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-13 06:42:18.934158: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
[DDPG] Building the actor model
2021-04-13 06:42:18.969469: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-04-13 06:42:18.969754: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-04-13 06:42:18.970035: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-13 06:42:18.971025: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:00:1e.0 name: Tesla T4 computeCapability: 7.5
coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s
2021-04-13 06:42:18.971062: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-04-13 06:42:18.971199: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-04-13 06:42:18.971286: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-04-13 06:42:18.971448: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-04-13 06:42:18.971730: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-04-13 06:42:18.972082: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-04-13 06:42:18.972241: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-04-13 06:42:18.972337: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-04-13 06:42:18.972618: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-13 06:42:18.973843: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-13 06:42:18.975127: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-04-13 06:42:18.975232: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-04-13 06:42:23.985782: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-04-13 06:42:23.986134: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2021-04-13 06:42:23.986241: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2021-04-13 06:42:23.989137: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-13 06:42:23.993483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-13 06:42:23.999968: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-13 06:42:24.004611: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 12393 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5)
[DDPG] Building the actor model
[DDPG] Building the critic model
[DDPG] Building the critic model
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[DDPG] Waiting for joint trajectory action
[DDPG] Found joint trajectory action!
[0m[ INFO] [1618296161.658243102, 5.232000000]: Loading robot model 'quadruped'...[0m
[0m[ INFO] [1618296161.659301378, 5.232000000]: No root/virtual joint specified in SRDF. Assuming fixed joint[0m
[33m[ WARN] [1618296161.659787134, 5.232000000]: Link dummy_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.[0m
[0m[ INFO] [1618296163.401746733, 5.927000000]: Ready to take commands for planning group front_right_leg.[0m
[0m[ INFO] [1618296165.197307984, 6.580000000]: Ready to take commands for planning group front_left_leg.[0m
[0m[ INFO] [1618296167.668273349, 7.380000000]: Ready to take commands for planning group back_right_leg.[0m
[0m[ INFO] [1618296169.786753229, 8.180000000]: Ready to take commands for planning group back_left_leg.[0m
